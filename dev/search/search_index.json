{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"_index/","text":"The Knative project provides a set of Kubernetes components that introduce event-driven and serverless capabilities for Kubernetes clusters. Knative APIs build on existing Kubernetes APIs, so that Knative resources are compatible with other Kubernetes-native resources, and can be managed by cluster administrators using existing Kubernetes tools. Common languages and frameworks that include Kubernetes-friendly tooling work smoothly with Knative to reduce the time spent solving common deployment issues, such as: Deploying a container Routing and managing traffic with blue/green deployment Scaling automatically and sizing workloads based on demand Binding running services to eventing ecosystems There are two core Knative components that can be installed and used together or independently to provide different functions: Knative Serving : Easily manage stateless services on Kubernetes by reducing the developer effort required for autoscaling, networking, and rollouts. Knative Eventing : Easily route events between on-cluster and off-cluster components by exposing event routing as configuration rather than embedded in code. These components are delivered as Kubernetes custom resource definitions (CRDs), which can be configured by a cluster administrator to provide default settings for developer-created applications and event workflow components. Note : Earlier versions of Knative included a build component. That component has since evolved into the separate Tekton Pipelines project. Getting started \u00b6 Installing Knative Getting started with app deployment Getting started with serving Getting started with eventing Configuration and networking \u00b6 Using a custom domain Assigning a static IP address for Knative on Google Kubernetes Engine Configuring HTTPS with a custom certificate Configuring high availability Samples and demos \u00b6 Autoscaling Binding running services to eventing ecosystems REST API sample All samples for serving All samples for eventing Observability \u00b6 Serving Metrics API Eventing Metrics API Collecting metrics Debugging \u00b6 Debugging application issues","title":"Welcome to Knative"},{"location":"_index/#getting-started","text":"Installing Knative Getting started with app deployment Getting started with serving Getting started with eventing","title":"Getting started"},{"location":"_index/#configuration-and-networking","text":"Using a custom domain Assigning a static IP address for Knative on Google Kubernetes Engine Configuring HTTPS with a custom certificate Configuring high availability","title":"Configuration and networking"},{"location":"_index/#samples-and-demos","text":"Autoscaling Binding running services to eventing ecosystems REST API sample All samples for serving All samples for eventing","title":"Samples and demos"},{"location":"_index/#observability","text":"Serving Metrics API Eventing Metrics API Collecting metrics","title":"Observability"},{"location":"_index/#debugging","text":"Debugging application issues","title":"Debugging"},{"location":"knative-offerings/","text":"Knative Offerings \u00b6 Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products. Please check with each of these vendors for what is or is not supported. Here is a list of commercial Knative products (alphabetically): Gardener : Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime. Google Cloud Run for Anthos : Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications. Google Cloud Run : A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds. IBM Cloud Code Engine : A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions. Red Hat Openshift Serverless : enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications. TriggerMesh Cloud : A fully-managed Knative and Tekton cloud-native integration platform. With support for AWS, Azure and Google event sources and brokers. Cloud Native Runtimes for VMware Tanzu : A serverless application runtime for Kubernetes that is based on Knative, and runs on a single Kubernetes cluster","title":"Knative Offerings"},{"location":"knative-offerings/#knative-offerings","text":"Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products. Please check with each of these vendors for what is or is not supported. Here is a list of commercial Knative products (alphabetically): Gardener : Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime. Google Cloud Run for Anthos : Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications. Google Cloud Run : A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds. IBM Cloud Code Engine : A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions. Red Hat Openshift Serverless : enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications. TriggerMesh Cloud : A fully-managed Knative and Tekton cloud-native integration platform. With support for AWS, Azure and Google event sources and brokers. Cloud Native Runtimes for VMware Tanzu : A serverless application runtime for Kubernetes that is based on Knative, and runs on a single Kubernetes cluster","title":"Knative Offerings"},{"location":"samples/","text":"Knative code samples \u00b6 Find and use Knative code samples to help you get up and running with common use cases. Code samples include content from the Knative team and community members. Browse all code samples to find other languages and use cases that might align closer with your goals. Knative owned and maintained \u00b6 View the set of Knative code samples that are actively tested and maintained: Eventing and Eventing Sources code samples Serving code samples Community owned and maintained \u00b6 View code samples that are contributed and maintained by the community . External code samples \u00b6 A list of links to Knative code samples that live outside of https://knative.dev : Image processing using Knative Eventing, Cloud Run on GKE (Knative Serving implementation) and Google Cloud Vision API A potpourri of Knative Eventing Examples Knfun - a complete Knative example of three functions using Twitter and Watson API that use kn to deploy and manage functions Knative Eventing (Cloud Events) example using spring-boot and spring-cloud-streams + Kafka Image processing pipeline using Knative Eventing on GKE, Google Cloud Vision API and ImageSharp library BigQuery processing pipeline using Knative Eventing on GKE, Cloud Scheduler, BigQuery, mathplotlib and SendGrid Please add links to your externally hosted Knative code sample.","title":"Knative code samples"},{"location":"samples/#knative-code-samples","text":"Find and use Knative code samples to help you get up and running with common use cases. Code samples include content from the Knative team and community members. Browse all code samples to find other languages and use cases that might align closer with your goals.","title":"Knative code samples"},{"location":"samples/#knative-owned-and-maintained","text":"View the set of Knative code samples that are actively tested and maintained: Eventing and Eventing Sources code samples Serving code samples","title":"Knative owned and maintained"},{"location":"samples/#community-owned-and-maintained","text":"View code samples that are contributed and maintained by the community .","title":"Community owned and maintained"},{"location":"samples/#external-code-samples","text":"A list of links to Knative code samples that live outside of https://knative.dev : Image processing using Knative Eventing, Cloud Run on GKE (Knative Serving implementation) and Google Cloud Vision API A potpourri of Knative Eventing Examples Knfun - a complete Knative example of three functions using Twitter and Watson API that use kn to deploy and manage functions Knative Eventing (Cloud Events) example using spring-boot and spring-cloud-streams + Kafka Image processing pipeline using Knative Eventing on GKE, Google Cloud Vision API and ImageSharp library BigQuery processing pipeline using Knative Eventing on GKE, Cloud Scheduler, BigQuery, mathplotlib and SendGrid Please add links to your externally hosted Knative code sample.","title":"External code samples"},{"location":"uninstall/","text":"Uninstalling Knative \u00b6 To uninstall an Operator-based Knative installation, follow the Uninstall an Operator-based Knative Installation procedure below. To uninstall a YAML-based Knative installation, follow the Uninstall a YAML-based Knative Installation procedure below. Uninstalling a YAML-based Knative installation \u00b6 To uninstall a YAML-based Knative installation: Uninstalling optional Serving extensions \u00b6 Uninstall any Serving extensions you have installed by following the relevant steps below: HPA autoscaling Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will uninstall the components needed to support HPA-class autoscaling: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml TLS with cert-manager Uninstall the component that integrates Knative with cert-manager: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Optional: if you no longer need cert-manager, uninstall it by following the steps in the cert-manager documentation . TLS via HTTP01 Uninstall the net-http01 controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml TLS wildcard support Uninstall the components needed to provision wildcard certificates in each namespace by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-nscert.yaml DomainMapping CRD To uninstall the DomainMapping components run: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-domainmapping.yaml kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-domainmapping-crds.yaml Uninstalling a networking layer \u00b6 Follow the relevant procedure to uninstall the networking layer you installed: Ambassador The following commands uninstall Ambassador and enable its Knative integration. Uninstall Ambassador by running: kubectl delete --namespace ambassador \\ -f https://getambassador.io/yaml/ambassador/ambassador-crds.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-service.yaml Delete the Ambassador namespace: kubectl delete namespace ambassador Contour The following commands uninstall Contour and enable its Knative integration. Uninstall the Knative Contour controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Uninstall Contour: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml Gloo Uninstall Gloo and the Knative integration by running: glooctl uninstall knative Istio The following commands uninstall Istio and enable its Knative integration. Uninstall the Knative Istio controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Optional: if you no longer need Istio, uninstall it by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Kong Uninstall Kong Ingress Controller by running: kubectl delete -f https://raw.githubusercontent.com/Kong/kubernetes-ingress-controller/0.9.x/deploy/single/all-in-one-dbless.yaml Kourier Uninstall the Knative Kourier controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml Uninstalling the Serving component \u00b6 Uninstall the Serving core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Uninstalling optional Eventing extensions \u00b6 Uninstall any Eventing extensions you have installed by following the relevant procedure below: Apache Kafka Sink Uninstall the Kafka Sink data plane: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Uninstall the Kafka controller: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Sugar Controller Uninstall the Eventing Sugar Controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml GitHub Source Uninstall a single-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-github/latest/github.yaml Uninstall a multi-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-github/latest/mt-github.yaml Apache Camel-K Source Uninstall the Apache Camel-K Source by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-camel/latest/camel.yaml Apache Kafka Source Uninstall the Apache Kafka source by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka/latest/source.yaml GCP Sources Uninstall the GCP sources by running: kubectl delete -f https://storage.googleapis.com/google-nightly/knative-gcp/latest/cloud-run-events.yaml Apache CouchDB Source Uninstall the Apache CouchDB source by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-couchdb/latest/couchdb.yaml VMware Sources and Bindings Uninstall the VMware sources and bindings by running: kubectl delete -f https://storage.googleapis.com/vmware-tanzu-nightly/sources-for-knative/latest/release.yaml Uninstalling an optional Broker (Eventing) layer \u00b6 Uninstall a Broker (Eventing) layer, if you installed one: Apache Kafka Broker Uninstall the Kafka Broker data plane by running the following command: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Uninstall the Kafka controller by running the following command: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml MT-Channel-based Uninstall the broker by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml Uninstalling optional channel (messaging) layers \u00b6 Uninstall each channel layer you have installed: Apache Kafka Channel Uninstall the Apache Kafka Channel by running: curl -L \"https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka/latest/channel-consolidated.yaml\" \\ | sed 's/REPLACE_WITH_CLUSTER_URL/my-cluster-kafka-bootstrap.kafka:9092/' \\ | kubectl delete -f - Google Cloud Pub/Sub Channel Uninstall the Google Cloud Pub/Sub Channel by running: kubectl delete -f https://storage.googleapis.com/google-nightly/knative-gcp/latest/cloud-run-events.yaml In-Memory (standalone) Uninstall the in-memory channel implementation by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml NATS Channel Uninstall the NATS Streaming channel by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-natss/latest/300-natss-channel.yaml Uninstall NATS Streaming for Kubernetes. For more information, see the eventing-natss repository in GitHub. Uninstalling the Eventing component \u00b6 Uninstall the Eventing core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Uninstall an Operator-based Knative installation \u00b6 To uninstall an Operator-based Knative installation, follow the procedures below. Removing the Knative Serving component \u00b6 Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving Removing Knative Eventing component \u00b6 Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work. Removing the Knative Operator: \u00b6 If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"Uninstalling Knative"},{"location":"uninstall/#uninstalling-knative","text":"To uninstall an Operator-based Knative installation, follow the Uninstall an Operator-based Knative Installation procedure below. To uninstall a YAML-based Knative installation, follow the Uninstall a YAML-based Knative Installation procedure below.","title":"Uninstalling Knative"},{"location":"uninstall/#uninstalling-a-yaml-based-knative-installation","text":"To uninstall a YAML-based Knative installation:","title":"Uninstalling a YAML-based Knative installation"},{"location":"uninstall/#uninstalling-optional-serving-extensions","text":"Uninstall any Serving extensions you have installed by following the relevant steps below: HPA autoscaling Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will uninstall the components needed to support HPA-class autoscaling: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml TLS with cert-manager Uninstall the component that integrates Knative with cert-manager: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Optional: if you no longer need cert-manager, uninstall it by following the steps in the cert-manager documentation . TLS via HTTP01 Uninstall the net-http01 controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml TLS wildcard support Uninstall the components needed to provision wildcard certificates in each namespace by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-nscert.yaml DomainMapping CRD To uninstall the DomainMapping components run: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-domainmapping.yaml kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-domainmapping-crds.yaml","title":"Uninstalling optional Serving extensions"},{"location":"uninstall/#uninstalling-a-networking-layer","text":"Follow the relevant procedure to uninstall the networking layer you installed: Ambassador The following commands uninstall Ambassador and enable its Knative integration. Uninstall Ambassador by running: kubectl delete --namespace ambassador \\ -f https://getambassador.io/yaml/ambassador/ambassador-crds.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-service.yaml Delete the Ambassador namespace: kubectl delete namespace ambassador Contour The following commands uninstall Contour and enable its Knative integration. Uninstall the Knative Contour controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Uninstall Contour: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml Gloo Uninstall Gloo and the Knative integration by running: glooctl uninstall knative Istio The following commands uninstall Istio and enable its Knative integration. Uninstall the Knative Istio controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Optional: if you no longer need Istio, uninstall it by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Kong Uninstall Kong Ingress Controller by running: kubectl delete -f https://raw.githubusercontent.com/Kong/kubernetes-ingress-controller/0.9.x/deploy/single/all-in-one-dbless.yaml Kourier Uninstall the Knative Kourier controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml","title":"Uninstalling a networking layer"},{"location":"uninstall/#uninstalling-the-serving-component","text":"Uninstall the Serving core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml","title":"Uninstalling the Serving component"},{"location":"uninstall/#uninstalling-optional-eventing-extensions","text":"Uninstall any Eventing extensions you have installed by following the relevant procedure below: Apache Kafka Sink Uninstall the Kafka Sink data plane: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Uninstall the Kafka controller: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Sugar Controller Uninstall the Eventing Sugar Controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml GitHub Source Uninstall a single-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-github/latest/github.yaml Uninstall a multi-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-github/latest/mt-github.yaml Apache Camel-K Source Uninstall the Apache Camel-K Source by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-camel/latest/camel.yaml Apache Kafka Source Uninstall the Apache Kafka source by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka/latest/source.yaml GCP Sources Uninstall the GCP sources by running: kubectl delete -f https://storage.googleapis.com/google-nightly/knative-gcp/latest/cloud-run-events.yaml Apache CouchDB Source Uninstall the Apache CouchDB source by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-couchdb/latest/couchdb.yaml VMware Sources and Bindings Uninstall the VMware sources and bindings by running: kubectl delete -f https://storage.googleapis.com/vmware-tanzu-nightly/sources-for-knative/latest/release.yaml","title":"Uninstalling optional Eventing extensions"},{"location":"uninstall/#uninstalling-an-optional-broker-eventing-layer","text":"Uninstall a Broker (Eventing) layer, if you installed one: Apache Kafka Broker Uninstall the Kafka Broker data plane by running the following command: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Uninstall the Kafka controller by running the following command: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml MT-Channel-based Uninstall the broker by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml","title":"Uninstalling an optional Broker (Eventing) layer"},{"location":"uninstall/#uninstalling-optional-channel-messaging-layers","text":"Uninstall each channel layer you have installed: Apache Kafka Channel Uninstall the Apache Kafka Channel by running: curl -L \"https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka/latest/channel-consolidated.yaml\" \\ | sed 's/REPLACE_WITH_CLUSTER_URL/my-cluster-kafka-bootstrap.kafka:9092/' \\ | kubectl delete -f - Google Cloud Pub/Sub Channel Uninstall the Google Cloud Pub/Sub Channel by running: kubectl delete -f https://storage.googleapis.com/google-nightly/knative-gcp/latest/cloud-run-events.yaml In-Memory (standalone) Uninstall the in-memory channel implementation by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml NATS Channel Uninstall the NATS Streaming channel by running: kubectl delete -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-natss/latest/300-natss-channel.yaml Uninstall NATS Streaming for Kubernetes. For more information, see the eventing-natss repository in GitHub.","title":"Uninstalling optional channel (messaging) layers"},{"location":"uninstall/#uninstalling-the-eventing-component","text":"Uninstall the Eventing core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml","title":"Uninstalling the Eventing component"},{"location":"uninstall/#uninstall-an-operator-based-knative-installation","text":"To uninstall an Operator-based Knative installation, follow the procedures below.","title":"Uninstall an Operator-based Knative installation"},{"location":"uninstall/#removing-the-knative-serving-component","text":"Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving","title":"Removing the Knative Serving component"},{"location":"uninstall/#removing-knative-eventing-component","text":"Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.","title":"Removing Knative Eventing component"},{"location":"uninstall/#removing-the-knative-operator","text":"If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"Removing the Knative Operator:"},{"location":"client/","text":"The following CLI tools are supported for use with Knative. kubectl \u00b6 You can use kubectl to apply the YAML files required to install Knative components, and also to create Knative resources, such as services and event sources using YAML. See Install and Set Up kubectl . kn \u00b6 kn provides a quick and easy interface for creating Knative resources such as services and event sources, without the need to create or modify YAML files directly. kn also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting. NOTE: kn cannot be used to install Knative components such as Serving or Eventing. See Installing kn . Connecting CLI tools to your cluster \u00b6 After you have installed kubectl or kn , these tools will search for the kubeconfig file of your cluster in the default location of $HOME/.kube/config , and will use this file to connect to the cluster. A kubeconfig file is usually automatically created when you create a Kubernetes cluster. For more information about kubeconfig files, see Organizing Cluster Access Using kubeconfig Files . Using kubeconfig files with your platform \u00b6 Instructions for using kubeconfig files are available for the following platforms: Amazon EKS Google GKE IBM IKS Red Hat OpenShift Cloud Platform Starting minikube writes this file automatically, or provides an appropriate context in an existing configuration file.","title":"CLI tools"},{"location":"client/#kubectl","text":"You can use kubectl to apply the YAML files required to install Knative components, and also to create Knative resources, such as services and event sources using YAML. See Install and Set Up kubectl .","title":"kubectl"},{"location":"client/#kn","text":"kn provides a quick and easy interface for creating Knative resources such as services and event sources, without the need to create or modify YAML files directly. kn also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting. NOTE: kn cannot be used to install Knative components such as Serving or Eventing. See Installing kn .","title":"kn"},{"location":"client/#connecting-cli-tools-to-your-cluster","text":"After you have installed kubectl or kn , these tools will search for the kubeconfig file of your cluster in the default location of $HOME/.kube/config , and will use this file to connect to the cluster. A kubeconfig file is usually automatically created when you create a Kubernetes cluster. For more information about kubeconfig files, see Organizing Cluster Access Using kubeconfig Files .","title":"Connecting CLI tools to your cluster"},{"location":"client/#using-kubeconfig-files-with-your-platform","text":"Instructions for using kubeconfig files are available for the following platforms: Amazon EKS Google GKE IBM IKS Red Hat OpenShift Cloud Platform Starting minikube writes this file automatically, or provides an appropriate context in an existing configuration file.","title":"Using kubeconfig files with your platform"},{"location":"client/configure-kn/","text":"Customizing kn \u00b6 You can customize your kn CLI setup by creating a config.yaml configuration file. You can provide this configuration by using the --config flag, otherwise the configuration is picked up from a default location. The default configuration location conforms to the XDG Base Directory Specification , and is different for Unix systems and Windows systems. If the XDG_CONFIG_HOME environment variable is set, the default configuration location that kn looks for is $XDG_CONFIG_HOME/kn . If the XDG_CONFIG_HOME environment variable is not set, kn looks for the configuration in the home directory of the user at $HOME/.config/kn/config.yaml . For Windows systems, the default kn configuration location is %APPDATA%\\kn . Example configuration file \u00b6 plugins : path-lookup : true directory : ~/.config/kn/plugins eventing : sink-mappings : - prefix : svc group : core version : v1 resource : services Where path-lookup specifies whether kn should look for plugins in the PATH environment variable. This is a boolean configuration option. The default value is false . directory specifies the directory where kn will look for plugins. The default path depends on the operating system, as described above. This can be any directory that is visible to the user. sink-mappings defines the Kubernetes Addressable resource that is used when you use the --sink flag with a kn CLI command. prefix : The prefix you want to use to describe your sink. Service, svc , channel , and broker are predefined prefixes in kn . group : The API group of the Kubernetes resource. version : The version of the Kubernetes resource. resource : The plural name of the Kubernetes resource type. For example, services or brokers .","title":"Customizing kn"},{"location":"client/configure-kn/#customizing-kn","text":"You can customize your kn CLI setup by creating a config.yaml configuration file. You can provide this configuration by using the --config flag, otherwise the configuration is picked up from a default location. The default configuration location conforms to the XDG Base Directory Specification , and is different for Unix systems and Windows systems. If the XDG_CONFIG_HOME environment variable is set, the default configuration location that kn looks for is $XDG_CONFIG_HOME/kn . If the XDG_CONFIG_HOME environment variable is not set, kn looks for the configuration in the home directory of the user at $HOME/.config/kn/config.yaml . For Windows systems, the default kn configuration location is %APPDATA%\\kn .","title":"Customizing kn"},{"location":"client/configure-kn/#example-configuration-file","text":"plugins : path-lookup : true directory : ~/.config/kn/plugins eventing : sink-mappings : - prefix : svc group : core version : v1 resource : services Where path-lookup specifies whether kn should look for plugins in the PATH environment variable. This is a boolean configuration option. The default value is false . directory specifies the directory where kn will look for plugins. The default path depends on the operating system, as described above. This can be any directory that is visible to the user. sink-mappings defines the Kubernetes Addressable resource that is used when you use the --sink flag with a kn CLI command. prefix : The prefix you want to use to describe your sink. Service, svc , channel , and broker are predefined prefixes in kn . group : The API group of the Kubernetes resource. version : The version of the Kubernetes resource. resource : The plural name of the Kubernetes resource type. For example, services or brokers .","title":"Example configuration file"},{"location":"client/install-kn/","text":"Installing kn \u00b6 This guide provides details about how you can install the Knative kn CLI. Install kn using brew \u00b6 For macOS, you can install kn by using Homebrew . brew install knative/client/kn Install kn using a binary \u00b6 You can install kn by downloading the executable binary for your system and placing it in the system path. A link to the latest stable binary release is available on the kn release page . Install kn using the nightly-built binary \u00b6 Nightly-built executable binaries are available for users who want to install the latest pre-release build of kn . WARNING: Nightly-built executable binaries include features which may not be included in the latest Knative release and are not considered to be stable. Links to the latest nightly-built executable binaries are available here: macOS Linux Windows Install kn using Go \u00b6 Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Running kn using container images \u00b6 WARNING: Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable. Links to images are available here: Latest release Nightly container image You can run kn from a container image. For example: docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list NOTE: Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn . Using kn with Tekton \u00b6 See the Tekton documentation .","title":"Install kn"},{"location":"client/install-kn/#installing-kn","text":"This guide provides details about how you can install the Knative kn CLI.","title":"Installing kn"},{"location":"client/install-kn/#install-kn-using-brew","text":"For macOS, you can install kn by using Homebrew . brew install knative/client/kn","title":"Install kn using brew"},{"location":"client/install-kn/#install-kn-using-a-binary","text":"You can install kn by downloading the executable binary for your system and placing it in the system path. A link to the latest stable binary release is available on the kn release page .","title":"Install kn using a binary"},{"location":"client/install-kn/#install-kn-using-the-nightly-built-binary","text":"Nightly-built executable binaries are available for users who want to install the latest pre-release build of kn . WARNING: Nightly-built executable binaries include features which may not be included in the latest Knative release and are not considered to be stable. Links to the latest nightly-built executable binaries are available here: macOS Linux Windows","title":"Install kn using the nightly-built binary"},{"location":"client/install-kn/#install-kn-using-go","text":"Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version","title":"Install kn using Go"},{"location":"client/install-kn/#running-kn-using-container-images","text":"WARNING: Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable. Links to images are available here: Latest release Nightly container image You can run kn from a container image. For example: docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list NOTE: Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn .","title":"Running kn using container images"},{"location":"client/install-kn/#using-kn-with-tekton","text":"See the Tekton documentation .","title":"Using kn with Tekton"},{"location":"client/kn-plugins/","text":"kn plugins \u00b6 The kn CLI supports the use of plugins. Plugins enable you to extend the functionality of your kn installation by adding custom commands and other shared commands that are not part of the core distribution of kn .","title":"kn plugins"},{"location":"client/kn-plugins/#kn-plugins","text":"The kn CLI supports the use of plugins. Plugins enable you to extend the functionality of your kn installation by adding custom commands and other shared commands that are not part of the core distribution of kn .","title":"kn plugins"},{"location":"eventing/","text":"Knative Eventing \u00b6 Knative Eventing is a system that is designed to address a common need for cloud native development and provides composable primitives to enable late-binding event sources and event consumers. Functionality \u00b6 Knative Eventing supports multiple modes of usage. The following scenarios are well-supported by the existing components; since the system is modular, it's also possible to combine the components in novel ways. I just want to publish events, I don't care who consumes them. Send events to a broker as an HTTP POST. Sink binding can be useful to decouple the destination configuration from your application. I just want to consume events like X, I don't care how they are published. Use a trigger to consume events from a Broker based on CloudEvents attributes. Your application will receive the events as an HTTP POST. I want to transform events through a series of steps. Use channels and subscriptions to define complex message-passing topologies. For simple pipelines, the Sequence automates construction of channels and subscriptions between each stage. Knative also supports some additional patterns such as Parallel fanout of events, and routing response events from both Channels and Brokers. Design overview \u00b6 Knative Eventing is designed around the following goals: The Knative Eventing resources are loosely coupled. These resources can be developed and deployed independently on, and across a variety of platforms (for example Kubernetes, VMs, SaaS or FaaS). Event producers and event consumers are independent. Any producer (or source), can generate events before there are active event consumers that are listening. Any event consumer can express interest in an event or class of events, before there are producers that are creating those events. Other services can be connected to the Eventing system. These services can perform the following functions: Create new applications without modifying the event producer or event consumer. Select and target specific subsets of the events from their producers. Ensure cross-service interoperability. Knative Eventing is consistent with the CloudEvents specification that is developed by the CNCF Serverless WG . Event consumers \u00b6 To enable delivery to multiple types of Services, Knative Eventing defines two generic interfaces that can be implemented by multiple Kubernetes resources: Addressable objects are able to receive and acknowledge an event delivered over HTTP to an address defined in their status.address.url field. As a special case, the core Kubernetes Service object also fulfils the Addressable interface. Callable objects are able to receive an event delivered over HTTP and transform the event, returning 0 or 1 new events in the HTTP response. These returned events may be further processed in the same way that events from an external event source are processed. Event registry \u00b6 Knative Eventing defines an EventType object to make it easier for consumers to discover the types of events they can consume from Brokers. The registry consists of a collection of event types. The event types stored in the registry contain (all) the required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism. To learn how to use the registry, see the Event Registry documentation . Higher Level eventing constructs \u00b6 There are cases where you may want to utilize a set of co-operating functions together and for those use cases, Knative Eventing provides two additional resources: Sequence provides a way to define an in-order list of functions. Parallel provides a way to define a list of branches for events. Observability \u00b6 Eventing Metrics API","title":"Overview"},{"location":"eventing/#knative-eventing","text":"Knative Eventing is a system that is designed to address a common need for cloud native development and provides composable primitives to enable late-binding event sources and event consumers.","title":"Knative Eventing"},{"location":"eventing/#functionality","text":"Knative Eventing supports multiple modes of usage. The following scenarios are well-supported by the existing components; since the system is modular, it's also possible to combine the components in novel ways. I just want to publish events, I don't care who consumes them. Send events to a broker as an HTTP POST. Sink binding can be useful to decouple the destination configuration from your application. I just want to consume events like X, I don't care how they are published. Use a trigger to consume events from a Broker based on CloudEvents attributes. Your application will receive the events as an HTTP POST. I want to transform events through a series of steps. Use channels and subscriptions to define complex message-passing topologies. For simple pipelines, the Sequence automates construction of channels and subscriptions between each stage. Knative also supports some additional patterns such as Parallel fanout of events, and routing response events from both Channels and Brokers.","title":"Functionality"},{"location":"eventing/#design-overview","text":"Knative Eventing is designed around the following goals: The Knative Eventing resources are loosely coupled. These resources can be developed and deployed independently on, and across a variety of platforms (for example Kubernetes, VMs, SaaS or FaaS). Event producers and event consumers are independent. Any producer (or source), can generate events before there are active event consumers that are listening. Any event consumer can express interest in an event or class of events, before there are producers that are creating those events. Other services can be connected to the Eventing system. These services can perform the following functions: Create new applications without modifying the event producer or event consumer. Select and target specific subsets of the events from their producers. Ensure cross-service interoperability. Knative Eventing is consistent with the CloudEvents specification that is developed by the CNCF Serverless WG .","title":"Design overview"},{"location":"eventing/#event-consumers","text":"To enable delivery to multiple types of Services, Knative Eventing defines two generic interfaces that can be implemented by multiple Kubernetes resources: Addressable objects are able to receive and acknowledge an event delivered over HTTP to an address defined in their status.address.url field. As a special case, the core Kubernetes Service object also fulfils the Addressable interface. Callable objects are able to receive an event delivered over HTTP and transform the event, returning 0 or 1 new events in the HTTP response. These returned events may be further processed in the same way that events from an external event source are processed.","title":"Event consumers"},{"location":"eventing/#event-registry","text":"Knative Eventing defines an EventType object to make it easier for consumers to discover the types of events they can consume from Brokers. The registry consists of a collection of event types. The event types stored in the registry contain (all) the required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism. To learn how to use the registry, see the Event Registry documentation .","title":"Event registry"},{"location":"eventing/#higher-level-eventing-constructs","text":"There are cases where you may want to utilize a set of co-operating functions together and for those use cases, Knative Eventing provides two additional resources: Sequence provides a way to define an in-order list of functions. Parallel provides a way to define a list of branches for events.","title":"Higher Level eventing constructs"},{"location":"eventing/#observability","text":"Eventing Metrics API","title":"Observability"},{"location":"eventing/accessing-traces/","text":"Accessing CloudEvent traces \u00b6 Depending on the request tracing tool that you have installed on your Knative Eventing cluster, see the corresponding section for details about how to visualize and trace your requests. Before you begin \u00b6 You must have a Knative cluster running with the Eventing component installed. Learn more Configuring tracing \u00b6 With the exception of importers, the Knative Eventing tracing is configured through the config-tracing ConfigMap in the knative-eventing namespace. Most importers do not use the ConfigMap and instead, use a static 1% sampling rate. You can use the config-tracing ConfigMap to configure the following Eventing components: - Brokers - Triggers - InMemoryChannel - ApiServerSource - PingSource - GitlabSource - KafkaSource - PrometheusSource Example: The following example config-tracing ConfigMap samples 10% of all CloudEvents: apiVersion : v1 kind : ConfigMap metadata : name : config-tracing namespace : knative-eventing data : enable : \"true\" zipkin-endpoint : \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\" sample-rate : \"0.1\" Configuration options \u00b6 You can configure your config-tracing with following options: backend : Valid values are zipkin , stackdriver , or none . The default is none . zipkin-endpoint : Specifies the URL to the zipkin collector where you want to send the traces. Must be set if backend is set to zipkin . stackdriver-project-id : Specifies the GCP project ID into which the Stackdriver traces are written. You must specify the backend as stackdriver . If backend is unspecified, the GCP project ID is read from GCP metadata when running on GCP. sample-rate : Specifies the sampling rate. Valid values are decimals from 0 to 1 (interpreted as a float64), which indicate the probability that any given request is sampled. An example value is 0.5 , which gives each request a 50% sampling probablity. debug : Enables debugging. Valid values are true or false . Defaults to false when not specified. Set to true to enable debug mode, which forces the sample-rate to 1.0 and sends all spans to the server. Viewing your config-tracing ConfigMap \u00b6 To view your current configuration: kubectl -n knative-eventing get configmap config-tracing -oyaml Editing and deploying your config-tracing ConfigMap \u00b6 To edit and then immediately deploy changes to your ConfigMap, run the following command: kubectl -n knative-eventing edit configmap config-tracing Accessing traces in Eventing \u00b6 To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section: Zipkin Jaeger Example \u00b6 The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the TestBrokerTracing End-to-End test. For this example, assume the following details: - Everything happens in the includes-incoming-trace-id-2qszn namespace. - The Broker is named br . - There are two Triggers that are associated with the Broker: - transformer - Filters to only allow events whose type is transformer . Sends the event to the Kubernetes Service transformer , which will reply with an identical event, except the replied event's type will be logger . - logger - Filters to only allow events whose type is logger . Sends the event to the Kubernetes Service logger . - An event is sent to the Broker with the type transformer , by the Pod named sender . Given the above, the expected path and behavior of an event is as follows: sender Pod sends the request to the Broker. Go to the Broker's ingress Pod. Go to the imc-dispatcher Channel (imc stands for InMemoryChannel). Go to both Triggers. Go to the Broker's filter Pod for the Trigger logger . The Trigger's filter ignores this event. Go to the Broker's filter Pod for the Trigger transformer . The filter does pass, so it goes to the Kubernetes Service pointed at, also named transformer . transformer Pod replies with the modified event. Go to an InMemory dispatcher. Go to the Broker's ingress Pod. Go to the InMemory dispatcher. Go to both Triggers. Go to the Broker's filter Pod for the Trigger transformer . The Trigger's filter ignores the event. Go to the Broker's filter Pod for the Trigger logger . The filter passes. Go to the logger Pod. There is no reply. This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section: This is the same screenshot without the annotations. If you are interested, here is the raw JSON of the trace.","title":"Accessing CloudEvent traces"},{"location":"eventing/accessing-traces/#accessing-cloudevent-traces","text":"Depending on the request tracing tool that you have installed on your Knative Eventing cluster, see the corresponding section for details about how to visualize and trace your requests.","title":"Accessing CloudEvent traces"},{"location":"eventing/accessing-traces/#before-you-begin","text":"You must have a Knative cluster running with the Eventing component installed. Learn more","title":"Before you begin"},{"location":"eventing/accessing-traces/#configuring-tracing","text":"With the exception of importers, the Knative Eventing tracing is configured through the config-tracing ConfigMap in the knative-eventing namespace. Most importers do not use the ConfigMap and instead, use a static 1% sampling rate. You can use the config-tracing ConfigMap to configure the following Eventing components: - Brokers - Triggers - InMemoryChannel - ApiServerSource - PingSource - GitlabSource - KafkaSource - PrometheusSource Example: The following example config-tracing ConfigMap samples 10% of all CloudEvents: apiVersion : v1 kind : ConfigMap metadata : name : config-tracing namespace : knative-eventing data : enable : \"true\" zipkin-endpoint : \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\" sample-rate : \"0.1\"","title":"Configuring tracing"},{"location":"eventing/accessing-traces/#configuration-options","text":"You can configure your config-tracing with following options: backend : Valid values are zipkin , stackdriver , or none . The default is none . zipkin-endpoint : Specifies the URL to the zipkin collector where you want to send the traces. Must be set if backend is set to zipkin . stackdriver-project-id : Specifies the GCP project ID into which the Stackdriver traces are written. You must specify the backend as stackdriver . If backend is unspecified, the GCP project ID is read from GCP metadata when running on GCP. sample-rate : Specifies the sampling rate. Valid values are decimals from 0 to 1 (interpreted as a float64), which indicate the probability that any given request is sampled. An example value is 0.5 , which gives each request a 50% sampling probablity. debug : Enables debugging. Valid values are true or false . Defaults to false when not specified. Set to true to enable debug mode, which forces the sample-rate to 1.0 and sends all spans to the server.","title":"Configuration options"},{"location":"eventing/accessing-traces/#viewing-your-config-tracing-configmap","text":"To view your current configuration: kubectl -n knative-eventing get configmap config-tracing -oyaml","title":"Viewing your config-tracing ConfigMap"},{"location":"eventing/accessing-traces/#editing-and-deploying-your-config-tracing-configmap","text":"To edit and then immediately deploy changes to your ConfigMap, run the following command: kubectl -n knative-eventing edit configmap config-tracing","title":"Editing and deploying your config-tracing ConfigMap"},{"location":"eventing/accessing-traces/#accessing-traces-in-eventing","text":"To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section: Zipkin Jaeger","title":"Accessing traces in Eventing"},{"location":"eventing/accessing-traces/#example","text":"The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the TestBrokerTracing End-to-End test. For this example, assume the following details: - Everything happens in the includes-incoming-trace-id-2qszn namespace. - The Broker is named br . - There are two Triggers that are associated with the Broker: - transformer - Filters to only allow events whose type is transformer . Sends the event to the Kubernetes Service transformer , which will reply with an identical event, except the replied event's type will be logger . - logger - Filters to only allow events whose type is logger . Sends the event to the Kubernetes Service logger . - An event is sent to the Broker with the type transformer , by the Pod named sender . Given the above, the expected path and behavior of an event is as follows: sender Pod sends the request to the Broker. Go to the Broker's ingress Pod. Go to the imc-dispatcher Channel (imc stands for InMemoryChannel). Go to both Triggers. Go to the Broker's filter Pod for the Trigger logger . The Trigger's filter ignores this event. Go to the Broker's filter Pod for the Trigger transformer . The filter does pass, so it goes to the Kubernetes Service pointed at, also named transformer . transformer Pod replies with the modified event. Go to an InMemory dispatcher. Go to the Broker's ingress Pod. Go to the InMemory dispatcher. Go to both Triggers. Go to the Broker's filter Pod for the Trigger transformer . The Trigger's filter ignores the event. Go to the Broker's filter Pod for the Trigger logger . The filter passes. Go to the logger Pod. There is no reply. This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section: This is the same screenshot without the annotations. If you are interested, here is the raw JSON of the trace.","title":"Example"},{"location":"eventing/event-delivery/","text":"Event delivery \u00b6 You can configure event delivery parameters for Knative Eventing components that are applied in cases where an event fails to be delivered Configuring subscription event delivery \u00b6 You can configure how events are delivered for each subscription by adding a delivery spec to the Subscription object, as shown in the following example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be a Knative service or a URI. In the example, the destination is a Service object, or Knative service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential back off policy, the back off delay is equal to backoffDelay*2^<numberOfRetries> . retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink. Broker event delivery \u00b6 See the broker documentation. Channel event delivery \u00b6 Failed events may, depending on the specific Channel implementation in use, be enhanced with extension attributes prior to forwarding to the deadLetterSink . These extension attributes are as follows: knativeerrorcode Type: Int Description: The HTTP Response StatusCode from the final event dispatch attempt. Constraints: Should always be present as every HTTP Response contains a StatusCode . Examples: \"500\" ...any HTTP StatusCode... knativeerrordata Type: String Description: The HTTP Response Body from the final event dispatch attempt. Constraints: Will be empty if the HTTP Response Body was empty, and might be truncated if the length is excessive. Examples: 'Internal Server Error: Failed to process event.' '{\"key\": \"value\"}' ...any HTTP Response Body... Channel Support \u00b6 The table below summarizes what delivery parameters are supported for each channel implementation. Channel Type Supported Delivery Parameters GCP PubSub none In-Memory deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay Natss none","title":"Event delivery"},{"location":"eventing/event-delivery/#event-delivery","text":"You can configure event delivery parameters for Knative Eventing components that are applied in cases where an event fails to be delivered","title":"Event delivery"},{"location":"eventing/event-delivery/#configuring-subscription-event-delivery","text":"You can configure how events are delivered for each subscription by adding a delivery spec to the Subscription object, as shown in the following example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be a Knative service or a URI. In the example, the destination is a Service object, or Knative service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential back off policy, the back off delay is equal to backoffDelay*2^<numberOfRetries> . retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink.","title":"Configuring subscription event delivery"},{"location":"eventing/event-delivery/#broker-event-delivery","text":"See the broker documentation.","title":"Broker event delivery"},{"location":"eventing/event-delivery/#channel-event-delivery","text":"Failed events may, depending on the specific Channel implementation in use, be enhanced with extension attributes prior to forwarding to the deadLetterSink . These extension attributes are as follows: knativeerrorcode Type: Int Description: The HTTP Response StatusCode from the final event dispatch attempt. Constraints: Should always be present as every HTTP Response contains a StatusCode . Examples: \"500\" ...any HTTP StatusCode... knativeerrordata Type: String Description: The HTTP Response Body from the final event dispatch attempt. Constraints: Will be empty if the HTTP Response Body was empty, and might be truncated if the length is excessive. Examples: 'Internal Server Error: Failed to process event.' '{\"key\": \"value\"}' ...any HTTP Response Body...","title":"Channel event delivery"},{"location":"eventing/event-delivery/#channel-support","text":"The table below summarizes what delivery parameters are supported for each channel implementation. Channel Type Supported Delivery Parameters GCP PubSub none In-Memory deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay Natss none","title":"Channel Support"},{"location":"eventing/event-registry/","text":"Event registry \u00b6 Overview \u00b6 The event registry maintains a catalog of event types that can be consumed from different brokers. It introduces the EventType custom resource in order to persist the event type information in the cluster data store. Before you begin \u00b6 Read about the broker and trigger objects. Be familiar with the CloudEvents spec , particularly the Context Attributes section. Be familiar with event sources . Discovering events with the registry \u00b6 Using the registry, you can discover different types of events that can be consumed by broker event meshes. The registry is designed for use with the broker and trigger model, and aims to help you create triggers. To see event types in the registry that are available to subscribe to, enter the following command: kubectl get eventtypes -n <namespace> Below, we show an example output of executing the above command using the default namespace in a testing cluster. We will address the question of how this registry was populated in a later section. NAME TYPE SOURCE SCHEMA BROKER DESCRIPTION READY REASON dev.knative.source.github.push-34cnb dev.knative.source.github.push https://github.com/knative/eventing default True dev.knative.source.github.push-44svn dev.knative.source.github.push https://github.com/knative/serving default True dev.knative.source.github.pullrequest-86jhv dev.knative.source.github.pull_request https://github.com/knative/eventing default True dev.knative.source.github.pullrequest-97shf dev.knative.source.github.pull_request https://github.com/knative/serving default True dev.knative.kafka.event-cjvcr dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#news default True dev.knative.kafka.event-tdt48 dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo default True google.pubsub.topic.publish-hrxhh google.pubsub.topic.publish //pubsub.googleapis.com/knative/topics/testing dev False BrokerIsNotReady NOTE: This assumes that the event sources emitting the events reference a broker as their sink. There are seven different EventType objects in the registry of the default namespace. Use the following command to see an example of what the YAML for an EventType object looks like: kubectl get eventtype dev.knative.source.github.push-34cnb -o yaml Omitting irrelevant fields: apiVersion : eventing.knative.dev/v1 kind : EventType metadata : name : dev.knative.source.github.push-34cnb namespace : default labels : eventing.knative.dev/sourceName : github-sample spec : type : dev.knative.source.github.push source : https://github.com/knative/eventing schema : description : broker : default status : conditions : - status : \"True\" type : BrokerExists - status : \"True\" type : BrokerReady - status : \"True\" type : Ready From a consumer standpoint, the fields that matter the most are the spec fields as well as the status . The name is advisory (i.e., non-authoritative), and we typically generate it ( generateName ) to avoid naming collisions (e.g., two EventTypes listening to pull requests on two different Github repositories). As name nor generateName are needed for consumers to create Triggers, we defer their discussion for later on. Regarding status , its main purpose it to tell consumers (or cluster operators) whether the EventType is ready for consumption or not. That readiness is based on the Broker being ready. We can see from the example output that the PubSub EventType is not ready, as its dev Broker isn't. Let's talk in more details about the spec fields: type : is authoritative. This refers to the CloudEvent type as it enters into the event mesh. It is mandatory. Event consumers can (and in most cases would) create Triggers filtering on this attribute. source : refers to the CloudEvent source as it enters into the event mesh. It is mandatory. Event consumers can (and in most cases would) create Triggers filtering on this attribute. schema : is a valid URI with the EventType schema. It may be a JSON schema, a protobuf schema, etc. It is optional. description : is a string describing what the EventType is about. It is optional. broker refers to the Broker that can provide the EventType. It is mandatory. Subscribing to events \u00b6 Now that you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events. Here are a few example Triggers that subscribe to events using exact matching on type and/or source , based on the above registry output: Subscribes to GitHub pushes from any source. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : push-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.push subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : push-service As per the registry output above, only two sources exist for that particular type of event ( knative's eventing and serving repositories). If later on new sources are registered for GitHub pushes, this trigger will be able to consume them. Subscribes to GitHub pull requests from knative's eventing repository. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gh-knative-eventing-pull-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.pull_request source : https://github.com/knative/eventing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gh-knative-eventing-pull-service Subscribes to Kafka messages sent to the knative-demo topic apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : kafka-knative-demo-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event source : /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : kafka-knative-demo-service Subscribes to PubSub messages from GCP's knative project sent to the testing topic apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gcp-pubsub-knative-testing-trigger namespace : default spec : broker : dev filter : attributes : source : //pubsub.googleapis.com/knative/topics/testing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gcp-pubsub-knative-testing-service Note that events won't be able to be consumed by this Trigger's subscriber until the Broker becomes ready. Populating the registry \u00b6 Now that we know how to discover events using the registry and how we can leverage that information to subscribe to events of interest, let's move on to the next topic: How do we actually populate the registry in the first place? Manual Registration In order to populate the registry, a cluster configurator can manually register the EventTypes. This means that the configurator can simply apply EventTypes yaml files, just as with any other Kubernetes resource: kubectl apply -f <event_type.yaml> Automatic Registration As Manual Registration might be tedious and error-prone, we also support automatic registration of EventTypes. The creation of the EventTypes is done upon instantiation of an Event Source. We currently support automatic registration of EventTypes for the following Event Sources: CronJobSource ApiServerSource GithubSource GcpPubSubSource KafkaSource AwsSqsSource Let's look at an example, in particular, the KafkaSource sample we used to populate the registry in our testing cluster. Below is what the yaml looks like. apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-sample namespace : default spec : bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - knative-demo - news sink : apiVersion : eventing.knative.dev/v1 kind : Broker name : default If you are interested in more information regarding configuration options of a KafkaSource, please refer to the KafKaSource sample . For this discussion, the relevant information from the yaml above are the sink and the topics . We observe that the sink is of kind Broker . We currently only support automatic creation of EventTypes for Sources instances that point to Brokers. Regarding topics , this is what we use to generate the EventTypes source field, which is equal to the CloudEvent source attribute. When you kubectl apply this yaml, the KafkaSource kafka-source-sample will be instantiated, and two EventTypes will be added to the registry (as there are two topics). You can see that in the registry example output from the previous sections. What's next \u00b6 To get started, install Knative Eventing if you haven't yet, and try experimenting with different Event Sources in your Knative cluster. Installing Knative in case you haven't already done so. Getting started with eventing in case you haven't read it. Knative code samples is a useful resource to better understand some of the Event Sources (remember to point them to a Broker if you want automatic registration of EventTypes in the registry).","title":"Event registry"},{"location":"eventing/event-registry/#event-registry","text":"","title":"Event registry"},{"location":"eventing/event-registry/#overview","text":"The event registry maintains a catalog of event types that can be consumed from different brokers. It introduces the EventType custom resource in order to persist the event type information in the cluster data store.","title":"Overview"},{"location":"eventing/event-registry/#before-you-begin","text":"Read about the broker and trigger objects. Be familiar with the CloudEvents spec , particularly the Context Attributes section. Be familiar with event sources .","title":"Before you begin"},{"location":"eventing/event-registry/#discovering-events-with-the-registry","text":"Using the registry, you can discover different types of events that can be consumed by broker event meshes. The registry is designed for use with the broker and trigger model, and aims to help you create triggers. To see event types in the registry that are available to subscribe to, enter the following command: kubectl get eventtypes -n <namespace> Below, we show an example output of executing the above command using the default namespace in a testing cluster. We will address the question of how this registry was populated in a later section. NAME TYPE SOURCE SCHEMA BROKER DESCRIPTION READY REASON dev.knative.source.github.push-34cnb dev.knative.source.github.push https://github.com/knative/eventing default True dev.knative.source.github.push-44svn dev.knative.source.github.push https://github.com/knative/serving default True dev.knative.source.github.pullrequest-86jhv dev.knative.source.github.pull_request https://github.com/knative/eventing default True dev.knative.source.github.pullrequest-97shf dev.knative.source.github.pull_request https://github.com/knative/serving default True dev.knative.kafka.event-cjvcr dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#news default True dev.knative.kafka.event-tdt48 dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo default True google.pubsub.topic.publish-hrxhh google.pubsub.topic.publish //pubsub.googleapis.com/knative/topics/testing dev False BrokerIsNotReady NOTE: This assumes that the event sources emitting the events reference a broker as their sink. There are seven different EventType objects in the registry of the default namespace. Use the following command to see an example of what the YAML for an EventType object looks like: kubectl get eventtype dev.knative.source.github.push-34cnb -o yaml Omitting irrelevant fields: apiVersion : eventing.knative.dev/v1 kind : EventType metadata : name : dev.knative.source.github.push-34cnb namespace : default labels : eventing.knative.dev/sourceName : github-sample spec : type : dev.knative.source.github.push source : https://github.com/knative/eventing schema : description : broker : default status : conditions : - status : \"True\" type : BrokerExists - status : \"True\" type : BrokerReady - status : \"True\" type : Ready From a consumer standpoint, the fields that matter the most are the spec fields as well as the status . The name is advisory (i.e., non-authoritative), and we typically generate it ( generateName ) to avoid naming collisions (e.g., two EventTypes listening to pull requests on two different Github repositories). As name nor generateName are needed for consumers to create Triggers, we defer their discussion for later on. Regarding status , its main purpose it to tell consumers (or cluster operators) whether the EventType is ready for consumption or not. That readiness is based on the Broker being ready. We can see from the example output that the PubSub EventType is not ready, as its dev Broker isn't. Let's talk in more details about the spec fields: type : is authoritative. This refers to the CloudEvent type as it enters into the event mesh. It is mandatory. Event consumers can (and in most cases would) create Triggers filtering on this attribute. source : refers to the CloudEvent source as it enters into the event mesh. It is mandatory. Event consumers can (and in most cases would) create Triggers filtering on this attribute. schema : is a valid URI with the EventType schema. It may be a JSON schema, a protobuf schema, etc. It is optional. description : is a string describing what the EventType is about. It is optional. broker refers to the Broker that can provide the EventType. It is mandatory.","title":"Discovering events with the registry"},{"location":"eventing/event-registry/#subscribing-to-events","text":"Now that you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events. Here are a few example Triggers that subscribe to events using exact matching on type and/or source , based on the above registry output: Subscribes to GitHub pushes from any source. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : push-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.push subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : push-service As per the registry output above, only two sources exist for that particular type of event ( knative's eventing and serving repositories). If later on new sources are registered for GitHub pushes, this trigger will be able to consume them. Subscribes to GitHub pull requests from knative's eventing repository. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gh-knative-eventing-pull-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.pull_request source : https://github.com/knative/eventing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gh-knative-eventing-pull-service Subscribes to Kafka messages sent to the knative-demo topic apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : kafka-knative-demo-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event source : /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : kafka-knative-demo-service Subscribes to PubSub messages from GCP's knative project sent to the testing topic apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gcp-pubsub-knative-testing-trigger namespace : default spec : broker : dev filter : attributes : source : //pubsub.googleapis.com/knative/topics/testing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gcp-pubsub-knative-testing-service Note that events won't be able to be consumed by this Trigger's subscriber until the Broker becomes ready.","title":"Subscribing to events"},{"location":"eventing/event-registry/#populating-the-registry","text":"Now that we know how to discover events using the registry and how we can leverage that information to subscribe to events of interest, let's move on to the next topic: How do we actually populate the registry in the first place? Manual Registration In order to populate the registry, a cluster configurator can manually register the EventTypes. This means that the configurator can simply apply EventTypes yaml files, just as with any other Kubernetes resource: kubectl apply -f <event_type.yaml> Automatic Registration As Manual Registration might be tedious and error-prone, we also support automatic registration of EventTypes. The creation of the EventTypes is done upon instantiation of an Event Source. We currently support automatic registration of EventTypes for the following Event Sources: CronJobSource ApiServerSource GithubSource GcpPubSubSource KafkaSource AwsSqsSource Let's look at an example, in particular, the KafkaSource sample we used to populate the registry in our testing cluster. Below is what the yaml looks like. apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-sample namespace : default spec : bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - knative-demo - news sink : apiVersion : eventing.knative.dev/v1 kind : Broker name : default If you are interested in more information regarding configuration options of a KafkaSource, please refer to the KafKaSource sample . For this discussion, the relevant information from the yaml above are the sink and the topics . We observe that the sink is of kind Broker . We currently only support automatic creation of EventTypes for Sources instances that point to Brokers. Regarding topics , this is what we use to generate the EventTypes source field, which is equal to the CloudEvent source attribute. When you kubectl apply this yaml, the KafkaSource kafka-source-sample will be instantiated, and two EventTypes will be added to the registry (as there are two topics). You can see that in the registry example output from the previous sections.","title":"Populating the registry"},{"location":"eventing/event-registry/#whats-next","text":"To get started, install Knative Eventing if you haven't yet, and try experimenting with different Event Sources in your Knative cluster. Installing Knative in case you haven't already done so. Getting started with eventing in case you haven't read it. Knative code samples is a useful resource to better understand some of the Event Sources (remember to point them to a Broker if you want automatic registration of EventTypes in the registry).","title":"What's next"},{"location":"eventing/getting-started/","text":"Getting Started with Knative Eventing \u00b6 After you install Knative Eventing, you can create, send, and verify events. This guide shows how you can use a basic workflow for managing events. Before you start to manage events, you must create the objects needed to transport the events. Creating a Knative Eventing namespace \u00b6 Namespaces are used to group together and organize your Knative resources. Create a new namespace called event-example by entering the following command: kubectl create namespace event-example Adding a broker to the namespace \u00b6 The broker allows you to route events to different event sinks or consumers. Add a broker named default to your namespace by entering the following command: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: broker metadata: name: default namespace: event-example EOF Verify that the broker is working correctly, by entering the following command: kubectl -n event-example get broker default This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If READY is False , wait a few moments and then run the command again. If you continue to receive the False status, see the Debugging Guide to troubleshoot the issue. Creating event consumers \u00b6 In this step, you create two event consumers, hello-display and goodbye-display , to demonstrate how you can configure your event producers to target a specific consumer. To deploy the hello-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: hello-display spec: replicas: 1 selector: matchLabels: &labels app: hello-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: hello-display spec: selector: app: hello-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF To deploy the goodbye-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: goodbye-display spec: replicas: 1 selector: matchLabels: &labels app: goodbye-display template: metadata: labels: *labels spec: containers: - name: event-display # Source code: https://github.com/knative/eventing/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: goodbye-display spec: selector: app: goodbye-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Verify that the event consumers are working by entering the following command: kubectl -n event-example get deployments hello-display goodbye-display This lists the hello-display and goodbye-display consumers that you deployed: NAME READY UP-TO-DATE AVAILABLE AGE hello-display 1/1 1 1 26s goodbye-display 1/1 1 1 16s The number of replicas in the READY column should match the number of replicas in the AVAILABLE column. If the numbers do not match, see the Debugging Guide to troubleshoot the issue. Creating triggers \u00b6 A trigger defines the events that each event consumer receives. Brokers use triggers to forward events to the correct consumers. Each trigger can specify a filter that enables selection of relevant events based on the Cloud Event context attributes. Create a trigger by entering the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-display spec: broker: default filter: attributes: type: greeting subscriber: ref: apiVersion: v1 kind: Service name: hello-display EOF The command creates a trigger that sends all events of type greeting to your event consumer named hello-display . To add a second trigger, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: goodbye-display spec: broker: default filter: attributes: source: sendoff subscriber: ref: apiVersion: v1 kind: Service name: goodbye-display EOF The command creates a trigger that sends all events of source sendoff to your event consumer named goodbye-display . Verify that the triggers are working correctly by running the following command: kubectl -n event-example get triggers This returns the hello-display and goodbye-display triggers that you created: NAME READY REASON BROKER SUBSCRIBER_URI AGE goodbye-display True default http://goodbye-display.event-example.svc.cluster.local/ 9s hello-display True default http://hello-display.event-example.svc.cluster.local/ 16s If the triggers are correctly configured, they will be ready and pointing to the correct broker ( default ) and SUBSCRIBER_URI . The SUBSCRIBER_URI has a value similar to triggerName.namespaceName.svc.cluster.local . The exact value depends on the broker implementation. If this value looks incorrect, see the Debugging Guide to troubleshoot the issue. Creating a pod as an event producer \u00b6 This guide uses curl commands to manually send individual events as HTTP requests to the broker, and demonstrate how these events are received by the correct event consumer. The broker can only be accessed from within the cluster where Knative Eventing is installed. You must create a pod within that cluster to act as an event producer that will execute the curl commands. To create a pod, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: v1 kind: Pod metadata: labels: run: curl name: curl spec: containers: # This could be any image that we can SSH into and has curl. - image: radial/busyboxplus:curl imagePullPolicy: IfNotPresent name: curl resources: {} stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true EOF Sending events to the broker \u00b6 SSH into the pod by running the following command: kubectl -n event-example attach curl -it You will see a prompt similar to the following: Defaulting container name to curl. Use 'kubectl describe pod/ -n event-example' to see all of the containers in this pod. If you don't see a command prompt, try pressing enter. [ root@curl:/ ]$ Make a HTTP request to the broker. To show the various types of events you can send, you will make three requests: To make the first request, which creates an event that has the type greeting , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: not-sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative!\"}' When the broker receives your event, hello-display will activate and send it to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the second request, which creates an event that has the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: not-greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Goodbye Knative!\"}' When the broker receives your event, goodbye-display will activate and send the event to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the third request, which creates an event that has the type greeting and the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative! Goodbye Knative!\"}' When the broker receives your event, hello-display and goodbye-display will activate and send the event to the event consumers of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT Exit SSH by typing exit into the command prompt. You have sent two events to the hello-display event consumer and two events to the goodbye-display event consumer (note that say-hello-goodbye activates the trigger conditions for both hello-display and goodbye-display ). You will verify that these events were received correctly in the next section. Verifying that events were received \u00b6 After you send the events, verify that the events were received by the correct subscribers. Look at the logs for the hello-display event consumer by entering the following command: kubectl -n event-example logs -l app=hello-display --tail=100 This returns the Attributes and Data of the events you sent to hello-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: not-sendoff id: say-hello time: 2019-05-20T17:59:43.81718488Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" } Look at the logs for the goodbye-display event consumer by entering the following command: kubectl -n event-example logs -l app=goodbye-display --tail=100 This returns the Attributes and Data of the events you sent to goodbye-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: not-greeting source: sendoff id: say-goodbye time: 2019-05-20T17:59:49.044926148Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Goodbye Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" } Cleaning up example resources \u00b6 You can delete the event-example namespace and its associated resources from your cluster if you do not plan to use it again in the future. Delete the event-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace event-example","title":"Getting started"},{"location":"eventing/getting-started/#getting-started-with-knative-eventing","text":"After you install Knative Eventing, you can create, send, and verify events. This guide shows how you can use a basic workflow for managing events. Before you start to manage events, you must create the objects needed to transport the events.","title":"Getting Started with Knative Eventing"},{"location":"eventing/getting-started/#creating-a-knative-eventing-namespace","text":"Namespaces are used to group together and organize your Knative resources. Create a new namespace called event-example by entering the following command: kubectl create namespace event-example","title":"Creating a Knative Eventing namespace"},{"location":"eventing/getting-started/#adding-a-broker-to-the-namespace","text":"The broker allows you to route events to different event sinks or consumers. Add a broker named default to your namespace by entering the following command: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: broker metadata: name: default namespace: event-example EOF Verify that the broker is working correctly, by entering the following command: kubectl -n event-example get broker default This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If READY is False , wait a few moments and then run the command again. If you continue to receive the False status, see the Debugging Guide to troubleshoot the issue.","title":"Adding a broker to the namespace"},{"location":"eventing/getting-started/#creating-event-consumers","text":"In this step, you create two event consumers, hello-display and goodbye-display , to demonstrate how you can configure your event producers to target a specific consumer. To deploy the hello-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: hello-display spec: replicas: 1 selector: matchLabels: &labels app: hello-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: hello-display spec: selector: app: hello-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF To deploy the goodbye-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: goodbye-display spec: replicas: 1 selector: matchLabels: &labels app: goodbye-display template: metadata: labels: *labels spec: containers: - name: event-display # Source code: https://github.com/knative/eventing/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: goodbye-display spec: selector: app: goodbye-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Verify that the event consumers are working by entering the following command: kubectl -n event-example get deployments hello-display goodbye-display This lists the hello-display and goodbye-display consumers that you deployed: NAME READY UP-TO-DATE AVAILABLE AGE hello-display 1/1 1 1 26s goodbye-display 1/1 1 1 16s The number of replicas in the READY column should match the number of replicas in the AVAILABLE column. If the numbers do not match, see the Debugging Guide to troubleshoot the issue.","title":"Creating event consumers"},{"location":"eventing/getting-started/#creating-triggers","text":"A trigger defines the events that each event consumer receives. Brokers use triggers to forward events to the correct consumers. Each trigger can specify a filter that enables selection of relevant events based on the Cloud Event context attributes. Create a trigger by entering the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-display spec: broker: default filter: attributes: type: greeting subscriber: ref: apiVersion: v1 kind: Service name: hello-display EOF The command creates a trigger that sends all events of type greeting to your event consumer named hello-display . To add a second trigger, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: goodbye-display spec: broker: default filter: attributes: source: sendoff subscriber: ref: apiVersion: v1 kind: Service name: goodbye-display EOF The command creates a trigger that sends all events of source sendoff to your event consumer named goodbye-display . Verify that the triggers are working correctly by running the following command: kubectl -n event-example get triggers This returns the hello-display and goodbye-display triggers that you created: NAME READY REASON BROKER SUBSCRIBER_URI AGE goodbye-display True default http://goodbye-display.event-example.svc.cluster.local/ 9s hello-display True default http://hello-display.event-example.svc.cluster.local/ 16s If the triggers are correctly configured, they will be ready and pointing to the correct broker ( default ) and SUBSCRIBER_URI . The SUBSCRIBER_URI has a value similar to triggerName.namespaceName.svc.cluster.local . The exact value depends on the broker implementation. If this value looks incorrect, see the Debugging Guide to troubleshoot the issue.","title":"Creating triggers"},{"location":"eventing/getting-started/#creating-a-pod-as-an-event-producer","text":"This guide uses curl commands to manually send individual events as HTTP requests to the broker, and demonstrate how these events are received by the correct event consumer. The broker can only be accessed from within the cluster where Knative Eventing is installed. You must create a pod within that cluster to act as an event producer that will execute the curl commands. To create a pod, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: v1 kind: Pod metadata: labels: run: curl name: curl spec: containers: # This could be any image that we can SSH into and has curl. - image: radial/busyboxplus:curl imagePullPolicy: IfNotPresent name: curl resources: {} stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true EOF","title":"Creating a pod as an event producer"},{"location":"eventing/getting-started/#sending-events-to-the-broker","text":"SSH into the pod by running the following command: kubectl -n event-example attach curl -it You will see a prompt similar to the following: Defaulting container name to curl. Use 'kubectl describe pod/ -n event-example' to see all of the containers in this pod. If you don't see a command prompt, try pressing enter. [ root@curl:/ ]$ Make a HTTP request to the broker. To show the various types of events you can send, you will make three requests: To make the first request, which creates an event that has the type greeting , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: not-sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative!\"}' When the broker receives your event, hello-display will activate and send it to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the second request, which creates an event that has the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: not-greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Goodbye Knative!\"}' When the broker receives your event, goodbye-display will activate and send the event to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the third request, which creates an event that has the type greeting and the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative! Goodbye Knative!\"}' When the broker receives your event, hello-display and goodbye-display will activate and send the event to the event consumers of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT Exit SSH by typing exit into the command prompt. You have sent two events to the hello-display event consumer and two events to the goodbye-display event consumer (note that say-hello-goodbye activates the trigger conditions for both hello-display and goodbye-display ). You will verify that these events were received correctly in the next section.","title":"Sending events to the broker"},{"location":"eventing/getting-started/#verifying-that-events-were-received","text":"After you send the events, verify that the events were received by the correct subscribers. Look at the logs for the hello-display event consumer by entering the following command: kubectl -n event-example logs -l app=hello-display --tail=100 This returns the Attributes and Data of the events you sent to hello-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: not-sendoff id: say-hello time: 2019-05-20T17:59:43.81718488Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" } Look at the logs for the goodbye-display event consumer by entering the following command: kubectl -n event-example logs -l app=goodbye-display --tail=100 This returns the Attributes and Data of the events you sent to goodbye-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: not-greeting source: sendoff id: say-goodbye time: 2019-05-20T17:59:49.044926148Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Goodbye Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" }","title":"Verifying that events were received"},{"location":"eventing/getting-started/#cleaning-up-example-resources","text":"You can delete the event-example namespace and its associated resources from your cluster if you do not plan to use it again in the future. Delete the event-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace event-example","title":"Cleaning up example resources"},{"location":"eventing/metrics/","text":"Metrics API \u00b6 NOTE: The metrics API may change in the future, this serves as a snapshot of the current metrics. Admin \u00b6 Administrators can monitor Eventing based on the metrics exposed by each Eventing component. Metrics are listed next. Broker - Ingress \u00b6 Use the following metrics to debug how broker ingress performs and what events are dispacthed via the ingress component. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name event_type namespace_name response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name event_type namespace_name response_code response_code_class unique_name Milliseconds Stable Broker - Filter \u00b6 Use the following metrics to debug how broker filter performs and what events are dispatched via the filter component. Also user can measure the latency of the actual filtering action on an event. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name container_name= filter_type namespace_name response_code response_code_class trigger_name unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name container_name filter_type namespace_name response_code response_code_class trigger_name unique_name Milliseconds Stable event_processing_latencies The time spent processing an event before it is dispatched to a Trigger subscriber Histogram broker_name container_name filter_type namespace_name trigger_name unique_name Milliseconds Stable In-memory Dispatcher \u00b6 In-memory channel can be evaluated via the following metrics. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events dispatched by the in-memory channel Counter container_name event_type= namespace_name= response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event from a in-memory Channel Histogram container_name event_type namespace_name= response_code response_code_class unique_name Milliseconds Stable NOTE: A number of metrics eg. controller, Go runtime and others are omitted here as they are common across most components. For more about these metrics check the Serving metrics API section . Eventing sources \u00b6 Eventing sources are created by users who own the related system, so they can trigger applications with events. Every source exposes by default a number of metrics to help user monitor events dispatched. Use the following metrics to verify that events have been delivered from the source side, thus verifying that the source and any connection with the source work as expected. Metric Name Description Type Tags Unit Status event_count Number of events sent by the source Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable retry_event_count Number of events sent by the source in retries Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable","title":"Metrics API"},{"location":"eventing/metrics/#metrics-api","text":"NOTE: The metrics API may change in the future, this serves as a snapshot of the current metrics.","title":"Metrics API"},{"location":"eventing/metrics/#admin","text":"Administrators can monitor Eventing based on the metrics exposed by each Eventing component. Metrics are listed next.","title":"Admin"},{"location":"eventing/metrics/#broker-ingress","text":"Use the following metrics to debug how broker ingress performs and what events are dispacthed via the ingress component. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name event_type namespace_name response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name event_type namespace_name response_code response_code_class unique_name Milliseconds Stable","title":"Broker - Ingress"},{"location":"eventing/metrics/#broker-filter","text":"Use the following metrics to debug how broker filter performs and what events are dispatched via the filter component. Also user can measure the latency of the actual filtering action on an event. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name container_name= filter_type namespace_name response_code response_code_class trigger_name unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name container_name filter_type namespace_name response_code response_code_class trigger_name unique_name Milliseconds Stable event_processing_latencies The time spent processing an event before it is dispatched to a Trigger subscriber Histogram broker_name container_name filter_type namespace_name trigger_name unique_name Milliseconds Stable","title":"Broker - Filter"},{"location":"eventing/metrics/#in-memory-dispatcher","text":"In-memory channel can be evaluated via the following metrics. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events dispatched by the in-memory channel Counter container_name event_type= namespace_name= response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event from a in-memory Channel Histogram container_name event_type namespace_name= response_code response_code_class unique_name Milliseconds Stable NOTE: A number of metrics eg. controller, Go runtime and others are omitted here as they are common across most components. For more about these metrics check the Serving metrics API section .","title":"In-memory Dispatcher"},{"location":"eventing/metrics/#eventing-sources","text":"Eventing sources are created by users who own the related system, so they can trigger applications with events. Every source exposes by default a number of metrics to help user monitor events dispatched. Use the following metrics to verify that events have been delivered from the source side, thus verifying that the source and any connection with the source work as expected. Metric Name Description Type Tags Unit Status event_count Number of events sent by the source Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable retry_event_count Number of events sent by the source in retries Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable","title":"Eventing sources"},{"location":"eventing/broker/","text":"Brokers \u00b6 Brokers are Kubernetes custom resources that define an event mesh for collecting a pool of CloudEvents . Brokers provide a discoverable endpoint, status.address , for event ingress, and triggers for event delivery. Event producers can send events to a broker by POSTing the event to the status.address.url of the broker. Event delivery mechanics are an implementation detail that depend on the configured broker class . Using brokers and triggers abstracts the details of event routing from the event producer and event consumer. After an event has entered a broker, it can be forwarded to subscribers by using triggers. Triggers allow events to be filtered by attributes, so that events with particular attributes can be sent to subscribers that have registered interest in events with those attributes. A subscriber can be any URL or Addressable resource. Subscribers can also reply to an active request from the broker, and can respond with a new CloudEvent that will be sent back into the broker. For most use cases, a single broker per namespace is sufficient, but there are several use cases where multiple brokers can simplify architecture. For example, separate brokers for events containing Personally Identifiable Information (PII) and non-PII events can simplify audit and access control rules. Broker types \u00b6 The following broker types are available for use with Knative Eventing. Multi-tenant channel-based broker \u00b6 Knative Eventing provides a multi-tenant (MT) channel-based broker implementation that uses channels for event routing. Before you can use the MT channel-based broker, you must install a channel implementation . Alternative broker implementations \u00b6 In the Knative Eventing ecosystem, alternative broker implementations are welcome as long as they respect the broker specifications . The following is a list of brokers provided by the community or vendors: GCP broker \u00b6 The GCP broker is optimized for running in GCP. For more details, refer to the documentation . Apache Kafka broker \u00b6 For information about the Apache Kafka broker, see link . Next steps \u00b6 Create a MT channel-based broker . Configure default broker ConfigMap settings . View the broker specifications .","title":"Overview"},{"location":"eventing/broker/#brokers","text":"Brokers are Kubernetes custom resources that define an event mesh for collecting a pool of CloudEvents . Brokers provide a discoverable endpoint, status.address , for event ingress, and triggers for event delivery. Event producers can send events to a broker by POSTing the event to the status.address.url of the broker. Event delivery mechanics are an implementation detail that depend on the configured broker class . Using brokers and triggers abstracts the details of event routing from the event producer and event consumer. After an event has entered a broker, it can be forwarded to subscribers by using triggers. Triggers allow events to be filtered by attributes, so that events with particular attributes can be sent to subscribers that have registered interest in events with those attributes. A subscriber can be any URL or Addressable resource. Subscribers can also reply to an active request from the broker, and can respond with a new CloudEvent that will be sent back into the broker. For most use cases, a single broker per namespace is sufficient, but there are several use cases where multiple brokers can simplify architecture. For example, separate brokers for events containing Personally Identifiable Information (PII) and non-PII events can simplify audit and access control rules.","title":"Brokers"},{"location":"eventing/broker/#broker-types","text":"The following broker types are available for use with Knative Eventing.","title":"Broker types"},{"location":"eventing/broker/#multi-tenant-channel-based-broker","text":"Knative Eventing provides a multi-tenant (MT) channel-based broker implementation that uses channels for event routing. Before you can use the MT channel-based broker, you must install a channel implementation .","title":"Multi-tenant channel-based broker"},{"location":"eventing/broker/#alternative-broker-implementations","text":"In the Knative Eventing ecosystem, alternative broker implementations are welcome as long as they respect the broker specifications . The following is a list of brokers provided by the community or vendors:","title":"Alternative broker implementations"},{"location":"eventing/broker/#gcp-broker","text":"The GCP broker is optimized for running in GCP. For more details, refer to the documentation .","title":"GCP broker"},{"location":"eventing/broker/#apache-kafka-broker","text":"For information about the Apache Kafka broker, see link .","title":"Apache Kafka broker"},{"location":"eventing/broker/#next-steps","text":"Create a MT channel-based broker . Configure default broker ConfigMap settings . View the broker specifications .","title":"Next steps"},{"location":"eventing/broker/broker-event-delivery/","text":"Event delivery \u00b6 You can configure how events are delivered for each broker by adding a delivery spec, as shown in the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : with-dead-letter-sink spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be any Addressable object that conforms to the Knative Eventing sink contract, such as a Knative service, a Kubernetes service, or a URI. In the example, the destination is a Service object, or Knative service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. This is a linearly increasing delay, which means that the back off delay increases by the given interval for each retry. When using the exponential back off policy, the back off delay increases by a multiplier of the given interval for each retry. retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink. The initial delivery attempt is not included in the retry count, so the total number of delivery attempts is equal to the retry value +1. Broker Support \u00b6 The following table summarizes which delivery parameters are supported for each broker implementation type: Broker Class Supported Delivery Parameters googlecloud deadLetterSink [^1], retry , backoffPolicy , backoffDelay [^2] Kafka deadLetterSink , retry , backoffPolicy , backoffDelay MTChannelBasedBroker depends on the underlying channel RabbitMQBroker deadLetterSink , retry , backoffPolicy , backoffDelay [^1]: deadLetterSink must be a GCP Pub/Sub topic URI: deadLetterSink : uri : pubsub://dead-letter-topic See the [`config-br-delivery`](https://github.com/google/knative-gcp/blob/master/config/core/configmaps/br-delivery.yaml) ConfigMap for a complete example. [^2]: The googlecloud broker only supports the exponential back off policy.","title":"Event delivery"},{"location":"eventing/broker/broker-event-delivery/#event-delivery","text":"You can configure how events are delivered for each broker by adding a delivery spec, as shown in the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : with-dead-letter-sink spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be any Addressable object that conforms to the Knative Eventing sink contract, such as a Knative service, a Kubernetes service, or a URI. In the example, the destination is a Service object, or Knative service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. This is a linearly increasing delay, which means that the back off delay increases by the given interval for each retry. When using the exponential back off policy, the back off delay increases by a multiplier of the given interval for each retry. retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink. The initial delivery attempt is not included in the retry count, so the total number of delivery attempts is equal to the retry value +1.","title":"Event delivery"},{"location":"eventing/broker/broker-event-delivery/#broker-support","text":"The following table summarizes which delivery parameters are supported for each broker implementation type: Broker Class Supported Delivery Parameters googlecloud deadLetterSink [^1], retry , backoffPolicy , backoffDelay [^2] Kafka deadLetterSink , retry , backoffPolicy , backoffDelay MTChannelBasedBroker depends on the underlying channel RabbitMQBroker deadLetterSink , retry , backoffPolicy , backoffDelay [^1]: deadLetterSink must be a GCP Pub/Sub topic URI: deadLetterSink : uri : pubsub://dead-letter-topic See the [`config-br-delivery`](https://github.com/google/knative-gcp/blob/master/config/core/configmaps/br-delivery.yaml) ConfigMap for a complete example. [^2]: The googlecloud broker only supports the exponential back off policy.","title":"Broker Support"},{"location":"eventing/broker/create-mtbroker/","text":"Creating a broker \u00b6 Once you have installed Knative Eventing, you can create an instance of the multi-tenant (MT) channel-based broker that is provided by default. The default backing channel type for an MT channel-based broker is InMemoryChannel. You can create a broker by using the kn CLI or by applying YAML files using kubectl . kn You can create a broker in current namespace by entering the following command: kn broker create <broker-name> -n <namespace> NOTE: If you choose not to specify a namespace, the broker will be created in the current namespace. Optional: Verify that the broker was created by listing existing brokers. Enter the following command: kn broker list Optional: You can also verify the broker exists by describing the broker you have created. Enter the following command: kn broker describe <broker-name> kubectl The YAML in the following example creates a broker named default in the current namespace. For more information about configuring broker options using YAML, see the full broker configuration example . Create a broker in the current namespace: kubectl apply -f - <<EOF apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : <broker-name> EOF Optional: Verify that the broker is working correctly, by entering the following command: kubectl -n <namespace> get broker <broker-name> This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If the READY status is False , wait a few moments and then run the command again.","title":"Creating a broker"},{"location":"eventing/broker/create-mtbroker/#creating-a-broker","text":"Once you have installed Knative Eventing, you can create an instance of the multi-tenant (MT) channel-based broker that is provided by default. The default backing channel type for an MT channel-based broker is InMemoryChannel. You can create a broker by using the kn CLI or by applying YAML files using kubectl . kn You can create a broker in current namespace by entering the following command: kn broker create <broker-name> -n <namespace> NOTE: If you choose not to specify a namespace, the broker will be created in the current namespace. Optional: Verify that the broker was created by listing existing brokers. Enter the following command: kn broker list Optional: You can also verify the broker exists by describing the broker you have created. Enter the following command: kn broker describe <broker-name> kubectl The YAML in the following example creates a broker named default in the current namespace. For more information about configuring broker options using YAML, see the full broker configuration example . Create a broker in the current namespace: kubectl apply -f - <<EOF apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : <broker-name> EOF Optional: Verify that the broker is working correctly, by entering the following command: kubectl -n <namespace> get broker <broker-name> This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If the READY status is False , wait a few moments and then run the command again.","title":"Creating a broker"},{"location":"eventing/broker/example-mtbroker/","text":"Broker configuration example \u00b6 The following is a full example of a multi-tenant (MT) channel-based Broker object which shows the possible configuration options that you can modify: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing delivery : deadLetterSink : ref : kind : Service namespace : example-namespace name : example-service apiVersion : v1 uri : example-uri retry : 5 backoffPolicy : exponential backoffDelay : \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\" - You can specify any valid name for your broker. Using default will create a broker named default . - The namespace must be an existing namespace in your cluster. Using default will create the broker in the current namespace. - You can set the eventing.knative.dev/broker.class annotation to change the class of the broker. The default broker class is MTChannelBasedBroker , but Knative also supports use of the KafkaBroker class. For more information about Kafka brokers, see the Apache Kafka Broker documentation. - spec.config is used to specify the default backing channel configuration for MT channel-based broker implementations. For more information on configuring the default channel type, see the documentation on ConfigMaps . - spec.delivery is used to configure event delivery options. Event delivery options specify what happens to an event that fails to be delivered to an event sink. For more information, see the documentation on broker event delivery .","title":"Broker configuration example"},{"location":"eventing/broker/example-mtbroker/#broker-configuration-example","text":"The following is a full example of a multi-tenant (MT) channel-based Broker object which shows the possible configuration options that you can modify: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing delivery : deadLetterSink : ref : kind : Service namespace : example-namespace name : example-service apiVersion : v1 uri : example-uri retry : 5 backoffPolicy : exponential backoffDelay : \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\" - You can specify any valid name for your broker. Using default will create a broker named default . - The namespace must be an existing namespace in your cluster. Using default will create the broker in the current namespace. - You can set the eventing.knative.dev/broker.class annotation to change the class of the broker. The default broker class is MTChannelBasedBroker , but Knative also supports use of the KafkaBroker class. For more information about Kafka brokers, see the Apache Kafka Broker documentation. - spec.config is used to specify the default backing channel configuration for MT channel-based broker implementations. For more information on configuring the default channel type, see the documentation on ConfigMaps . - spec.delivery is used to configure event delivery options. Event delivery options specify what happens to an event that fails to be delivered to an event sink. For more information, see the documentation on broker event delivery .","title":"Broker configuration example"},{"location":"eventing/broker/configmaps/","text":"ConfigMaps \u00b6 Knative Eventing provides a config-br-defaults ConfigMap that contains the configuration settings that govern default broker creation. The default config-br-defaults ConfigMap is as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing Channel implementation options \u00b6 The following example shows a Broker object where the spec.config configuration is specified in a config-br-default-channel ConfigMap: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing A Broker object that does not have a spec.config specified uses the config-br-default-channel ConfigMap dy default because this is specified in the config-br-defaults ConfigMap. However, if you have installed a different channel implementation, for example, Kafka, and would like this to be used as the default channel implementation for any broker that is created, you can change the config-br-defaults ConfigMap to look like this: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing Now every broker created in the cluster that does not have a spec.config will be configured to use the kafka-channel ConfigMap. For more information about creating a kafka-channel ConfigMap to use with your broker, see the Kafka Channel ConfigMap documentation. Changing the default channel implementation for a namespace \u00b6 You can modify the default broker creation behavior for one or more namespaces. For example, if you wanted to use the kafka-channel ConfigMap for all other brokers created, but wanted to use config-br-default-channel ConfigMap for namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing namespace-2: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing Broker class options \u00b6 When a broker is created without a specified BrokerClass annotation, the default MTChannelBasedBroker broker class is used, as specified in the config-br-defaults ConfigMap. The following example creates a broker called default in the default namespace, and uses MTChannelBasedBroker as the implementation: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default namespace: default EOF Configuring the broker class \u00b6 To configure a broker class, you can modify the eventing.knative.dev/broker.class annotation and spec.config for the Broker object. MTChannelBasedBroker is the broker class default. Modify the eventing.knative.dev/broker.class annotation. Replace MTChannelBasedBroker with the class type you want to use: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default Configure the spec.config with the details of the ConfigMap that defines the backing channel for the broker class: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing Configuring the default BrokerClass for the cluster \u00b6 \u200b You can configure the clusterDefault broker class so that any broker created in the cluster that does not have a BrokerClass annotation uses this default class. Example \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker \u200b Configuring the default BrokerClass for namespaces \u00b6 \u200b You can modify the default broker class for one or more namespaces. For example, if you want to use a KafkaBroker class for all other brokers created on the cluster, but you want to use the MTChannelBasedBroker class for brokers created in namespace-1 and namespace-2 , you would use the following ConfigMap settings: \u200b apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: KafkaBroker namespaceDefaults: namespace1: brokerClass: MTChannelBasedBroker namespace2: brokerClass: MTChannelBasedBroker","title":"ConfigMaps"},{"location":"eventing/broker/configmaps/#configmaps","text":"Knative Eventing provides a config-br-defaults ConfigMap that contains the configuration settings that govern default broker creation. The default config-br-defaults ConfigMap is as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing","title":"ConfigMaps"},{"location":"eventing/broker/configmaps/#channel-implementation-options","text":"The following example shows a Broker object where the spec.config configuration is specified in a config-br-default-channel ConfigMap: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing A Broker object that does not have a spec.config specified uses the config-br-default-channel ConfigMap dy default because this is specified in the config-br-defaults ConfigMap. However, if you have installed a different channel implementation, for example, Kafka, and would like this to be used as the default channel implementation for any broker that is created, you can change the config-br-defaults ConfigMap to look like this: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing Now every broker created in the cluster that does not have a spec.config will be configured to use the kafka-channel ConfigMap. For more information about creating a kafka-channel ConfigMap to use with your broker, see the Kafka Channel ConfigMap documentation.","title":"Channel implementation options"},{"location":"eventing/broker/configmaps/#changing-the-default-channel-implementation-for-a-namespace","text":"You can modify the default broker creation behavior for one or more namespaces. For example, if you wanted to use the kafka-channel ConfigMap for all other brokers created, but wanted to use config-br-default-channel ConfigMap for namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing namespace-2: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing","title":"Changing the default channel implementation for a namespace"},{"location":"eventing/broker/configmaps/#broker-class-options","text":"When a broker is created without a specified BrokerClass annotation, the default MTChannelBasedBroker broker class is used, as specified in the config-br-defaults ConfigMap. The following example creates a broker called default in the default namespace, and uses MTChannelBasedBroker as the implementation: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default namespace: default EOF","title":"Broker class options"},{"location":"eventing/broker/configmaps/#configuring-the-broker-class","text":"To configure a broker class, you can modify the eventing.knative.dev/broker.class annotation and spec.config for the Broker object. MTChannelBasedBroker is the broker class default. Modify the eventing.knative.dev/broker.class annotation. Replace MTChannelBasedBroker with the class type you want to use: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default Configure the spec.config with the details of the ConfigMap that defines the backing channel for the broker class: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing","title":"Configuring the broker class"},{"location":"eventing/broker/configmaps/#configuring-the-default-brokerclass-for-the-cluster","text":"\u200b You can configure the clusterDefault broker class so that any broker created in the cluster that does not have a BrokerClass annotation uses this default class.","title":"Configuring the default BrokerClass for the cluster"},{"location":"eventing/broker/configmaps/#example","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker \u200b","title":"Example"},{"location":"eventing/broker/configmaps/#configuring-the-default-brokerclass-for-namespaces","text":"\u200b You can modify the default broker class for one or more namespaces. For example, if you want to use a KafkaBroker class for all other brokers created on the cluster, but you want to use the MTChannelBasedBroker class for brokers created in namespace-1 and namespace-2 , you would use the following ConfigMap settings: \u200b apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: KafkaBroker namespaceDefaults: namespace1: brokerClass: MTChannelBasedBroker namespace2: brokerClass: MTChannelBasedBroker","title":"Configuring the default BrokerClass for namespaces"},{"location":"eventing/broker/kafka-broker/","text":"Apache Kafka Broker \u00b6 The Apache Kafka Broker is a native Broker implementation, that reduces network hops, supports any Kafka version, and has a better integration with Apache Kafka for the Knative Broker and Trigger model. Notable features are: Control plane High Availability Horizontally scalable data plane Extensively configurable Ordered delivery of events based on CloudEvents partitioning extension Support any Kafka version, see compatibility matrix Prerequisites \u00b6 Installing Eventing using YAML files . An Apache Kafka cluster (if you're just getting started you can follow Strimzi Quickstart page ). Installation \u00b6 Install the Kafka controller by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Verify that kafka-controller , kafka-broker-receiver and kafka-broker-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-broker-dispatcher 1 /1 1 1 4s kafka-broker-receiver 1 /1 1 1 5s Create a Kafka Broker \u00b6 A Kafka Broker object looks like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka name : default namespace : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing # Optional dead letter sink, you can specify either: # - deadLetterSink.ref, which is a reference to a Callable # - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster) delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service spec.config should reference any ConfigMap that looks like the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Number of topic partitions default.topic.partitions : \"10\" # Replication factor of topic messages. default.topic.replication.factor : \"1\" # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster) bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" The above ConfigMap is installed in the cluster. You can edit the configuration or create a new one with the same values depending on your needs. NOTE: The default.topic.replication.factor value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the default.topic.replication.factor value should not be more than 1 . Set as default broker implementation \u00b6 To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the config-br-defaults ConfigMap in the knative-eventing namespace. This allows you to avoid configuring individual or per-namespace settings for each broker, such as metadata.annotations.eventing.knative.dev/broker.class or spec.config . The following YAML is an example of a config-br-defaults ConfigMap using Kafka broker as the default implementation. apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | clusterDefault: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespaceDefaults: namespace1: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespace2: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing Security \u00b6 Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the ConfigMap referenced by broker.spec.config , we can reference a Secret : apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Other configurations # ... # Reference a Secret called my_secret auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the ConfigMap referenced by broker.spec.config , in this case: knative-eventing . Note: Certificates and keys must be in PEM format . Authentication using SASL \u00b6 Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice. Authentication using SASL without encryption \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Authentication using SASL and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Encryption using SSL without client authentication \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true Authentication and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> NOTE: ca.crt can be omitted to fallback to use system's root CA set. Kafka Producer and Consumer configurations \u00b6 Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations and Consumer configurations . Enable debug logging for data plane components \u00b6 The following YAML shows the default logging configuration for data plane components, that is created during the installation step: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"INFO\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> To change the logging level to DEBUG , you must: Apply the following kafka-config-logging ConfigMap or replace level=\"INFO\" with level=\"DEBUG\" to the ConfigMap kafka-config-logging : apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-broker-receiver and the kafka-broker-dispatcher , by entering the following commands: kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver kubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher Configuring the order of delivered events \u00b6 When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees. You can configure the delivery order of events using the kafka.eventing.knative.dev/delivery.order annotation on the Trigger object: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : kafka.eventing.knative.dev/delivery.order : ordered spec : broker : my-kafka-broker subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service The supported consumer delivery guarantees are: unordered : Unordered consumer is a non-blocking consumer that potentially delivers messages unordered, while preserving proper offset management. ordered : Ordered consumer is a per-partition blocking consumer that delivers messages in order. unordered is the default ordering guarantee, while ordered is considered unstable, use with caution . Additional information \u00b6 To report a bug or request a feature, open an issue in the eventing-kafka-broker repository .","title":"Overview"},{"location":"eventing/broker/kafka-broker/#apache-kafka-broker","text":"The Apache Kafka Broker is a native Broker implementation, that reduces network hops, supports any Kafka version, and has a better integration with Apache Kafka for the Knative Broker and Trigger model. Notable features are: Control plane High Availability Horizontally scalable data plane Extensively configurable Ordered delivery of events based on CloudEvents partitioning extension Support any Kafka version, see compatibility matrix","title":"Apache Kafka Broker"},{"location":"eventing/broker/kafka-broker/#prerequisites","text":"Installing Eventing using YAML files . An Apache Kafka cluster (if you're just getting started you can follow Strimzi Quickstart page ).","title":"Prerequisites"},{"location":"eventing/broker/kafka-broker/#installation","text":"Install the Kafka controller by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Verify that kafka-controller , kafka-broker-receiver and kafka-broker-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-broker-dispatcher 1 /1 1 1 4s kafka-broker-receiver 1 /1 1 1 5s","title":"Installation"},{"location":"eventing/broker/kafka-broker/#create-a-kafka-broker","text":"A Kafka Broker object looks like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka name : default namespace : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing # Optional dead letter sink, you can specify either: # - deadLetterSink.ref, which is a reference to a Callable # - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster) delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service spec.config should reference any ConfigMap that looks like the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Number of topic partitions default.topic.partitions : \"10\" # Replication factor of topic messages. default.topic.replication.factor : \"1\" # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster) bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" The above ConfigMap is installed in the cluster. You can edit the configuration or create a new one with the same values depending on your needs. NOTE: The default.topic.replication.factor value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the default.topic.replication.factor value should not be more than 1 .","title":"Create a Kafka Broker"},{"location":"eventing/broker/kafka-broker/#set-as-default-broker-implementation","text":"To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the config-br-defaults ConfigMap in the knative-eventing namespace. This allows you to avoid configuring individual or per-namespace settings for each broker, such as metadata.annotations.eventing.knative.dev/broker.class or spec.config . The following YAML is an example of a config-br-defaults ConfigMap using Kafka broker as the default implementation. apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | clusterDefault: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespaceDefaults: namespace1: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespace2: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing","title":"Set as default broker implementation"},{"location":"eventing/broker/kafka-broker/#security","text":"Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the ConfigMap referenced by broker.spec.config , we can reference a Secret : apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Other configurations # ... # Reference a Secret called my_secret auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the ConfigMap referenced by broker.spec.config , in this case: knative-eventing . Note: Certificates and keys must be in PEM format .","title":"Security"},{"location":"eventing/broker/kafka-broker/#authentication-using-sasl","text":"Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice.","title":"Authentication using SASL"},{"location":"eventing/broker/kafka-broker/#authentication-using-sasl-without-encryption","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL without encryption"},{"location":"eventing/broker/kafka-broker/#authentication-using-sasl-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL and encryption using SSL"},{"location":"eventing/broker/kafka-broker/#encryption-using-ssl-without-client-authentication","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true","title":"Encryption using SSL without client authentication"},{"location":"eventing/broker/kafka-broker/#authentication-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> NOTE: ca.crt can be omitted to fallback to use system's root CA set.","title":"Authentication and encryption using SSL"},{"location":"eventing/broker/kafka-broker/#kafka-producer-and-consumer-configurations","text":"Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations and Consumer configurations .","title":"Kafka Producer and Consumer configurations"},{"location":"eventing/broker/kafka-broker/#enable-debug-logging-for-data-plane-components","text":"The following YAML shows the default logging configuration for data plane components, that is created during the installation step: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"INFO\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> To change the logging level to DEBUG , you must: Apply the following kafka-config-logging ConfigMap or replace level=\"INFO\" with level=\"DEBUG\" to the ConfigMap kafka-config-logging : apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-broker-receiver and the kafka-broker-dispatcher , by entering the following commands: kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver kubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher","title":"Enable debug logging for data plane components"},{"location":"eventing/broker/kafka-broker/#configuring-the-order-of-delivered-events","text":"When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees. You can configure the delivery order of events using the kafka.eventing.knative.dev/delivery.order annotation on the Trigger object: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : kafka.eventing.knative.dev/delivery.order : ordered spec : broker : my-kafka-broker subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service The supported consumer delivery guarantees are: unordered : Unordered consumer is a non-blocking consumer that potentially delivers messages unordered, while preserving proper offset management. ordered : Ordered consumer is a per-partition blocking consumer that delivers messages in order. unordered is the default ordering guarantee, while ordered is considered unstable, use with caution .","title":"Configuring the order of delivered events"},{"location":"eventing/broker/kafka-broker/#additional-information","text":"To report a bug or request a feature, open an issue in the eventing-kafka-broker repository .","title":"Additional information"},{"location":"eventing/broker/kafka-broker/kafka-configmap/","text":"Kafka Channel ConfigMap \u00b6 NOTE: This guide assumes Knative Eventing is installed in the knative-eventing namespace. If you have installed Knative Eventing in a different namespace, replace knative-eventing with the name of that namespace. To use Kafka channels, you must create a YAML file that specifies how these channels will be created. NOTE: You must install the Kafka Channel first. You can copy the following sample code into your kafka-channel ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 NOTE: This example specifies two extra parameters that are specific to Kafka Channels; numPartitions and replicationFactor . To create a Broker that uses the KafkaChannel, specify the kafka-channel ConfigMap: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: MTChannelBasedBroker name: kafka-backed-broker namespace: default spec: config: apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing EOF","title":"Kafka Channel ConfigMap"},{"location":"eventing/broker/kafka-broker/kafka-configmap/#kafka-channel-configmap","text":"NOTE: This guide assumes Knative Eventing is installed in the knative-eventing namespace. If you have installed Knative Eventing in a different namespace, replace knative-eventing with the name of that namespace. To use Kafka channels, you must create a YAML file that specifies how these channels will be created. NOTE: You must install the Kafka Channel first. You can copy the following sample code into your kafka-channel ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 NOTE: This example specifies two extra parameters that are specific to Kafka Channels; numPartitions and replicationFactor . To create a Broker that uses the KafkaChannel, specify the kafka-channel ConfigMap: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: MTChannelBasedBroker name: kafka-backed-broker namespace: default spec: config: apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing EOF","title":"Kafka Channel ConfigMap"},{"location":"eventing/broker/triggers/","text":"Triggers \u00b6 A trigger represents a desire to subscribe to events from a specific broker. The subscriber value must be a Destination . Simple example which will receive all the events from the given ( default ) broker and deliver them to Knative Serving service my-service : kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: my-service EOF Simple example which will receive all the events from the given ( default ) broker and deliver them to the custom path /my-custom-path for the Kubernetes service my-service : kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default subscriber: ref: apiVersion: v1 kind: Service name: my-service uri: /my-custom-path EOF Trigger filtering \u00b6 Exact match filtering on any number of CloudEvents attributes as well as extensions are supported. If your filter sets multiple attributes, an event must have all of the attributes for the trigger to filter it. Note that we only support exact matching on string values. Example \u00b6 This example filters events from the default broker that are of type dev.knative.foo.bar and have the extension myextension with the value my-extension-value . kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default filter: attributes: type: dev.knative.foo.bar myextension: my-extension-value subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: my-service EOF","title":"Triggers"},{"location":"eventing/broker/triggers/#triggers","text":"A trigger represents a desire to subscribe to events from a specific broker. The subscriber value must be a Destination . Simple example which will receive all the events from the given ( default ) broker and deliver them to Knative Serving service my-service : kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: my-service EOF Simple example which will receive all the events from the given ( default ) broker and deliver them to the custom path /my-custom-path for the Kubernetes service my-service : kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default subscriber: ref: apiVersion: v1 kind: Service name: my-service uri: /my-custom-path EOF","title":"Triggers"},{"location":"eventing/broker/triggers/#trigger-filtering","text":"Exact match filtering on any number of CloudEvents attributes as well as extensions are supported. If your filter sets multiple attributes, an event must have all of the attributes for the trigger to filter it. Note that we only support exact matching on string values.","title":"Trigger filtering"},{"location":"eventing/broker/triggers/#example","text":"This example filters events from the default broker that are of type dev.knative.foo.bar and have the extension myextension with the value my-extension-value . kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default filter: attributes: type: dev.knative.foo.bar myextension: my-extension-value subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: my-service EOF","title":"Example"},{"location":"eventing/channels/","text":"Channels \u00b6 Channels are Kubernetes custom resources that define a single event forwarding and persistence layer. A channel provides an event delivery mechanism that can fan-out received events, through subscriptions, to multiple destinations, or sinks. Examples of sinks include brokers and Knative services. Next steps \u00b6 Learn about default available channel types Create a channel Create a subscription","title":"Overview"},{"location":"eventing/channels/#channels","text":"Channels are Kubernetes custom resources that define a single event forwarding and persistence layer. A channel provides an event delivery mechanism that can fan-out received events, through subscriptions, to multiple destinations, or sinks. Examples of sinks include brokers and Knative services.","title":"Channels"},{"location":"eventing/channels/#next-steps","text":"Learn about default available channel types Create a channel Create a subscription","title":"Next steps"},{"location":"eventing/channels/channel-types-defaults/","text":"Channel types and defaults \u00b6 Knative provides the InMemoryChannel channel implementation by default. This default implementation is useful for developers who do not want to configure a specific implementation type, such as Apache Kafka or NATSS channels. NOTE: InMemoryChannel channels should not be used in production environments. The default channel implementation is specified in the default-ch-webhook ConfigMap in the knative-eventing namespace. For more information about modifying ConfigMaps, see Configuring the Eventing Operator custom resource . In the following example, the cluster default channel implementation is InMemoryChannel, while the namespace default channel implementation for the example-namespace is KafkaChannel. apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: example-namespace: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 2 replicationFactor: 1 Default channels can be configured for the cluster, a namespace on the cluster, or both. NOTE: If a default channel implementation is configured for a namespace, this will overwrite the configuration for the cluster. Next steps \u00b6 Create an InMemoryChannel","title":"Channel types and defaults"},{"location":"eventing/channels/channel-types-defaults/#channel-types-and-defaults","text":"Knative provides the InMemoryChannel channel implementation by default. This default implementation is useful for developers who do not want to configure a specific implementation type, such as Apache Kafka or NATSS channels. NOTE: InMemoryChannel channels should not be used in production environments. The default channel implementation is specified in the default-ch-webhook ConfigMap in the knative-eventing namespace. For more information about modifying ConfigMaps, see Configuring the Eventing Operator custom resource . In the following example, the cluster default channel implementation is InMemoryChannel, while the namespace default channel implementation for the example-namespace is KafkaChannel. apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: example-namespace: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 2 replicationFactor: 1 Default channels can be configured for the cluster, a namespace on the cluster, or both. NOTE: If a default channel implementation is configured for a namespace, this will overwrite the configuration for the cluster.","title":"Channel types and defaults"},{"location":"eventing/channels/channel-types-defaults/#next-steps","text":"Create an InMemoryChannel","title":"Next steps"},{"location":"eventing/channels/channels-crds/","text":"Available Channels \u00b6 This is a non-exhaustive list of the available Channels for Knative Eventing. Notes: Inclusion in this list is not an endorsement, nor does it imply any level of support. Name Status Support Description GCP PubSub Proof of Concept None Channels are backed by GCP PubSub . InMemoryChannel Proof of Concept None In-memory channels are a best effort Channel. They should NOT be used in Production. They are useful for development. KafkaChannel - Consolidated Proof of Concept None Channels are backed by Apache Kafka topics. The original Knative KafkaChannel implementation which utilizes a single combined Kafka Producer / Consumer deployment. KafkaChannel - Distributed Proof of Concept None Channels are backed by Apache Kafka topics. An alternate KafkaChannel implementation, contributed by SAP's Kyma project, which provides a more granular deployment of Producers / Consumers. NatssChannel Proof of Concept None Channels are backed by NATS Streaming .","title":"Available Channels"},{"location":"eventing/channels/channels-crds/#available-channels","text":"This is a non-exhaustive list of the available Channels for Knative Eventing. Notes: Inclusion in this list is not an endorsement, nor does it imply any level of support. Name Status Support Description GCP PubSub Proof of Concept None Channels are backed by GCP PubSub . InMemoryChannel Proof of Concept None In-memory channels are a best effort Channel. They should NOT be used in Production. They are useful for development. KafkaChannel - Consolidated Proof of Concept None Channels are backed by Apache Kafka topics. The original Knative KafkaChannel implementation which utilizes a single combined Kafka Producer / Consumer deployment. KafkaChannel - Distributed Proof of Concept None Channels are backed by Apache Kafka topics. An alternate KafkaChannel implementation, contributed by SAP's Kyma project, which provides a more granular deployment of Producers / Consumers. NatssChannel Proof of Concept None Channels are backed by NATS Streaming .","title":"Available Channels"},{"location":"eventing/channels/create-default-channel/","text":"Creating a channel using cluster or namespace defaults \u00b6 Developers can create channels of any supported implementation type by creating an instance of a Channel object. Create a Channel object: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : my-channel namespace : default Since this object is created in the default namespace, according to the default ConfigMap example in the previous section, it will be an InMemoryChannel channel implementation. After the Channel object is created, a mutating admission webhook sets the spec.channelTemplate based on the default channel implementation: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : my-channel namespace : default spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel NOTE: The spec.channelTemplate property cannot be changed after creation, since it is set by the default channel mechanism, not the user. The channel controller creates a backing channel instance based on the spec.channelTemplate . When this mechanism is used, two objects are created, a generic Channel object, and an InMemoryChannel object. The generic object acts as a proxy for the InMemoryChannel object, by copying its subscriptions to and setting its status to that of the InMemoryChannel object. NOTE: Defaults only apply on object creation. Defaults are applied by the webhook only on Channel or Sequence creation. If the default settings change, the new defaults will only apply to newly-created channels, brokers, or sequences. Existing ones will not change.","title":"Creating a channel using cluster or namespace defaults"},{"location":"eventing/channels/create-default-channel/#creating-a-channel-using-cluster-or-namespace-defaults","text":"Developers can create channels of any supported implementation type by creating an instance of a Channel object. Create a Channel object: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : my-channel namespace : default Since this object is created in the default namespace, according to the default ConfigMap example in the previous section, it will be an InMemoryChannel channel implementation. After the Channel object is created, a mutating admission webhook sets the spec.channelTemplate based on the default channel implementation: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : my-channel namespace : default spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel NOTE: The spec.channelTemplate property cannot be changed after creation, since it is set by the default channel mechanism, not the user. The channel controller creates a backing channel instance based on the spec.channelTemplate . When this mechanism is used, two objects are created, a generic Channel object, and an InMemoryChannel object. The generic object acts as a proxy for the InMemoryChannel object, by copying its subscriptions to and setting its status to that of the InMemoryChannel object. NOTE: Defaults only apply on object creation. Defaults are applied by the webhook only on Channel or Sequence creation. If the default settings change, the new defaults will only apply to newly-created channels, brokers, or sequences. Existing ones will not change.","title":"Creating a channel using cluster or namespace defaults"},{"location":"eventing/channels/subscriptions/","text":"Subscriptions \u00b6 After you have created a channel and a sink, you can create a subscription to enable event delivery. Creating a subscription \u00b6 kn Create a subscription between a channel and a sink: kn subscription create <subscription_name> \\ --channel <Group:Version:Kind>:<channel_name> \\ --sink <sink_prefix>:<sink_name> \\ --sink-reply <sink_prefix>:<sink_name> \\ --sink-dead-letter <sink_prefix>:<sink_name> --channel specifies the source for cloud events that should be processed. You must provide the channel name. If you are not using the default channel that is backed by the Channel resource, you must prefix the channel name with the <Group:Version:Kind> for the specified channel type. For example, this will be messaging.knative.dev:v1beta1:KafkaChannel for a Kafka backed channel. --sink specifies the target destination to which the event should be delivered. By default, the <sink_name> is interpreted as a Knative service of this name, in the same namespace as the subscription. You can specify the type of the sink by using one of the following prefixes: ksvc : A Knative service. svc : A Kubernetes Service. channel : A channel that should be used as destination. Only default channel types can be referenced here. broker : An Eventing broker. \\ - --sink-reply and --sink-dead-letter are optional arguments. They can be used to specify where the sink reply will be sent to, and where to send the cloud event in case of a failure, respectively. Both use the same naming conventions for specifying the sink as the --sink flag. Example command: kn subscription create mysubscription --channel mychannel --sink ksvc:myservice This example command creates a channel named mysubscription , that routes events from a channel named mychannel to a Knative service named myservice . NOTE: The sink prefix is optional. It is also possible to specify the service for --sink as just --sink <service_name> and omit the ksvc prefix. YAML Create a Subscription object in a YAML file: apiVersion : messaging.knative.dev/v1beta1 kind : Subscription metadata : name : <subscription_name> # Name of the subscription. namespace : default spec : channel : apiVersion : messaging.knative.dev/v1 kind : Channel name : <channel_name> # Configuration settings for the channel that the subscription connects to. delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service_name> # Configuration settings for event delivery. # This tells the subscription what happens to events that cannot be delivered to the subscriber. # When this is configured, events that failed to be consumed are sent to the deadLetterSink. # The event is dropped, no re-delivery of the event is attempted, and an error is logged in the system. # The deadLetterSink value must be a Destination. subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service_name> # Configuration settings for the subscriber. This is the event sink that events are delivered to from the channel. Apply the YAML file: kubectl apply -f <filename> Listing subscriptions \u00b6 You can list all existing subscriptions by using the kn CLI tool. List all subscriptions: kn subscription list - List subscriptions in YAML format: kn subscription list -o yaml Describing a subscription \u00b6 You can print details about a subscription by using the kn CLI tool: kn subscription describe <subscription_name> Deleting subscriptions \u00b6 You can delete a subscription by using the kn or kubectl CLI tools. kn kn subscription delete <subscription_name> kubectl kubectl subscription delete <subscription_name>","title":"Subscriptions"},{"location":"eventing/channels/subscriptions/#subscriptions","text":"After you have created a channel and a sink, you can create a subscription to enable event delivery.","title":"Subscriptions"},{"location":"eventing/channels/subscriptions/#creating-a-subscription","text":"kn Create a subscription between a channel and a sink: kn subscription create <subscription_name> \\ --channel <Group:Version:Kind>:<channel_name> \\ --sink <sink_prefix>:<sink_name> \\ --sink-reply <sink_prefix>:<sink_name> \\ --sink-dead-letter <sink_prefix>:<sink_name> --channel specifies the source for cloud events that should be processed. You must provide the channel name. If you are not using the default channel that is backed by the Channel resource, you must prefix the channel name with the <Group:Version:Kind> for the specified channel type. For example, this will be messaging.knative.dev:v1beta1:KafkaChannel for a Kafka backed channel. --sink specifies the target destination to which the event should be delivered. By default, the <sink_name> is interpreted as a Knative service of this name, in the same namespace as the subscription. You can specify the type of the sink by using one of the following prefixes: ksvc : A Knative service. svc : A Kubernetes Service. channel : A channel that should be used as destination. Only default channel types can be referenced here. broker : An Eventing broker. \\ - --sink-reply and --sink-dead-letter are optional arguments. They can be used to specify where the sink reply will be sent to, and where to send the cloud event in case of a failure, respectively. Both use the same naming conventions for specifying the sink as the --sink flag. Example command: kn subscription create mysubscription --channel mychannel --sink ksvc:myservice This example command creates a channel named mysubscription , that routes events from a channel named mychannel to a Knative service named myservice . NOTE: The sink prefix is optional. It is also possible to specify the service for --sink as just --sink <service_name> and omit the ksvc prefix. YAML Create a Subscription object in a YAML file: apiVersion : messaging.knative.dev/v1beta1 kind : Subscription metadata : name : <subscription_name> # Name of the subscription. namespace : default spec : channel : apiVersion : messaging.knative.dev/v1 kind : Channel name : <channel_name> # Configuration settings for the channel that the subscription connects to. delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service_name> # Configuration settings for event delivery. # This tells the subscription what happens to events that cannot be delivered to the subscriber. # When this is configured, events that failed to be consumed are sent to the deadLetterSink. # The event is dropped, no re-delivery of the event is attempted, and an error is logged in the system. # The deadLetterSink value must be a Destination. subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service_name> # Configuration settings for the subscriber. This is the event sink that events are delivered to from the channel. Apply the YAML file: kubectl apply -f <filename>","title":"Creating a subscription"},{"location":"eventing/channels/subscriptions/#listing-subscriptions","text":"You can list all existing subscriptions by using the kn CLI tool. List all subscriptions: kn subscription list - List subscriptions in YAML format: kn subscription list -o yaml","title":"Listing subscriptions"},{"location":"eventing/channels/subscriptions/#describing-a-subscription","text":"You can print details about a subscription by using the kn CLI tool: kn subscription describe <subscription_name>","title":"Describing a subscription"},{"location":"eventing/channels/subscriptions/#deleting-subscriptions","text":"You can delete a subscription by using the kn or kubectl CLI tools. kn kn subscription delete <subscription_name> kubectl kubectl subscription delete <subscription_name>","title":"Deleting subscriptions"},{"location":"eventing/debugging/","text":"Debugging Knative Eventing \u00b6 This is an evolving document on how to debug a non-working Knative Eventing setup. Audience \u00b6 This document is intended for people that are familiar with the object model of Knative Eventing . You don't need to be an expert, but do need to know roughly how things fit together. Prerequisites \u00b6 Setup Knative Eventing and an Eventing-contrib resource . Example \u00b6 This guide uses an example consisting of an event source that sends events to a function. See example.yaml for the entire YAML. For any commands in this guide to work, you must apply example.yaml : kubectl apply --filename example.yaml Triggering Events \u00b6 Knative events will occur whenever a Kubernetes Event occurs in the knative-debug namespace. We can cause this to occur with the following commands: kubectl --namespace knative-debug run to-be-deleted --image = image-that-doesnt-exist --restart = Never # 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event. sleep 5 kubectl --namespace knative-debug delete pod to-be-deleted Then we can see the Kubernetes Event s (note that these are not Knative events!): kubectl --namespace knative-debug get events This should produce output along the lines of: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 20s 20s 1 to-be-deleted.157aadb9f376fc4e Pod Normal Scheduled default-scheduler Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2 Where are my events? \u00b6 You've applied example.yaml and you are inspecting fn 's logs: kubectl --namespace knative-debug logs -l app = fn -c user-container But you don't see any events arrive. Where is the problem? Control Plane \u00b6 We will first check the control plane, to ensure everything should be working properly. Resources \u00b6 The first thing to check are all the created resources, do their statuses contain ready true? We will attempt to determine why from the most basic pieces out: fn - The Deployment has no dependencies inside Knative. svc - The Service has no dependencies inside Knative. chan - The Channel depends on its backing channel implementation and somewhat depends on sub . src - The Source depends on chan . sub - The Subscription depends on both chan and svc . fn \u00b6 kubectl --namespace knative-debug get deployment fn -o jsonpath = '{.status.availableReplicas}' We want to see 1 . If you don't, then you need to debug the Deployment . Is there anything obviously wrong mentioned in the status ? kubectl --namespace knative-debug get deployment fn --output yaml If it is not obvious what is wrong, then you need to debug the Deployment , which is out of scope of this document. Verify that the Pod is Ready : kubectl --namespace knative-debug get pod -l app = fn -o jsonpath = '{.items[*].status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, then try to debug the Deployment using the Kubernetes Application Debugging guide. svc \u00b6 kubectl --namespace knative-debug get service svc We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml . Verify it points at the expected pod. svcLabels = $( kubectl --namespace knative-debug get service svc -o go-template = '{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' ) kubectl --namespace knative-debug get pods -l $svcLabels This should return a single Pod, which if you inspect is the one generated by fn . chan \u00b6 chan uses the in-memory-channel . This is a very basic channel and has few failure modes that will be exhibited in chan 's status . kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, get the full resource: kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml If status is completely missing, it implies that something is wrong with the in-memory-channel controller. See Channel Controller . Next verify that chan is addressable: kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.address.hostname}' This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller . We will verify that the two resources that the chan creates exist and are Ready . Service \u00b6 chan creates a K8s Service . kubectl --namespace knative-debug get service -l messaging.knative.dev/role = in -memory-channel It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that src can send events to it. If it doesn't exist, it implies that something went wrong during chan reconciliation. See Channel Controller . src \u00b6 src is a ApiServerSource . First we will verify that src is writing to chan . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.spec.sink}' Which should return map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan] . If it doesn't, then src was setup incorrectly and its spec needs to be fixed. Fixing should be as simple as updating its spec to have the correct sink (see example.yaml ). Now that we know src is sending to chan , let's verify that it is Ready . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' sub \u00b6 sub is a Subscription from chan to fn . Verify that sub is Ready : kubectl --namespace knative-debug get subscription sub -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' This should return True . If it doesn't then, look at all the status entries. kubectl --namespace knative-debug get subscription sub --output yaml Controllers \u00b6 Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs. Deployment Controller \u00b6 The Kubernetes Deployment Controller, controlling fn , is out of scope for this document. Service Controller \u00b6 The Kubernetes Service Controller, controlling svc , is out of scope for this document. Channel Controller \u00b6 There is not a single Channel Controller. Instead, there is one Controller for each Channel CRD. chan uses the InMemoryChannel Channel CRD , whose Controller is: kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller --output yaml See its logs with: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller Pay particular attention to any lines that have a logging level of warning or error . Source Controller \u00b6 Each Source will have its own Controller. src is a ApiServerSource , so its Controller is: kubectl --namespace knative-eventing get pod -l app = sources-controller This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller . ApiServerSource Controller \u00b6 The ApiServerSource Controller is run in the same binary as some other Source Controllers from Eventing. It is: kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller View its logs with: kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller Pay particular attention to any lines that have a logging level of warning or error . Subscription Controller \u00b6 The Subscription Controller controls sub . It attempts to resolve the addresses that a Channel should send events to, and once resolved, inject those into the Channel 's spec.subscribable . kubectl --namespace knative-eventing get pod -l app = eventing-controller View its logs with: kubectl --namespace knative-eventing logs -l app = eventing-controller Pay particular attention to any lines that have a logging level of warning or error . Data Plane \u00b6 The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane. The Knative event takes the following path: Event is generated by src . In this case, it is caused by having a Kubernetes Event trigger it, but as far as Knative is concerned, the Source is generating the event denovo (from nothing). src is POSTing the event to chan 's address, http://chan-kn-channel.knative-debug.svc.cluster.local . The Channel Dispatcher receives the request and introspects the Host header to determine which Channel it corresponds to. It sees that it corresponds to knative-debug/chan so forwards the request to the subscribers defined in sub , in particular svc , which is backed by fn . fn receives the request and logs it. We will investigate components in the order in which events should travel. Channel Dispatcher \u00b6 The Channel Dispatcher is the component that receives POSTs pushing events into Channel s and then POSTs to subscribers of those Channel s when an event is received. For the in-memory-channel used in this example, there is a single binary that handles both the receiving and dispatching sides for all in-memory-channel Channel s. First we will inspect the Dispatcher's logs to see if it is anything obvious: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = dispatcher -c dispatcher Ideally we will see lines like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.424Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.425Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.981Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } Which shows that the request is being received and then sent to svc , which is returning a 2XX response code (likely 200, 202, or 204). However if we see something like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"error\" , \"ts\" : \"2019-08-16T16:10:38.169Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"fanout/fanout_handler.go:121\" , \"msg\" : \"Fanout had an error\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" , \"error\" : \"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\" , \"stacktrace\" : \"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\" } Then we know there was a problem posting to http://svc.knative-debug.svc.cluster.local/ . TODO Finish this section. Especially after the Channel Dispatcher emits K8s events about failures. fn \u00b6 TODO Fill in this section. TODO Finish the guide. \u00b6","title":"Debugging"},{"location":"eventing/debugging/#debugging-knative-eventing","text":"This is an evolving document on how to debug a non-working Knative Eventing setup.","title":"Debugging Knative Eventing"},{"location":"eventing/debugging/#audience","text":"This document is intended for people that are familiar with the object model of Knative Eventing . You don't need to be an expert, but do need to know roughly how things fit together.","title":"Audience"},{"location":"eventing/debugging/#prerequisites","text":"Setup Knative Eventing and an Eventing-contrib resource .","title":"Prerequisites"},{"location":"eventing/debugging/#example","text":"This guide uses an example consisting of an event source that sends events to a function. See example.yaml for the entire YAML. For any commands in this guide to work, you must apply example.yaml : kubectl apply --filename example.yaml","title":"Example"},{"location":"eventing/debugging/#triggering-events","text":"Knative events will occur whenever a Kubernetes Event occurs in the knative-debug namespace. We can cause this to occur with the following commands: kubectl --namespace knative-debug run to-be-deleted --image = image-that-doesnt-exist --restart = Never # 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event. sleep 5 kubectl --namespace knative-debug delete pod to-be-deleted Then we can see the Kubernetes Event s (note that these are not Knative events!): kubectl --namespace knative-debug get events This should produce output along the lines of: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 20s 20s 1 to-be-deleted.157aadb9f376fc4e Pod Normal Scheduled default-scheduler Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2","title":"Triggering Events"},{"location":"eventing/debugging/#where-are-my-events","text":"You've applied example.yaml and you are inspecting fn 's logs: kubectl --namespace knative-debug logs -l app = fn -c user-container But you don't see any events arrive. Where is the problem?","title":"Where are my events?"},{"location":"eventing/debugging/#control-plane","text":"We will first check the control plane, to ensure everything should be working properly.","title":"Control Plane"},{"location":"eventing/debugging/#resources","text":"The first thing to check are all the created resources, do their statuses contain ready true? We will attempt to determine why from the most basic pieces out: fn - The Deployment has no dependencies inside Knative. svc - The Service has no dependencies inside Knative. chan - The Channel depends on its backing channel implementation and somewhat depends on sub . src - The Source depends on chan . sub - The Subscription depends on both chan and svc .","title":"Resources"},{"location":"eventing/debugging/#fn","text":"kubectl --namespace knative-debug get deployment fn -o jsonpath = '{.status.availableReplicas}' We want to see 1 . If you don't, then you need to debug the Deployment . Is there anything obviously wrong mentioned in the status ? kubectl --namespace knative-debug get deployment fn --output yaml If it is not obvious what is wrong, then you need to debug the Deployment , which is out of scope of this document. Verify that the Pod is Ready : kubectl --namespace knative-debug get pod -l app = fn -o jsonpath = '{.items[*].status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, then try to debug the Deployment using the Kubernetes Application Debugging guide.","title":"fn"},{"location":"eventing/debugging/#svc","text":"kubectl --namespace knative-debug get service svc We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml . Verify it points at the expected pod. svcLabels = $( kubectl --namespace knative-debug get service svc -o go-template = '{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' ) kubectl --namespace knative-debug get pods -l $svcLabels This should return a single Pod, which if you inspect is the one generated by fn .","title":"svc"},{"location":"eventing/debugging/#chan","text":"chan uses the in-memory-channel . This is a very basic channel and has few failure modes that will be exhibited in chan 's status . kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, get the full resource: kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml If status is completely missing, it implies that something is wrong with the in-memory-channel controller. See Channel Controller . Next verify that chan is addressable: kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.address.hostname}' This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller . We will verify that the two resources that the chan creates exist and are Ready .","title":"chan"},{"location":"eventing/debugging/#service","text":"chan creates a K8s Service . kubectl --namespace knative-debug get service -l messaging.knative.dev/role = in -memory-channel It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that src can send events to it. If it doesn't exist, it implies that something went wrong during chan reconciliation. See Channel Controller .","title":"Service"},{"location":"eventing/debugging/#src","text":"src is a ApiServerSource . First we will verify that src is writing to chan . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.spec.sink}' Which should return map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan] . If it doesn't, then src was setup incorrectly and its spec needs to be fixed. Fixing should be as simple as updating its spec to have the correct sink (see example.yaml ). Now that we know src is sending to chan , let's verify that it is Ready . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}'","title":"src"},{"location":"eventing/debugging/#sub","text":"sub is a Subscription from chan to fn . Verify that sub is Ready : kubectl --namespace knative-debug get subscription sub -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' This should return True . If it doesn't then, look at all the status entries. kubectl --namespace knative-debug get subscription sub --output yaml","title":"sub"},{"location":"eventing/debugging/#controllers","text":"Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs.","title":"Controllers"},{"location":"eventing/debugging/#deployment-controller","text":"The Kubernetes Deployment Controller, controlling fn , is out of scope for this document.","title":"Deployment Controller"},{"location":"eventing/debugging/#service-controller","text":"The Kubernetes Service Controller, controlling svc , is out of scope for this document.","title":"Service Controller"},{"location":"eventing/debugging/#channel-controller","text":"There is not a single Channel Controller. Instead, there is one Controller for each Channel CRD. chan uses the InMemoryChannel Channel CRD , whose Controller is: kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller --output yaml See its logs with: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller Pay particular attention to any lines that have a logging level of warning or error .","title":"Channel Controller"},{"location":"eventing/debugging/#source-controller","text":"Each Source will have its own Controller. src is a ApiServerSource , so its Controller is: kubectl --namespace knative-eventing get pod -l app = sources-controller This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller .","title":"Source Controller"},{"location":"eventing/debugging/#apiserversource-controller","text":"The ApiServerSource Controller is run in the same binary as some other Source Controllers from Eventing. It is: kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller View its logs with: kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller Pay particular attention to any lines that have a logging level of warning or error .","title":"ApiServerSource Controller"},{"location":"eventing/debugging/#subscription-controller","text":"The Subscription Controller controls sub . It attempts to resolve the addresses that a Channel should send events to, and once resolved, inject those into the Channel 's spec.subscribable . kubectl --namespace knative-eventing get pod -l app = eventing-controller View its logs with: kubectl --namespace knative-eventing logs -l app = eventing-controller Pay particular attention to any lines that have a logging level of warning or error .","title":"Subscription Controller"},{"location":"eventing/debugging/#data-plane","text":"The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane. The Knative event takes the following path: Event is generated by src . In this case, it is caused by having a Kubernetes Event trigger it, but as far as Knative is concerned, the Source is generating the event denovo (from nothing). src is POSTing the event to chan 's address, http://chan-kn-channel.knative-debug.svc.cluster.local . The Channel Dispatcher receives the request and introspects the Host header to determine which Channel it corresponds to. It sees that it corresponds to knative-debug/chan so forwards the request to the subscribers defined in sub , in particular svc , which is backed by fn . fn receives the request and logs it. We will investigate components in the order in which events should travel.","title":"Data Plane"},{"location":"eventing/debugging/#channel-dispatcher","text":"The Channel Dispatcher is the component that receives POSTs pushing events into Channel s and then POSTs to subscribers of those Channel s when an event is received. For the in-memory-channel used in this example, there is a single binary that handles both the receiving and dispatching sides for all in-memory-channel Channel s. First we will inspect the Dispatcher's logs to see if it is anything obvious: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = dispatcher -c dispatcher Ideally we will see lines like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.424Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.425Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.981Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } Which shows that the request is being received and then sent to svc , which is returning a 2XX response code (likely 200, 202, or 204). However if we see something like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"error\" , \"ts\" : \"2019-08-16T16:10:38.169Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"fanout/fanout_handler.go:121\" , \"msg\" : \"Fanout had an error\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" , \"error\" : \"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\" , \"stacktrace\" : \"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\" } Then we know there was a problem posting to http://svc.knative-debug.svc.cluster.local/ . TODO Finish this section. Especially after the Channel Dispatcher emits K8s events about failures.","title":"Channel Dispatcher"},{"location":"eventing/debugging/#fn_1","text":"TODO Fill in this section.","title":"fn"},{"location":"eventing/debugging/#todo-finish-the-guide","text":"","title":"TODO Finish the guide."},{"location":"eventing/flows/","text":"Eventing Flows \u00b6 Knative Eventing provides a collection of custom resource definitions (CRDs) that you can use to define event flows: Sequence is for defining an in-order list of functions. Parallel is for defining a list of branches, each receiving the same CloudEvent.","title":"Overview"},{"location":"eventing/flows/#eventing-flows","text":"Knative Eventing provides a collection of custom resource definitions (CRDs) that you can use to define event flows: Sequence is for defining an in-order list of functions. Parallel is for defining a list of branches, each receiving the same CloudEvent.","title":"Eventing Flows"},{"location":"eventing/flows/parallel/","text":"Parallel \u00b6 Parallel CRD provides a way to easily define a list of branches, each receiving the same CloudEvent sent to the Parallel ingress channel. Typically, each branch consists of a filter function guarding the execution of the branch. Parallel creates Channel s and Subscription s under the hood. Usage \u00b6 Parallel Spec \u00b6 Parallel has three parts for the Spec: branches defines the list of filter and subscriber pairs, one per branch, and optionally a reply object. For each branch: (optional) the filter is evaluated and when it returns an event the subscriber is executed. Both filter and subscriber must be Addressable . the event returned by the subscriber is sent to the branch reply object. When the reply is empty, the event is sent to the spec.reply object (see below). (optional) channelTemplate defines the Template which will be used to create Channel s. (optional) reply defines where the result of each branch is sent to when the branch does not have its own reply object. Parallel Status \u00b6 Parallel has three parts for the Status: conditions which details the overall status of the Parallel object ingressChannelStatus and branchesStatuses which convey the status of underlying Channel and Subscription resource that are created as part of this Parallel. address which is exposed so that Parallel can be used where Addressable can be used. Sending to this address will target the Channel which is fronting this Parallel (same as ingressChannelStatus ). Examples \u00b6 Learn how to use Parallel by following the examples","title":"Parallel"},{"location":"eventing/flows/parallel/#parallel","text":"Parallel CRD provides a way to easily define a list of branches, each receiving the same CloudEvent sent to the Parallel ingress channel. Typically, each branch consists of a filter function guarding the execution of the branch. Parallel creates Channel s and Subscription s under the hood.","title":"Parallel"},{"location":"eventing/flows/parallel/#usage","text":"","title":"Usage"},{"location":"eventing/flows/parallel/#parallel-spec","text":"Parallel has three parts for the Spec: branches defines the list of filter and subscriber pairs, one per branch, and optionally a reply object. For each branch: (optional) the filter is evaluated and when it returns an event the subscriber is executed. Both filter and subscriber must be Addressable . the event returned by the subscriber is sent to the branch reply object. When the reply is empty, the event is sent to the spec.reply object (see below). (optional) channelTemplate defines the Template which will be used to create Channel s. (optional) reply defines where the result of each branch is sent to when the branch does not have its own reply object.","title":"Parallel Spec"},{"location":"eventing/flows/parallel/#parallel-status","text":"Parallel has three parts for the Status: conditions which details the overall status of the Parallel object ingressChannelStatus and branchesStatuses which convey the status of underlying Channel and Subscription resource that are created as part of this Parallel. address which is exposed so that Parallel can be used where Addressable can be used. Sending to this address will target the Channel which is fronting this Parallel (same as ingressChannelStatus ).","title":"Parallel Status"},{"location":"eventing/flows/parallel/#examples","text":"Learn how to use Parallel by following the examples","title":"Examples"},{"location":"eventing/flows/sequence/","text":"Sequence \u00b6 Sequence CRD provides a way to define an in-order list of functions that will be invoked. Each step can modify, filter or create a new kind of an event. Sequence creates Channel s and Subscription s under the hood. Usage \u00b6 Sequence Spec \u00b6 Sequence has three parts for the Spec: Steps which defines the in-order list of Subscriber s, aka, which functions are executed in the listed order. These are specified using the messaging.v1.SubscriberSpec just like you would when creating Subscription . Each step should be Addressable . ChannelTemplate defines the Template which will be used to create Channel s between the steps. Reply (Optional) Reference to where the results of the final step in the sequence are sent to. Sequence Status \u00b6 Sequence has four parts for the Status: Conditions which detail the overall Status of the Sequence object ChannelStatuses which convey the Status of underlying Channel resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Status of the Channel before the first Step. SubscriptionStatuses which convey the Status of underlying Subscription resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Subscription which is created to wire the first channel to the first step in the Steps array. AddressStatus which is exposed so that Sequence can be used where Addressable can be used. Sending to this address will target the Channel which is fronting the first Step in the Sequence. Examples \u00b6 For each of these examples below, You will use a PingSource as the source of events. We also use a very simple transformer which performs very trivial transformation of the incoming events to demonstrate they have passed through each stage. Sequence with no reply (terminal last Step) \u00b6 For the first example, we'll use a 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. Sequence with reply (last Step produces output) \u00b6 For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to an event display pod. Chaining Sequences together \u00b6 For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to another Sequence that does the same message modifications as the first pipeline (with different steps however). Using Sequence with Broker/Trigger model \u00b6 You can also create a Trigger which targets Sequence . This time we'll wire PingSource to send events to a Broker and then we'll have the Sequence emit the resulting Events back into the Broker so that the results of the Sequence can be observed by other Trigger s.","title":"Overview"},{"location":"eventing/flows/sequence/#sequence","text":"Sequence CRD provides a way to define an in-order list of functions that will be invoked. Each step can modify, filter or create a new kind of an event. Sequence creates Channel s and Subscription s under the hood.","title":"Sequence"},{"location":"eventing/flows/sequence/#usage","text":"","title":"Usage"},{"location":"eventing/flows/sequence/#sequence-spec","text":"Sequence has three parts for the Spec: Steps which defines the in-order list of Subscriber s, aka, which functions are executed in the listed order. These are specified using the messaging.v1.SubscriberSpec just like you would when creating Subscription . Each step should be Addressable . ChannelTemplate defines the Template which will be used to create Channel s between the steps. Reply (Optional) Reference to where the results of the final step in the sequence are sent to.","title":"Sequence Spec"},{"location":"eventing/flows/sequence/#sequence-status","text":"Sequence has four parts for the Status: Conditions which detail the overall Status of the Sequence object ChannelStatuses which convey the Status of underlying Channel resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Status of the Channel before the first Step. SubscriptionStatuses which convey the Status of underlying Subscription resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Subscription which is created to wire the first channel to the first step in the Steps array. AddressStatus which is exposed so that Sequence can be used where Addressable can be used. Sending to this address will target the Channel which is fronting the first Step in the Sequence.","title":"Sequence Status"},{"location":"eventing/flows/sequence/#examples","text":"For each of these examples below, You will use a PingSource as the source of events. We also use a very simple transformer which performs very trivial transformation of the incoming events to demonstrate they have passed through each stage.","title":"Examples"},{"location":"eventing/flows/sequence/#sequence-with-no-reply-terminal-last-step","text":"For the first example, we'll use a 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message.","title":"Sequence with no reply (terminal last Step)"},{"location":"eventing/flows/sequence/#sequence-with-reply-last-step-produces-output","text":"For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to an event display pod.","title":"Sequence with reply (last Step produces output)"},{"location":"eventing/flows/sequence/#chaining-sequences-together","text":"For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to another Sequence that does the same message modifications as the first pipeline (with different steps however).","title":"Chaining Sequences together"},{"location":"eventing/flows/sequence/#using-sequence-with-brokertrigger-model","text":"You can also create a Trigger which targets Sequence . This time we'll wire PingSource to send events to a Broker and then we'll have the Sequence emit the resulting Events back into the Broker so that the results of the Sequence can be observed by other Trigger s.","title":"Using Sequence with Broker/Trigger model"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/","text":"Sequence wired to event-display \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 Change default below to create the steps in the Namespace where you want resources created. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display Change default below to create the Sequence in the Namespace where you want the resources to be created. kubectl -n default create -f ./sequence.yaml Create the Service displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources to be created. kubectl -n default create -f ./event-display.yaml Create the PingSource targeting the Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Wait a bit and then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mode3 source: /apis/v1/namespaces/default/pingsources/ping-source id: e8fa7906-ab62-4e61-9c13-a9406e2130a9 time: 2020 -03-02T20:52:00.0004957Z datacontenttype: application/json Extensions, knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Displaying sequence output"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#sequence-wired-to-event-display","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Sequence wired to event-display"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-knative-services","text":"Change default below to create the steps in the Namespace where you want resources created. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display Change default below to create the Sequence in the Namespace where you want the resources to be created. kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-service-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources to be created. kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-pingsource-targeting-the-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Wait a bit and then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mode3 source: /apis/v1/namespaces/default/pingsources/ping-source id: e8fa7906-ab62-4e61-9c13-a9406e2130a9 time: 2020 -03-02T20:52:00.0004957Z datacontenttype: application/json Extensions, knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/","text":"Sequence wired to another Sequence \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and sending it to a second Sequence and finally displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 Change default below to create the steps in the Namespace where you want resources created. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fourth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 3\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fifth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 4\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sixth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 5\" --- kubectl -n default create -f ./steps.yaml Create the first Sequence \u00b6 The sequence1.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : first-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Sequence apiVersion : flows.knative.dev/v1 name : second-sequence Change default below to create the Sequence in the Namespace where you want your resources created. kubectl -n default create -f ./sequence1.yaml Create the second Sequence \u00b6 The sequence2.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : second-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fourth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fifth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : sixth reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display kubectl -n default create -f ./sequence2.yaml Create the Service displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containerers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources created. kubectl -n default create -f ./event-display.yaml Create the PingSource targeting the first Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : first-sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 29d531df-78d8-4d11-9ffd-ba24045241a9 time: 2020 -03-02T21:18:00.0011708Z datacontenttype: application/json Extensions, knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Using Sequences in series"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#sequence-wired-to-another-sequence","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and sending it to a second Sequence and finally displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Sequence wired to another Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-knative-services","text":"Change default below to create the steps in the Namespace where you want resources created. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fourth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 3\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fifth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 4\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sixth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 5\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-first-sequence","text":"The sequence1.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : first-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Sequence apiVersion : flows.knative.dev/v1 name : second-sequence Change default below to create the Sequence in the Namespace where you want your resources created. kubectl -n default create -f ./sequence1.yaml","title":"Create the first Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-second-sequence","text":"The sequence2.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : second-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fourth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fifth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : sixth reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display kubectl -n default create -f ./sequence2.yaml","title":"Create the second Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-service-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containerers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources created. kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-pingsource-targeting-the-first-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : first-sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the first Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 29d531df-78d8-4d11-9ffd-ba24045241a9 time: 2020 -03-02T21:18:00.0011708Z datacontenttype: application/json Extensions, knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/flows/sequence/sequence-terminal/","text":"Sequence terminal \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence . Sequence can then do either external work, or out of band create additional events. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 First create the 3 steps that will be referenced in the Steps. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third Change default below to create the Sequence in the Namespace where you want the resources to be created. Here, if you are using different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. kubectl -n default create -f ./sequence.yaml Create the PingSource targeting the Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every 2 minutes, it might take some time for the events to show up in the logs. kubectl -n default get pods Let's look at the logs for the first Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = first -c user-container --tail = -1 2020 /03/02 21 :28:00 listening on 8080 , appending \" - Handled by 0\" to events 2020 /03/02 21 :28:01 Received a new event: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! } 2020 /03/02 21 :28:01 Transform the event to: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } And you can see that the initial PingSource message (\"Hello World!\") has now been modified by the first step in the Sequence to include \" - Handled by 0\". Exciting :) Then we can look at the output of the second Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = second -c user-container --tail = -1 2020 /03/02 21 :28:02 listening on 8080 , appending \" - Handled by 1\" to events 2020 /03/02 21 :28:02 Received a new event: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } 2020 /03/02 21 :28:02 Transform the event to: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } And as expected it's now been handled by both the first and second Step as reflected by the Message being now: \"Hello world! - Handled by 0 - Handled by 1\" Then we can look at the output of the last Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = third -c user-container --tail = -1 2020 /03/02 21 :28:03 listening on 8080 , appending \" - Handled by 2\" to events 2020 /03/02 21 :28:03 Received a new event: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } 2020 /03/02 21 :28:03 Transform the event to: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2 }","title":"Create additional events"},{"location":"eventing/flows/sequence/sequence-terminal/#sequence-terminal","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence . Sequence can then do either external work, or out of band create additional events. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Sequence terminal"},{"location":"eventing/flows/sequence/sequence-terminal/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-terminal/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-knative-services","text":"First create the 3 steps that will be referenced in the Steps. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third Change default below to create the Sequence in the Namespace where you want the resources to be created. Here, if you are using different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-pingsource-targeting-the-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Sequence"},{"location":"eventing/flows/sequence/sequence-terminal/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every 2 minutes, it might take some time for the events to show up in the logs. kubectl -n default get pods Let's look at the logs for the first Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = first -c user-container --tail = -1 2020 /03/02 21 :28:00 listening on 8080 , appending \" - Handled by 0\" to events 2020 /03/02 21 :28:01 Received a new event: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! } 2020 /03/02 21 :28:01 Transform the event to: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } And you can see that the initial PingSource message (\"Hello World!\") has now been modified by the first step in the Sequence to include \" - Handled by 0\". Exciting :) Then we can look at the output of the second Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = second -c user-container --tail = -1 2020 /03/02 21 :28:02 listening on 8080 , appending \" - Handled by 1\" to events 2020 /03/02 21 :28:02 Received a new event: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } 2020 /03/02 21 :28:02 Transform the event to: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } And as expected it's now been handled by both the first and second Step as reflected by the Message being now: \"Hello world! - Handled by 0 - Handled by 1\" Then we can look at the output of the last Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = third -c user-container --tail = -1 2020 /03/02 21 :28:03 listening on 8080 , appending \" - Handled by 2\" to events 2020 /03/02 21 :28:03 Received a new event: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } 2020 /03/02 21 :28:03 Transform the event to: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2 }","title":"Inspecting the results"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/","text":"Using Sequence with Broker and Trigger \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events into the Broker, then we create a Filter that wires those events into a Sequence consisting of 3 steps. Then we take the end of the Sequence and feed newly minted events back into the Broker and create another Trigger which will then display those events. Prerequisites \u00b6 Knative Serving InMemoryChannel NOTE: The examples use the default namespace. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Setup \u00b6 Creating the Broker \u00b6 To create the cluster default Broker type: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF Create the Knative Services \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" - name : TYPE value : \"samples.http.mod3\" --- Change the default namespace below to create the services in the namespace where you have configured your broker. kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. Also, change the spec.reply.name to point to your Broker apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Broker apiVersion : eventing.knative.dev/v1 name : default Change the default namespace below to create the sequence in the namespace where you have configured your broker. kubectl -n default create -f ./sequence.yaml Create the PingSource targeting the Broker \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default Change the default namespace below to create the PingSource in the namespace where you have configured your broker and sequence. kubectl -n default create -f ./ping-source.yaml Create the Trigger targeting the Sequence \u00b6 apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : sequence-trigger spec : broker : default filter : attributes : type : dev.knative.sources.ping subscriber : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence Change the default namespace below to create the trigger in the namespace where you have configured your broker and sequence. kubectl -n default create -f ./trigger.yaml Create the Service and Trigger displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sequence-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : display-trigger spec : broker : default filter : attributes : type : samples.http.mod3 subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : sequence-display --- Change default namespace below to create the service and trigger in the namespace where you have configured your broker. kubectl -n default create -f ./display-trigger.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the sequence-display pods. kubectl -n default get pods View the logs for the sequence-display pod: kubectl -n default logs -l serving.knative.dev/service = sequence-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mod3 source: /apis/v1/namespaces/default/pingsources/ping-source id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2 time: 2020 -03-03T14:56:00.000652027Z datacontenttype: application/json Extensions, knativearrivaltime: 2020 -03-03T14:56:00.018390608Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; default-kne-trigger-kn-channel.default.svc.cluster.local traceparent: 00 -e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message {\"Hello World!\"} has been appended to it by each of the steps in the Sequence.","title":"Using with Broker and Trigger"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#using-sequence-with-broker-and-trigger","text":"We are going to create the following logical configuration. We create a PingSource, feeding events into the Broker, then we create a Filter that wires those events into a Sequence consisting of 3 steps. Then we take the end of the Sequence and feed newly minted events back into the Broker and create another Trigger which will then display those events.","title":"Using Sequence with Broker and Trigger"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#prerequisites","text":"Knative Serving InMemoryChannel NOTE: The examples use the default namespace. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#creating-the-broker","text":"To create the cluster default Broker type: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF","title":"Creating the Broker"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-knative-services","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" - name : TYPE value : \"samples.http.mod3\" --- Change the default namespace below to create the services in the namespace where you have configured your broker. kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. Also, change the spec.reply.name to point to your Broker apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Broker apiVersion : eventing.knative.dev/v1 name : default Change the default namespace below to create the sequence in the namespace where you have configured your broker. kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-pingsource-targeting-the-broker","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default Change the default namespace below to create the PingSource in the namespace where you have configured your broker and sequence. kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Broker"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-trigger-targeting-the-sequence","text":"apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : sequence-trigger spec : broker : default filter : attributes : type : dev.knative.sources.ping subscriber : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence Change the default namespace below to create the trigger in the namespace where you have configured your broker and sequence. kubectl -n default create -f ./trigger.yaml","title":"Create the Trigger targeting the Sequence"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-service-and-trigger-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sequence-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : display-trigger spec : broker : default filter : attributes : type : samples.http.mod3 subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : sequence-display --- Change default namespace below to create the service and trigger in the namespace where you have configured your broker. kubectl -n default create -f ./display-trigger.yaml","title":"Create the Service and Trigger displaying the events created by Sequence"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the sequence-display pods. kubectl -n default get pods View the logs for the sequence-display pod: kubectl -n default logs -l serving.knative.dev/service = sequence-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mod3 source: /apis/v1/namespaces/default/pingsources/ping-source id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2 time: 2020 -03-03T14:56:00.000652027Z datacontenttype: application/json Extensions, knativearrivaltime: 2020 -03-03T14:56:00.018390608Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; default-kne-trigger-kn-channel.default.svc.cluster.local traceparent: 00 -e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message {\"Hello World!\"} has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/samples/","text":"Knative Eventing code samples \u00b6 Use the following code samples to help you understand the various use cases for Knative Eventing and Event Sources. Learn more about Knative Eventing and Eventing Sources . See all Knative code samples","title":"Overview"},{"location":"eventing/samples/#knative-eventing-code-samples","text":"Use the following code samples to help you understand the various use cases for Knative Eventing and Event Sources. Learn more about Knative Eventing and Eventing Sources . See all Knative code samples","title":"Knative Eventing code samples"},{"location":"eventing/samples/cloud-audit-logs-source/","text":"CloudAuditLogsSource \u00b6 Please refer to the example in knative-gcp.","title":"CloudAuditLogsSource"},{"location":"eventing/samples/cloud-audit-logs-source/#cloudauditlogssource","text":"Please refer to the example in knative-gcp.","title":"CloudAuditLogsSource"},{"location":"eventing/samples/cloud-pubsub-source/","text":"CloudPubSubSource \u00b6 Please refer to the example in knative-gcp.","title":"CloudPubSubSource"},{"location":"eventing/samples/cloud-pubsub-source/#cloudpubsubsource","text":"Please refer to the example in knative-gcp.","title":"CloudPubSubSource"},{"location":"eventing/samples/cloud-scheduler-source/","text":"CloudSchedulerSource \u00b6 Please refer to the example in knative-gcp.","title":"CloudSchedulerSource"},{"location":"eventing/samples/cloud-scheduler-source/#cloudschedulersource","text":"Please refer to the example in knative-gcp.","title":"CloudSchedulerSource"},{"location":"eventing/samples/cloud-storage-source/","text":"CloudStorageSource \u00b6 Please refer to the example in knative-gcp.","title":"CloudStorageSource"},{"location":"eventing/samples/cloud-storage-source/#cloudstoragesource","text":"Please refer to the example in knative-gcp.","title":"CloudStorageSource"},{"location":"eventing/samples/container-source/","text":"Container Source Example \u00b6 ContainerSource will start a container image which will generate events under certain situations and send messages to a sink URI. It also can be an easy way to support your own event sources in Knative. This guide shows how to configure ContainerSource as an event source for functions and summarizes guidelines for creating your own event source as a ContainerSource. Create a heartbeats ContainerSource \u00b6 Prerequisites \u00b6 Setup Knative Serving . Setup Knative Eventing and Sources . Prepare the heartbeats image \u00b6 Knative event-sources has a sample of heartbeats event source. You could clone the source code by git clone -b \"v0.23.0\" https://github.com/knative/eventing.git And then build a heartbeats image and publish to your image repo with ko publish knative.dev/eventing/cmd/heartbeats Note : ko publish requires: KO_DOCKER_REPO to be set. (e.g. gcr.io/[gcloud-project] or docker.io/<username> ) you to be authenticated with your KO_DOCKER_REPO docker to be installed Create a Knative Service \u00b6 In order to verify ContainerSource is working, we will create a Event Display Service that dumps incoming messages to its log. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Use following command to create the service from service.yaml : kubectl apply --filename service.yaml The status of the created service can be seen using: kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON event-display http://event-display.default.1.2.3.4.xip.io event-display-gqjbw event-display-gqjbw True Create a ContainerSource using the heartbeats image \u00b6 In order to run the heartbeats container as an event source, you have to create a concrete ContainerSource with specific arguments and environment settings. Be sure to replace heartbeats_image_uri with a valid uri for your heartbeats image in your image repo in heartbeats-source.yaml file. Note that arguments and environment variables are set and will be passed to the container. apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : <heartbeats_image_uri> name : heartbeats args : - --period=1 env : - name : POD_NAME value : \"mypod\" - name : POD_NAMESPACE value : \"event-test\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Use the following command to create the event source from heartbeats-source.yaml : kubectl apply --filename heartbeats-source.yaml Verify \u00b6 We will verify that the message was sent to the Knative eventing system by looking at event-display service logs. kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m You should see log lines showing the request headers and body of the event message sent by the heartbeats source to the display function: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" } Create a new event source using ContainerSource \u00b6 In order to create a new event source using ContainerSource, you will create a container image at first, and then create a ContainerSource with the image uri and specify the values of parameters. Develop, build and publish a container image \u00b6 The container image can be developed with any language, build and publish with any tools you like. Here are some basic guidelines: The container image must have a main method to start with. The main method will accept parameters from arguments and environment variables. Two environments variables will be injected by the ContainerSource controller, K_SINK and K_CE_OVERRIDES , resolved from spec.sink and spec.ceOverrides respectively. The event messages shall be sent to the sink URI specified in K_SINK . The message can be any format. CloudEvents format is recommended. heartbeats event source is a sample for your reference. Create the ContainerSource using this container image \u00b6 When the container image is ready, a YAML file will be used to create a concrete ContainerSource . Use heartbeats-source.yaml as a sample for reference. Learn more about the ContainerSource specification .","title":"Container source"},{"location":"eventing/samples/container-source/#container-source-example","text":"ContainerSource will start a container image which will generate events under certain situations and send messages to a sink URI. It also can be an easy way to support your own event sources in Knative. This guide shows how to configure ContainerSource as an event source for functions and summarizes guidelines for creating your own event source as a ContainerSource.","title":"Container Source Example"},{"location":"eventing/samples/container-source/#create-a-heartbeats-containersource","text":"","title":"Create a heartbeats ContainerSource"},{"location":"eventing/samples/container-source/#prerequisites","text":"Setup Knative Serving . Setup Knative Eventing and Sources .","title":"Prerequisites"},{"location":"eventing/samples/container-source/#prepare-the-heartbeats-image","text":"Knative event-sources has a sample of heartbeats event source. You could clone the source code by git clone -b \"v0.23.0\" https://github.com/knative/eventing.git And then build a heartbeats image and publish to your image repo with ko publish knative.dev/eventing/cmd/heartbeats Note : ko publish requires: KO_DOCKER_REPO to be set. (e.g. gcr.io/[gcloud-project] or docker.io/<username> ) you to be authenticated with your KO_DOCKER_REPO docker to be installed","title":"Prepare the heartbeats image"},{"location":"eventing/samples/container-source/#create-a-knative-service","text":"In order to verify ContainerSource is working, we will create a Event Display Service that dumps incoming messages to its log. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Use following command to create the service from service.yaml : kubectl apply --filename service.yaml The status of the created service can be seen using: kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON event-display http://event-display.default.1.2.3.4.xip.io event-display-gqjbw event-display-gqjbw True","title":"Create a Knative Service"},{"location":"eventing/samples/container-source/#create-a-containersource-using-the-heartbeats-image","text":"In order to run the heartbeats container as an event source, you have to create a concrete ContainerSource with specific arguments and environment settings. Be sure to replace heartbeats_image_uri with a valid uri for your heartbeats image in your image repo in heartbeats-source.yaml file. Note that arguments and environment variables are set and will be passed to the container. apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : <heartbeats_image_uri> name : heartbeats args : - --period=1 env : - name : POD_NAME value : \"mypod\" - name : POD_NAMESPACE value : \"event-test\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Use the following command to create the event source from heartbeats-source.yaml : kubectl apply --filename heartbeats-source.yaml","title":"Create a ContainerSource using the heartbeats image"},{"location":"eventing/samples/container-source/#verify","text":"We will verify that the message was sent to the Knative eventing system by looking at event-display service logs. kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m You should see log lines showing the request headers and body of the event message sent by the heartbeats source to the display function: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" }","title":"Verify"},{"location":"eventing/samples/container-source/#create-a-new-event-source-using-containersource","text":"In order to create a new event source using ContainerSource, you will create a container image at first, and then create a ContainerSource with the image uri and specify the values of parameters.","title":"Create a new event source using ContainerSource"},{"location":"eventing/samples/container-source/#develop-build-and-publish-a-container-image","text":"The container image can be developed with any language, build and publish with any tools you like. Here are some basic guidelines: The container image must have a main method to start with. The main method will accept parameters from arguments and environment variables. Two environments variables will be injected by the ContainerSource controller, K_SINK and K_CE_OVERRIDES , resolved from spec.sink and spec.ceOverrides respectively. The event messages shall be sent to the sink URI specified in K_SINK . The message can be any format. CloudEvents format is recommended. heartbeats event source is a sample for your reference.","title":"Develop, build and publish a container image"},{"location":"eventing/samples/container-source/#create-the-containersource-using-this-container-image","text":"When the container image is ready, a YAML file will be used to create a concrete ContainerSource . Use heartbeats-source.yaml as a sample for reference. Learn more about the ContainerSource specification .","title":"Create the ContainerSource using this container image"},{"location":"eventing/samples/github-source/","text":"GitHub source \u00b6 GitHub Source example shows how to wire GitHub events for consumption by a Knative Service. Before you begin \u00b6 Set up Knative Serving . Ensure Knative Serving is configured with a domain name that allows GitHub to call into the cluster. If you're using GKE, you'll also want to assign a static IP address . Set up Knative Eventing with the GitHub source. Create a Knative Service \u00b6 To verify the GitHub source is working, create a simple Knative Service that dumps incoming messages to its log. The service.yaml file defines this basic Service. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : github-message-dumper spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Enter the following command to create the service from service.yaml : kubectl --namespace default apply --filename service.yaml Create GitHub Tokens \u00b6 Create a personal access token for GitHub that the GitHub source can use to register webhooks with the GitHub API. Also decide on a secret token that your code will use to authenticate the incoming webhooks from GitHub ( secretToken ). The token can be named anything you find convenient. The Source requires repo:public_repo and admin:repo_hook , to let it fire events from your public repositories and to create webhooks for those repositories. Copy and save this token; GitHub will force you to generate it again if misplaced. Here's an example for a token named \"GitHubSource Sample\" with the recommended scopes: Update githubsecret.yaml with those values. If your generated access token is 'personal_access_token_value' and you choose your secretToken as 'asdfasfdsaf' , you'd modify githubsecret.yaml like so: apiVersion : v1 kind : Secret metadata : name : githubsecret type : Opaque stringData : accessToken : personal_access_token_value secretToken : asdfasfdsaf Hint: you can makeup a random secretToken with: head -c 8 /dev/urandom | base64 Then, apply the githubsecret using kubectl : kubectl --namespace default apply --filename githubsecret.yaml Create Event Source for GitHub Events \u00b6 In order to receive GitHub events, you have to create a concrete Event Source for a specific namespace. Be sure to replace the ownerAndRepository value with a valid GitHub public repository owned by your GitHub user. If using GitHub enterprise you will need to add an additional githubAPIURL field to the spec specifying your GitHub enterprise API endpoint, see here apiVersion : sources.knative.dev/v1alpha1 kind : GitHubSource metadata : name : githubsourcesample spec : eventTypes : - pull_request ownerAndRepository : <YOUR USER>/<YOUR REPO> accessToken : secretKeyRef : name : githubsecret key : accessToken secretToken : secretKeyRef : name : githubsecret key : secretToken sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : github-message-dumper Then, apply that yaml using kubectl : kubectl --namespace default apply --filename github-source.yaml Verify \u00b6 Verify the GitHub webhook was created by looking at the list of webhooks under the Settings tab in your GitHub repository. A hook should be listed that points to your Knative cluster with a green check mark to the left of the hook URL, as shown below. Create Events \u00b6 Create a pull request in your GitHub repository. We will verify that the GitHub events were sent into the Knative eventing system by looking at our message dumper function logs. kubectl --namespace default get pods kubectl --namespace default logs github-event-display-XXXX user-container You should log lines similar to: 2018/11/08 18:25:34 Message Dumper received a message: POST / HTTP/1.1 Host: github-event-display.knative-demo.svc.cluster.local Accept-Encoding: gzip Ce-Cloudeventsversion: 0.1 Ce-Eventid: a8d4cf20-e383-11e8-8069-46e3c8ad2b4d Ce-Eventtime: 2018-11-08T18:25:32.819548012Z Ce-Eventtype: dev.knative.source.github.pull_request Ce-Source: https://github.com/someuser/somerepo/pull/1 Content-Length: 21060 Content-Type: application/json User-Agent: Go-http-client/1.1 X-B3-Parentspanid: b2e514c3dbe94c03 X-B3-Sampled: 1 X-B3-Spanid: c85e346d89c8be4e X-B3-Traceid: abf6292d458fb8e7 X-Envoy-Expected-Rq-Timeout-Ms: 60000 X-Envoy-Internal: true X-Forwarded-For: 127.0.0.1, 127.0.0.1 X-Forwarded-Proto: http X-Request-Id: 8a2201af-5075-9447-b593-ec3a243aff52 {\"action\":\"opened\",\"number\":1,\"pull_request\": ...} Cleanup \u00b6 You can remove the Github webhook by deleting the Github source: kubectl --namespace default delete --filename github-source.yaml Similarly, you can remove the Service and Secret via: kubectl --namespace default delete --filename service.yaml kubectl --namespace default delete --filename githubsecret.yaml","title":"GitHub source"},{"location":"eventing/samples/github-source/#github-source","text":"GitHub Source example shows how to wire GitHub events for consumption by a Knative Service.","title":"GitHub source"},{"location":"eventing/samples/github-source/#before-you-begin","text":"Set up Knative Serving . Ensure Knative Serving is configured with a domain name that allows GitHub to call into the cluster. If you're using GKE, you'll also want to assign a static IP address . Set up Knative Eventing with the GitHub source.","title":"Before you begin"},{"location":"eventing/samples/github-source/#create-a-knative-service","text":"To verify the GitHub source is working, create a simple Knative Service that dumps incoming messages to its log. The service.yaml file defines this basic Service. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : github-message-dumper spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Enter the following command to create the service from service.yaml : kubectl --namespace default apply --filename service.yaml","title":"Create a Knative Service"},{"location":"eventing/samples/github-source/#create-github-tokens","text":"Create a personal access token for GitHub that the GitHub source can use to register webhooks with the GitHub API. Also decide on a secret token that your code will use to authenticate the incoming webhooks from GitHub ( secretToken ). The token can be named anything you find convenient. The Source requires repo:public_repo and admin:repo_hook , to let it fire events from your public repositories and to create webhooks for those repositories. Copy and save this token; GitHub will force you to generate it again if misplaced. Here's an example for a token named \"GitHubSource Sample\" with the recommended scopes: Update githubsecret.yaml with those values. If your generated access token is 'personal_access_token_value' and you choose your secretToken as 'asdfasfdsaf' , you'd modify githubsecret.yaml like so: apiVersion : v1 kind : Secret metadata : name : githubsecret type : Opaque stringData : accessToken : personal_access_token_value secretToken : asdfasfdsaf Hint: you can makeup a random secretToken with: head -c 8 /dev/urandom | base64 Then, apply the githubsecret using kubectl : kubectl --namespace default apply --filename githubsecret.yaml","title":"Create GitHub Tokens"},{"location":"eventing/samples/github-source/#create-event-source-for-github-events","text":"In order to receive GitHub events, you have to create a concrete Event Source for a specific namespace. Be sure to replace the ownerAndRepository value with a valid GitHub public repository owned by your GitHub user. If using GitHub enterprise you will need to add an additional githubAPIURL field to the spec specifying your GitHub enterprise API endpoint, see here apiVersion : sources.knative.dev/v1alpha1 kind : GitHubSource metadata : name : githubsourcesample spec : eventTypes : - pull_request ownerAndRepository : <YOUR USER>/<YOUR REPO> accessToken : secretKeyRef : name : githubsecret key : accessToken secretToken : secretKeyRef : name : githubsecret key : secretToken sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : github-message-dumper Then, apply that yaml using kubectl : kubectl --namespace default apply --filename github-source.yaml","title":"Create Event Source for GitHub Events"},{"location":"eventing/samples/github-source/#verify","text":"Verify the GitHub webhook was created by looking at the list of webhooks under the Settings tab in your GitHub repository. A hook should be listed that points to your Knative cluster with a green check mark to the left of the hook URL, as shown below.","title":"Verify"},{"location":"eventing/samples/github-source/#create-events","text":"Create a pull request in your GitHub repository. We will verify that the GitHub events were sent into the Knative eventing system by looking at our message dumper function logs. kubectl --namespace default get pods kubectl --namespace default logs github-event-display-XXXX user-container You should log lines similar to: 2018/11/08 18:25:34 Message Dumper received a message: POST / HTTP/1.1 Host: github-event-display.knative-demo.svc.cluster.local Accept-Encoding: gzip Ce-Cloudeventsversion: 0.1 Ce-Eventid: a8d4cf20-e383-11e8-8069-46e3c8ad2b4d Ce-Eventtime: 2018-11-08T18:25:32.819548012Z Ce-Eventtype: dev.knative.source.github.pull_request Ce-Source: https://github.com/someuser/somerepo/pull/1 Content-Length: 21060 Content-Type: application/json User-Agent: Go-http-client/1.1 X-B3-Parentspanid: b2e514c3dbe94c03 X-B3-Sampled: 1 X-B3-Spanid: c85e346d89c8be4e X-B3-Traceid: abf6292d458fb8e7 X-Envoy-Expected-Rq-Timeout-Ms: 60000 X-Envoy-Internal: true X-Forwarded-For: 127.0.0.1, 127.0.0.1 X-Forwarded-Proto: http X-Request-Id: 8a2201af-5075-9447-b593-ec3a243aff52 {\"action\":\"opened\",\"number\":1,\"pull_request\": ...}","title":"Create Events"},{"location":"eventing/samples/github-source/#cleanup","text":"You can remove the Github webhook by deleting the Github source: kubectl --namespace default delete --filename github-source.yaml Similarly, you can remove the Service and Secret via: kubectl --namespace default delete --filename service.yaml kubectl --namespace default delete --filename githubsecret.yaml","title":"Cleanup"},{"location":"eventing/samples/gitlab-source/","text":"GitLab source \u00b6 GitLab Source example shows how to wire GitLab events for consumption by a Knative Service. Gitlab source deployment \u00b6 Prerequisites \u00b6 You will need: An internet-accessible Kubernetes cluster with Knative Serving installed. Follow the installation instructions if you need to create one. Ensure Knative Serving is configured with a domain name that allows GitLab to call into the cluster. If you're using GKE, you'll also want to assign a static IP address . Install Knative Eventing . Install GitLab Event Source \u00b6 GitLab Event source lives in the knative-sandbox/eventing-gitlab . Head to the releases page, find the latest release with gitlab.yaml artifact and replace the <RELEASE> with version tag: kubectl apply -f https://github.com/knative-sandbox/eventing-gitlab/releases/download/<RELEASE>/gitlab.yaml Check that the manager is running: kubectl -n knative-sources get pods --selector control-plane = gitlab-controller-manager With the controller running you can now move on to a user persona and setup a GitLab webhook as well as a function that will consume GitLab events. Using the GitLab Event Source \u00b6 You are now ready to use the Event Source and trigger functions based on GitLab projects events. We will: Create a Knative service which will receive the events. To keep things simple this service will simply dump the events to stdout , this is the so-called: event_display Create a GitLab access token and a random secret token used to secure the webhooks Create the event source by posting a GitLab source object manifest to Kubernetes Create a Knative Service \u00b6 The event-display.yaml file shown below defines the basic service which will receive events from the GitLab source. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : gitlab-event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create the service: kubectl -n default apply -f event-display.yaml Create GitLab Tokens \u00b6 Create a personal access token which the GitLab source will use to register webhooks with the GitLab API. The token must have an \"api\" access scope in order to create repository webhooks. Also decide on a secret token that your source will use to authenticate the incoming webhooks from GitLab. Update a secret values in secret.yaml defined below: accessToken is the personal access token created in step 1 and secretToken is any token of your choosing. Hint: you can generate a random secretToken with: head -c 8 /dev/urandom | base64 secret.yaml : apiVersion : v1 kind : Secret metadata : name : gitlabsecret type : Opaque stringData : accessToken : <personal_access_token_value> secretToken : <random_string> Create the secret using kubectl . kubectl -n default apply -f secret.yaml Create Event Source for GitLab Events \u00b6 In order to receive GitLab events, you have to create a concrete Event Source for a specific namespace. Replace the projectUrl value in the gitlabsource.yaml file with your GitLab project URL, for example https://gitlab.com/knative-examples/functions . gitlabsource.yaml : apiVersion : sources.knative.dev/v1alpha1 kind : GitLabSource metadata : name : gitlabsource-sample spec : eventTypes : - push_events - issues_events projectUrl : <project url> accessToken : secretKeyRef : name : gitlabsecret key : accessToken secretToken : secretKeyRef : name : gitlabsecret key : secretToken sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gitlab-event-display Apply the yaml file using kubectl : kubectl -n default apply -f gitlabsource.yaml Verify \u00b6 Verify that GitLab webhook was created by looking at the list of webhooks under Settings >> Integrations in your GitLab project. A hook should be listed that points to your Knative cluster. Create a push event and check the logs of the Pod backing the gitlab-event-display knative service. You will see the event: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 0.3 type: dev.knative.sources.gitlabsource.Push Hook source: https://gitlab.com/<user>/<project> id: f83c080f-c2af-48ff-8d8b-fd5b21c5938e time: 2020-03-12T11:08:41.414572482Z datacontenttype: application/json Data, { <Event payload> } Cleanup \u00b6 You can remove the GitLab webhook by deleting the GitLab source: kubectl --namespace default delete --filename gitlabsource.yaml Similarly, you can remove the Service and Secret via: kubectl --namespace default delete --filename event-display.yaml kubectl --namespace default delete --filename secret.yaml","title":"GitLab source"},{"location":"eventing/samples/gitlab-source/#gitlab-source","text":"GitLab Source example shows how to wire GitLab events for consumption by a Knative Service.","title":"GitLab source"},{"location":"eventing/samples/gitlab-source/#gitlab-source-deployment","text":"","title":"Gitlab source deployment"},{"location":"eventing/samples/gitlab-source/#prerequisites","text":"You will need: An internet-accessible Kubernetes cluster with Knative Serving installed. Follow the installation instructions if you need to create one. Ensure Knative Serving is configured with a domain name that allows GitLab to call into the cluster. If you're using GKE, you'll also want to assign a static IP address . Install Knative Eventing .","title":"Prerequisites"},{"location":"eventing/samples/gitlab-source/#install-gitlab-event-source","text":"GitLab Event source lives in the knative-sandbox/eventing-gitlab . Head to the releases page, find the latest release with gitlab.yaml artifact and replace the <RELEASE> with version tag: kubectl apply -f https://github.com/knative-sandbox/eventing-gitlab/releases/download/<RELEASE>/gitlab.yaml Check that the manager is running: kubectl -n knative-sources get pods --selector control-plane = gitlab-controller-manager With the controller running you can now move on to a user persona and setup a GitLab webhook as well as a function that will consume GitLab events.","title":"Install GitLab Event Source"},{"location":"eventing/samples/gitlab-source/#using-the-gitlab-event-source","text":"You are now ready to use the Event Source and trigger functions based on GitLab projects events. We will: Create a Knative service which will receive the events. To keep things simple this service will simply dump the events to stdout , this is the so-called: event_display Create a GitLab access token and a random secret token used to secure the webhooks Create the event source by posting a GitLab source object manifest to Kubernetes","title":"Using the GitLab Event Source"},{"location":"eventing/samples/gitlab-source/#create-a-knative-service","text":"The event-display.yaml file shown below defines the basic service which will receive events from the GitLab source. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : gitlab-event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create the service: kubectl -n default apply -f event-display.yaml","title":"Create a Knative Service"},{"location":"eventing/samples/gitlab-source/#create-gitlab-tokens","text":"Create a personal access token which the GitLab source will use to register webhooks with the GitLab API. The token must have an \"api\" access scope in order to create repository webhooks. Also decide on a secret token that your source will use to authenticate the incoming webhooks from GitLab. Update a secret values in secret.yaml defined below: accessToken is the personal access token created in step 1 and secretToken is any token of your choosing. Hint: you can generate a random secretToken with: head -c 8 /dev/urandom | base64 secret.yaml : apiVersion : v1 kind : Secret metadata : name : gitlabsecret type : Opaque stringData : accessToken : <personal_access_token_value> secretToken : <random_string> Create the secret using kubectl . kubectl -n default apply -f secret.yaml","title":"Create GitLab Tokens"},{"location":"eventing/samples/gitlab-source/#create-event-source-for-gitlab-events","text":"In order to receive GitLab events, you have to create a concrete Event Source for a specific namespace. Replace the projectUrl value in the gitlabsource.yaml file with your GitLab project URL, for example https://gitlab.com/knative-examples/functions . gitlabsource.yaml : apiVersion : sources.knative.dev/v1alpha1 kind : GitLabSource metadata : name : gitlabsource-sample spec : eventTypes : - push_events - issues_events projectUrl : <project url> accessToken : secretKeyRef : name : gitlabsecret key : accessToken secretToken : secretKeyRef : name : gitlabsecret key : secretToken sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gitlab-event-display Apply the yaml file using kubectl : kubectl -n default apply -f gitlabsource.yaml","title":"Create Event Source for GitLab Events"},{"location":"eventing/samples/gitlab-source/#verify","text":"Verify that GitLab webhook was created by looking at the list of webhooks under Settings >> Integrations in your GitLab project. A hook should be listed that points to your Knative cluster. Create a push event and check the logs of the Pod backing the gitlab-event-display knative service. You will see the event: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 0.3 type: dev.knative.sources.gitlabsource.Push Hook source: https://gitlab.com/<user>/<project> id: f83c080f-c2af-48ff-8d8b-fd5b21c5938e time: 2020-03-12T11:08:41.414572482Z datacontenttype: application/json Data, { <Event payload> }","title":"Verify"},{"location":"eventing/samples/gitlab-source/#cleanup","text":"You can remove the GitLab webhook by deleting the GitLab source: kubectl --namespace default delete --filename gitlabsource.yaml Similarly, you can remove the Service and Secret via: kubectl --namespace default delete --filename event-display.yaml kubectl --namespace default delete --filename secret.yaml","title":"Cleanup"},{"location":"eventing/samples/helloworld/","text":"Knative Eventing - Hello World app \u00b6 Following examples include a simple web app written in the language of your choice that you can use to test knative eventing. It shows how to consume a CloudEvent in Knative eventing, and optionally how to respond back with another CloudEvent in the HTTP response. We will deploy the app as a Kubernetes Deployment along with a Kubernetes Service . However, you can also deploy the app as a Knative Serving Service Prerequisites \u00b6 A Kubernetes cluster with Knative Eventing installed. If you decide to deploy the app as a Knative Serving Service then you will have to install Knative Serving . Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Overview"},{"location":"eventing/samples/helloworld/#knative-eventing-hello-world-app","text":"Following examples include a simple web app written in the language of your choice that you can use to test knative eventing. It shows how to consume a CloudEvent in Knative eventing, and optionally how to respond back with another CloudEvent in the HTTP response. We will deploy the app as a Kubernetes Deployment along with a Kubernetes Service . However, you can also deploy the app as a Knative Serving Service","title":"Knative Eventing - Hello World app"},{"location":"eventing/samples/helloworld/#prerequisites","text":"A Kubernetes cluster with Knative Eventing installed. If you decide to deploy the app as a Knative Serving Service then you will have to install Knative Serving . Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Prerequisites"},{"location":"eventing/samples/helloworld/helloworld-go/","text":"Hello World - Golang \u00b6 A simple web app written in Go that you can use to test knative eventing. It shows how to consume a CloudEvent in Knative eventing, and optionally how to respond back with another CloudEvent in the http response, using the Go SDK for CloudEvents We will deploy the app as a Kubernetes Deployment along with a Kubernetes Service . However, you can also deploy the app as a Knative Serving Service . Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/helloworld/helloworld-go Before you begin \u00b6 A Kubernetes cluster with Knative Eventing installed. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Recreating the sample code \u00b6 Create a new file named helloworld.go and paste the following code. This code creates a basic web server which listens on port 8080: import ( \"context\" \"log\" cloudevents \"github.com/cloudevents/sdk-go/v2\" \"github.com/google/uuid\" ) func receive ( ctx context . Context , event cloudevents . Event ) ( * cloudevents . Event , cloudevents . Result ) { // Here is where your code to process the event will go. // In this example we will log the event msg log . Printf ( \"Event received. \\n%s\\n\" , event ) data := & HelloWorld {} if err := event . DataAs ( data ); err != nil { log . Printf ( \"Error while extracting cloudevent Data: %s\\n\" , err . Error ()) return nil , cloudevents . NewHTTPResult ( 400 , \"failed to convert data: %s\" , err ) } log . Printf ( \"Hello World Message from received event %q\" , data . Msg ) // Respond with another event (optional) // This is optional and is intended to show how to respond back with another event after processing. // The response will go back into the knative eventing system just like any other event newEvent := cloudevents . NewEvent () newEvent . SetID ( uuid . New (). String ()) newEvent . SetSource ( \"knative/eventing/samples/hello-world\" ) newEvent . SetType ( \"dev.knative.samples.hifromknative\" ) if err := newEvent . SetData ( cloudevents . ApplicationJSON , HiFromKnative { Msg : \"Hi from helloworld-go app!\" }); err != nil { return nil , cloudevents . NewHTTPResult ( 500 , \"failed to set response data: %s\" , err ) } log . Printf ( \"Responding with event\\n%s\\n\" , newEvent ) return & newEvent , nil } func main () { log . Print ( \"Hello world sample started.\" ) c , err := cloudevents . NewDefaultClient () if err != nil { log . Fatalf ( \"failed to create client, %v\" , err ) } log . Fatal ( c . StartReceiver ( context . Background (), receive )) } Create a new file named eventschemas.go and paste the following code. This defines the data schema of the CloudEvents. package main // HelloWorld defines the Data of CloudEvent with type=dev.knative.samples.helloworld type HelloWorld struct { // Msg holds the message from the event Msg string `json:\"msg,omitempty,string\"` } // HiFromKnative defines the Data of CloudEvent with type=dev.knative.samples.hifromknative type HiFromKnative struct { // Msg holds the message from the event Msg string `json:\"msg,omitempty,string\"` } In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Go app, see Deploying Go servers with Docker . # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. # https://hub.docker.com/_/golang FROM golang:1.14 as builder # Copy local code to the container image. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod = readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o helloworld # Use a Docker multi-stage build to create a lean production image. # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/helloworld /helloworld # Run the web service on container startup. CMD [ \"/helloworld\" ] Create a new file, sample-app.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. # Namespace for sample application apiVersion : v1 kind : Namespace metadata : name : knative-samples --- # A default broker apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : knative-samples spec : {} --- # Helloworld-go app deploment apiVersion : apps/v1 kind : Deployment metadata : name : helloworld-go namespace : knative-samples spec : replicas : 1 selector : matchLabels : &labels app : helloworld-go template : metadata : labels : *labels spec : containers : - name : helloworld-go image : docker.io/{username}/helloworld-go --- # Service that exposes helloworld-go app. # This will be the subscriber for the Trigger kind : Service apiVersion : v1 metadata : name : helloworld-go namespace : knative-samples spec : selector : app : helloworld-go ports : - protocol : TCP port : 80 targetPort : 8080 --- # Knative Eventing Trigger to trigger the helloworld-go service apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : helloworld-go namespace : knative-samples spec : broker : default filter : attributes : type : dev.knative.samples.helloworld source : dev.knative.samples/helloworldsource subscriber : ref : apiVersion : v1 kind : Service name : helloworld-go Use the go tool to create a go.mod manifest. go mod init github.com/knative/docs/docs/serving/samples/hello-world/helloworld-go Building and deploying the sample \u00b6 Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-go . # Push the container to docker registry docker push { username } /helloworld-go After the build has completed and the container is pushed to docker hub, you can deploy the sample application into your cluster. Ensure that the container image value in sample-app.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename sample-app.yaml Above command created a namespace knative-samples and create a default Broker it. Verify using the following command: kubectl get broker --namespace knative-samples Note: you can also use injection based on labels with the Eventing sugar controller. For how to install the Eventing sugar controller, see Install optional Eventing extensions . It deployed the helloworld-go app as a K8s Deployment and created a K8s service names helloworld-go. Verify using the following command. kubectl --namespace knative-samples get deployments helloworld-go kubectl --namespace knative-samples get svc helloworld-go It created a Knative Eventing Trigger to route certain events to the helloworld-go application. Make sure that Ready=true kubectl --namespace knative-samples get trigger helloworld-go Send and verify CloudEvents \u00b6 Once you have deployed the application and verified that the namespace, sample application and trigger are ready, let's send a CloudEvent. Send CloudEvent to the Broker \u00b6 We can send an http request directly to the Broker with correct CloudEvent headers set. Deploy a curl pod and SSH into it kubectl --namespace knative-samples run curl --image = radial/busyboxplus:curl -it Get the Broker URL kubectl --namespace knative-samples get broker default Run the following in the SSH terminal. Please replace the URL with the URL of the default broker. curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/knative-samples/default\" \\ -X POST \\ -H \"Ce-Id: 536808d3-88be-4077-9d7a-a3f162705f79\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: dev.knative.samples.helloworld\" \\ -H \"Ce-Source: dev.knative.samples/helloworldsource\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello World from the curl pod.\"}' exit Verify that event is received by helloworld-go app \u00b6 Helloworld-go app logs the context and the msg of the above event, and replies back with another event. Display helloworld-go app logs kubectl --namespace knative-samples logs -l app = helloworld-go --tail = 50 You should see something similar to: Event received. Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.helloworld source: dev.knative.samples/helloworldsource id: 536808d3-88be-4077-9d7a-a3f162705f79 time: 2019 -10-04T22:35:26.05871736Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:35:26Z knativehistory: default-kn2-trigger-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -971d4644229653483d38c46e92a959c7-92c66312e4bb39be-00 Data, { \"msg\" : \"Hello World from the curl pod.\" } Hello World Message \"Hello World from the curl pod.\" Responded with event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 37458d77-01f5-411e-a243-a459bbf79682 datacontenttype: application/json Data, { \"msg\" : \"Hi from Knative!\" } Play around with the CloudEvent attributes in the curl command and the trigger specification to understand how Triggers work . Verify reply from helloworld-go app \u00b6 helloworld-go app replies back with an event of type= dev.knative.samples.hifromknative , and source=knative/eventing/samples/hello-world . This event enters the eventing mesh via the Broker and can be delivered to other services using a Trigger Deploy a pod that receives any CloudEvent and logs the event to its output. kubectl --namespace knative-samples apply --filename - << END # event-display app deploment apiVersion: apps/v1 kind: Deployment metadata: name: event-display namespace: knative-samples spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: helloworld-go # Source code: https://github.com/knative/eventing/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- # Service that exposes event-display app. # This will be the subscriber for the Trigger kind: Service apiVersion: v1 metadata: name: event-display namespace: knative-samples spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 END Create a trigger to deliver the event to the above service kubectl --namespace knative-samples apply --filename - << END apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: event-display namespace: knative-samples spec: broker: default filter: attributes: type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world subscriber: ref: apiVersion: v1 kind: Service name: event-display END Send a CloudEvent to the Broker Check the logs of event-display service kubectl --namespace knative-samples logs -l app = event-display --tail = 50 You should see something similar to: cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 8a7384b9-8bbe-4634-bf0f-ead07e450b2a time: 2019 -10-04T22:53:39.844943931Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:53:39Z knativehistory: default-kn2-ingress-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -4b01db030b9ea04bb150b77c8fa86509-2740816590a7604f-00 Data, { \"msg\" : \"Hi from helloworld-go app!\" } Note: You could use the above approach to test your applications too. Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename sample-app.yaml","title":"GO"},{"location":"eventing/samples/helloworld/helloworld-go/#hello-world-golang","text":"A simple web app written in Go that you can use to test knative eventing. It shows how to consume a CloudEvent in Knative eventing, and optionally how to respond back with another CloudEvent in the http response, using the Go SDK for CloudEvents We will deploy the app as a Kubernetes Deployment along with a Kubernetes Service . However, you can also deploy the app as a Knative Serving Service . Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/helloworld/helloworld-go","title":"Hello World - Golang"},{"location":"eventing/samples/helloworld/helloworld-go/#before-you-begin","text":"A Kubernetes cluster with Knative Eventing installed. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"eventing/samples/helloworld/helloworld-go/#recreating-the-sample-code","text":"Create a new file named helloworld.go and paste the following code. This code creates a basic web server which listens on port 8080: import ( \"context\" \"log\" cloudevents \"github.com/cloudevents/sdk-go/v2\" \"github.com/google/uuid\" ) func receive ( ctx context . Context , event cloudevents . Event ) ( * cloudevents . Event , cloudevents . Result ) { // Here is where your code to process the event will go. // In this example we will log the event msg log . Printf ( \"Event received. \\n%s\\n\" , event ) data := & HelloWorld {} if err := event . DataAs ( data ); err != nil { log . Printf ( \"Error while extracting cloudevent Data: %s\\n\" , err . Error ()) return nil , cloudevents . NewHTTPResult ( 400 , \"failed to convert data: %s\" , err ) } log . Printf ( \"Hello World Message from received event %q\" , data . Msg ) // Respond with another event (optional) // This is optional and is intended to show how to respond back with another event after processing. // The response will go back into the knative eventing system just like any other event newEvent := cloudevents . NewEvent () newEvent . SetID ( uuid . New (). String ()) newEvent . SetSource ( \"knative/eventing/samples/hello-world\" ) newEvent . SetType ( \"dev.knative.samples.hifromknative\" ) if err := newEvent . SetData ( cloudevents . ApplicationJSON , HiFromKnative { Msg : \"Hi from helloworld-go app!\" }); err != nil { return nil , cloudevents . NewHTTPResult ( 500 , \"failed to set response data: %s\" , err ) } log . Printf ( \"Responding with event\\n%s\\n\" , newEvent ) return & newEvent , nil } func main () { log . Print ( \"Hello world sample started.\" ) c , err := cloudevents . NewDefaultClient () if err != nil { log . Fatalf ( \"failed to create client, %v\" , err ) } log . Fatal ( c . StartReceiver ( context . Background (), receive )) } Create a new file named eventschemas.go and paste the following code. This defines the data schema of the CloudEvents. package main // HelloWorld defines the Data of CloudEvent with type=dev.knative.samples.helloworld type HelloWorld struct { // Msg holds the message from the event Msg string `json:\"msg,omitempty,string\"` } // HiFromKnative defines the Data of CloudEvent with type=dev.knative.samples.hifromknative type HiFromKnative struct { // Msg holds the message from the event Msg string `json:\"msg,omitempty,string\"` } In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Go app, see Deploying Go servers with Docker . # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. # https://hub.docker.com/_/golang FROM golang:1.14 as builder # Copy local code to the container image. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod = readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o helloworld # Use a Docker multi-stage build to create a lean production image. # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/helloworld /helloworld # Run the web service on container startup. CMD [ \"/helloworld\" ] Create a new file, sample-app.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. # Namespace for sample application apiVersion : v1 kind : Namespace metadata : name : knative-samples --- # A default broker apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : knative-samples spec : {} --- # Helloworld-go app deploment apiVersion : apps/v1 kind : Deployment metadata : name : helloworld-go namespace : knative-samples spec : replicas : 1 selector : matchLabels : &labels app : helloworld-go template : metadata : labels : *labels spec : containers : - name : helloworld-go image : docker.io/{username}/helloworld-go --- # Service that exposes helloworld-go app. # This will be the subscriber for the Trigger kind : Service apiVersion : v1 metadata : name : helloworld-go namespace : knative-samples spec : selector : app : helloworld-go ports : - protocol : TCP port : 80 targetPort : 8080 --- # Knative Eventing Trigger to trigger the helloworld-go service apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : helloworld-go namespace : knative-samples spec : broker : default filter : attributes : type : dev.knative.samples.helloworld source : dev.knative.samples/helloworldsource subscriber : ref : apiVersion : v1 kind : Service name : helloworld-go Use the go tool to create a go.mod manifest. go mod init github.com/knative/docs/docs/serving/samples/hello-world/helloworld-go","title":"Recreating the sample code"},{"location":"eventing/samples/helloworld/helloworld-go/#building-and-deploying-the-sample","text":"Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-go . # Push the container to docker registry docker push { username } /helloworld-go After the build has completed and the container is pushed to docker hub, you can deploy the sample application into your cluster. Ensure that the container image value in sample-app.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename sample-app.yaml Above command created a namespace knative-samples and create a default Broker it. Verify using the following command: kubectl get broker --namespace knative-samples Note: you can also use injection based on labels with the Eventing sugar controller. For how to install the Eventing sugar controller, see Install optional Eventing extensions . It deployed the helloworld-go app as a K8s Deployment and created a K8s service names helloworld-go. Verify using the following command. kubectl --namespace knative-samples get deployments helloworld-go kubectl --namespace knative-samples get svc helloworld-go It created a Knative Eventing Trigger to route certain events to the helloworld-go application. Make sure that Ready=true kubectl --namespace knative-samples get trigger helloworld-go","title":"Building and deploying the sample"},{"location":"eventing/samples/helloworld/helloworld-go/#send-and-verify-cloudevents","text":"Once you have deployed the application and verified that the namespace, sample application and trigger are ready, let's send a CloudEvent.","title":"Send and verify CloudEvents"},{"location":"eventing/samples/helloworld/helloworld-go/#send-cloudevent-to-the-broker","text":"We can send an http request directly to the Broker with correct CloudEvent headers set. Deploy a curl pod and SSH into it kubectl --namespace knative-samples run curl --image = radial/busyboxplus:curl -it Get the Broker URL kubectl --namespace knative-samples get broker default Run the following in the SSH terminal. Please replace the URL with the URL of the default broker. curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/knative-samples/default\" \\ -X POST \\ -H \"Ce-Id: 536808d3-88be-4077-9d7a-a3f162705f79\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: dev.knative.samples.helloworld\" \\ -H \"Ce-Source: dev.knative.samples/helloworldsource\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello World from the curl pod.\"}' exit","title":"Send CloudEvent to the Broker"},{"location":"eventing/samples/helloworld/helloworld-go/#verify-that-event-is-received-by-helloworld-go-app","text":"Helloworld-go app logs the context and the msg of the above event, and replies back with another event. Display helloworld-go app logs kubectl --namespace knative-samples logs -l app = helloworld-go --tail = 50 You should see something similar to: Event received. Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.helloworld source: dev.knative.samples/helloworldsource id: 536808d3-88be-4077-9d7a-a3f162705f79 time: 2019 -10-04T22:35:26.05871736Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:35:26Z knativehistory: default-kn2-trigger-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -971d4644229653483d38c46e92a959c7-92c66312e4bb39be-00 Data, { \"msg\" : \"Hello World from the curl pod.\" } Hello World Message \"Hello World from the curl pod.\" Responded with event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 37458d77-01f5-411e-a243-a459bbf79682 datacontenttype: application/json Data, { \"msg\" : \"Hi from Knative!\" } Play around with the CloudEvent attributes in the curl command and the trigger specification to understand how Triggers work .","title":"Verify that event is received by helloworld-go app"},{"location":"eventing/samples/helloworld/helloworld-go/#verify-reply-from-helloworld-go-app","text":"helloworld-go app replies back with an event of type= dev.knative.samples.hifromknative , and source=knative/eventing/samples/hello-world . This event enters the eventing mesh via the Broker and can be delivered to other services using a Trigger Deploy a pod that receives any CloudEvent and logs the event to its output. kubectl --namespace knative-samples apply --filename - << END # event-display app deploment apiVersion: apps/v1 kind: Deployment metadata: name: event-display namespace: knative-samples spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: helloworld-go # Source code: https://github.com/knative/eventing/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- # Service that exposes event-display app. # This will be the subscriber for the Trigger kind: Service apiVersion: v1 metadata: name: event-display namespace: knative-samples spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 END Create a trigger to deliver the event to the above service kubectl --namespace knative-samples apply --filename - << END apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: event-display namespace: knative-samples spec: broker: default filter: attributes: type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world subscriber: ref: apiVersion: v1 kind: Service name: event-display END Send a CloudEvent to the Broker Check the logs of event-display service kubectl --namespace knative-samples logs -l app = event-display --tail = 50 You should see something similar to: cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 8a7384b9-8bbe-4634-bf0f-ead07e450b2a time: 2019 -10-04T22:53:39.844943931Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:53:39Z knativehistory: default-kn2-ingress-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -4b01db030b9ea04bb150b77c8fa86509-2740816590a7604f-00 Data, { \"msg\" : \"Hi from helloworld-go app!\" } Note: You could use the above approach to test your applications too.","title":"Verify reply from helloworld-go app"},{"location":"eventing/samples/helloworld/helloworld-go/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename sample-app.yaml","title":"Removing the sample app deployment"},{"location":"eventing/samples/helloworld/helloworld-python/","text":"Hello World - Python \u00b6 A simple web app written in Python that you can use to test knative eventing. It shows how to consume a CloudEvent in Knative eventing, and optionally how to respond back with another CloudEvent in the http response, by adding the Cloud Eventing headers outlined in the Cloud Events standard definition. We will deploy the app as a Kubernetes Deployment along with a Kubernetes Service . However, you can also deploy the app as a Knative Serving Service . Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: # Clone the relevant branch version such as \"release-0.13\" git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/helloworld/helloworld-python Before you begin \u00b6 A Kubernetes cluster with Knative Eventing installed. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Recreating the sample code \u00b6 Create a new file named helloworld.py and paste the following code. This code creates a basic web server which listens on port 8080: from flask import Flask , request , make_response import uuid app = Flask ( __name__ ) @app . route ( '/' , methods = [ 'POST' ]) def hello_world (): app . logger . warning ( request . data ) # Respond with another event (optional) response = make_response ({ \"msg\" : \"Hi from helloworld-python app!\" }) response . headers [ \"Ce-Id\" ] = str ( uuid . uuid4 ()) response . headers [ \"Ce-specversion\" ] = \"0.3\" response . headers [ \"Ce-Source\" ] = \"knative/eventing/samples/hello-world\" response . headers [ \"Ce-Type\" ] = \"dev.knative.samples.hifromknative\" return response if __name__ == '__main__' : app . run ( debug = True , host = '0.0.0.0' , port = 8080 ) Add a requirements.txt file containing the following contents: Flask == 1 .1.1 1. In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Go app, see Deploying Go servers with Docker . FROM python:alpine3.7 COPY . /app WORKDIR /app RUN pip install -r requirements.txt EXPOSE 8080 ENTRYPOINT [ \"python\" ] CMD [ \"helloworld.py\" ] Create a new file, sample-app.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. # Namespace for sample application with eventing enabled apiVersion : v1 kind : Namespace metadata : name : knative-samples labels : eventing.knative.dev/injection : enabled --- # Helloworld-python app deploment apiVersion : apps/v1 kind : Deployment metadata : name : helloworld-python namespace : knative-samples spec : replicas : 1 selector : matchLabels : &labels app : helloworld-python template : metadata : labels : *labels spec : containers : - name : helloworld-python image : docker.io/{username}/helloworld-python imagePullPolicy : IfNotPresent --- # Service that exposes helloworld-python app. # This will be the subscriber for the Trigger kind : Service apiVersion : v1 metadata : name : helloworld-python namespace : knative-samples spec : selector : app : helloworld-python ports : - protocol : TCP port : 80 targetPort : 8080 --- # Knative Eventing Trigger to trigger the helloworld-python service apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : helloworld-python namespace : knative-samples spec : broker : default filter : attributes : type : dev.knative.samples.helloworld source : dev.knative.samples/helloworldsource subscriber : ref : apiVersion : v1 kind : Service name : helloworld-python Building and deploying the sample \u00b6 Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-python . # Push the container to docker registry docker push { username } /helloworld-python After the build has completed and the container is pushed to Docker Hub, you can deploy the sample application into your cluster. Ensure that the container image value in sample-app.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename sample-app.yaml 1. Above command created a namespace knative-samples and labelled it with knative-eventing-injection=enabled , to enable eventing in the namespace. Verify using the following command: kubectl get ns knative-samples --show-labels 1. It deployed the helloworld-python app as a K8s Deployment and created a K8s service names helloworld-python. Verify using the following command. kubectl --namespace knative-samples get deployments helloworld-python kubectl --namespace knative-samples get svc helloworld-python 1. It created a Knative Eventing Trigger to route certain events to the helloworld-python application. Make sure that Ready=true kubectl --namespace knative-samples get trigger helloworld-python Send and verify CloudEvents \u00b6 After you have deployed the application, and have verified that the namespace, sample application and trigger are ready, you can send a CloudEvent. Send CloudEvent to the Broker \u00b6 You can send an HTTP request directly to the Knative broker if the correct CloudEvent headers are set. Deploy a curl pod and SSH into it kubectl --namespace knative-samples run curl --image = radial/busyboxplus:curl -it Run the following in the SSH terminal curl -v \"default-broker.knative-samples.svc.cluster.local\" \\ -X POST \\ -H \"Ce-Id: 536808d3-88be-4077-9d7a-a3f162705f79\" \\ -H \"Ce-specversion: 0.3\" \\ -H \"Ce-Type: dev.knative.samples.helloworld\" \\ -H \"Ce-Source: dev.knative.samples/helloworldsource\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello World from the curl pod.\"}' exit Verify that event is received by helloworld-python app \u00b6 Helloworld-python app logs the context and the msg of the above event, and replies back with another event. 1. Display helloworld-python app logs kubectl --namespace knative-samples logs -l app = helloworld-python --tail = 50 You should see something similar to: Event received. Context: Context Attributes, specversion: 0 .3 type: dev.knative.samples.helloworld source: dev.knative.samples/helloworldsource id: 536808d3-88be-4077-9d7a-a3f162705f79 time: 2019 -10-04T22:35:26.05871736Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:35:26Z knativehistory: default-kn2-trigger-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -971d4644229653483d38c46e92a959c7-92c66312e4bb39be-00 Hello World Message \"Hello World from the curl pod.\" Responded with event Validation: valid Context Attributes, specversion: 0 .2 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 37458d77-01f5-411e-a243-a459bbf79682 Data, { \"msg\" : \"Hi from Knative!\" } Try the CloudEvent attributes in the curl command and the trigger specification to understand how triggers work. Verify reply from helloworld-python app \u00b6 The helloworld-python app replies with an event type type= dev.knative.samples.hifromknative , and source source=knative/eventing/samples/hello-world . The event enters the eventing mesh through the broker, and can be delivered to event sinks using a trigger Deploy a pod that receives any CloudEvent and logs the event to its output. kubectl --namespace knative-samples apply --filename - << END # event-display app deploment apiVersion: apps/v1 kind: Deployment metadata: name: event-display namespace: knative-samples spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: helloworld-python image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- # Service that exposes event-display app. # This will be the subscriber for the Trigger kind: Service apiVersion: v1 metadata: name: event-display namespace: knative-samples spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 END Create a trigger to deliver the event to the above service kubectl --namespace knative-samples apply --filename - << END apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: event-display namespace: knative-samples spec: broker: default filter: attributes: type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world subscriber: ref: apiVersion: v1 kind: Service name: event-display END Send a CloudEvent to the Broker Check the logs of event-display service kubectl --namespace knative-samples logs -l app = event-display --tail = 50 You should see something similar to: cloudevents.Event Validation: valid Context Attributes, specversion: 0 .3 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 8a7384b9-8bbe-4634-bf0f-ead07e450b2a time: 2019 -10-04T22:53:39.844943931Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:53:39Z knativehistory: default-kn2-ingress-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -4b01db030b9ea04bb150b77c8fa86509-2740816590a7604f-00 Data, { \"msg\" : \"Hi from helloworld- app!\" } Note: You could use the above approach to test your applications too. Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename sample-app.yaml","title":"Python"},{"location":"eventing/samples/helloworld/helloworld-python/#hello-world-python","text":"A simple web app written in Python that you can use to test knative eventing. It shows how to consume a CloudEvent in Knative eventing, and optionally how to respond back with another CloudEvent in the http response, by adding the Cloud Eventing headers outlined in the Cloud Events standard definition. We will deploy the app as a Kubernetes Deployment along with a Kubernetes Service . However, you can also deploy the app as a Knative Serving Service . Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: # Clone the relevant branch version such as \"release-0.13\" git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/helloworld/helloworld-python","title":"Hello World - Python"},{"location":"eventing/samples/helloworld/helloworld-python/#before-you-begin","text":"A Kubernetes cluster with Knative Eventing installed. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"eventing/samples/helloworld/helloworld-python/#recreating-the-sample-code","text":"Create a new file named helloworld.py and paste the following code. This code creates a basic web server which listens on port 8080: from flask import Flask , request , make_response import uuid app = Flask ( __name__ ) @app . route ( '/' , methods = [ 'POST' ]) def hello_world (): app . logger . warning ( request . data ) # Respond with another event (optional) response = make_response ({ \"msg\" : \"Hi from helloworld-python app!\" }) response . headers [ \"Ce-Id\" ] = str ( uuid . uuid4 ()) response . headers [ \"Ce-specversion\" ] = \"0.3\" response . headers [ \"Ce-Source\" ] = \"knative/eventing/samples/hello-world\" response . headers [ \"Ce-Type\" ] = \"dev.knative.samples.hifromknative\" return response if __name__ == '__main__' : app . run ( debug = True , host = '0.0.0.0' , port = 8080 ) Add a requirements.txt file containing the following contents: Flask == 1 .1.1 1. In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Go app, see Deploying Go servers with Docker . FROM python:alpine3.7 COPY . /app WORKDIR /app RUN pip install -r requirements.txt EXPOSE 8080 ENTRYPOINT [ \"python\" ] CMD [ \"helloworld.py\" ] Create a new file, sample-app.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. # Namespace for sample application with eventing enabled apiVersion : v1 kind : Namespace metadata : name : knative-samples labels : eventing.knative.dev/injection : enabled --- # Helloworld-python app deploment apiVersion : apps/v1 kind : Deployment metadata : name : helloworld-python namespace : knative-samples spec : replicas : 1 selector : matchLabels : &labels app : helloworld-python template : metadata : labels : *labels spec : containers : - name : helloworld-python image : docker.io/{username}/helloworld-python imagePullPolicy : IfNotPresent --- # Service that exposes helloworld-python app. # This will be the subscriber for the Trigger kind : Service apiVersion : v1 metadata : name : helloworld-python namespace : knative-samples spec : selector : app : helloworld-python ports : - protocol : TCP port : 80 targetPort : 8080 --- # Knative Eventing Trigger to trigger the helloworld-python service apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : helloworld-python namespace : knative-samples spec : broker : default filter : attributes : type : dev.knative.samples.helloworld source : dev.knative.samples/helloworldsource subscriber : ref : apiVersion : v1 kind : Service name : helloworld-python","title":"Recreating the sample code"},{"location":"eventing/samples/helloworld/helloworld-python/#building-and-deploying-the-sample","text":"Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-python . # Push the container to docker registry docker push { username } /helloworld-python After the build has completed and the container is pushed to Docker Hub, you can deploy the sample application into your cluster. Ensure that the container image value in sample-app.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename sample-app.yaml 1. Above command created a namespace knative-samples and labelled it with knative-eventing-injection=enabled , to enable eventing in the namespace. Verify using the following command: kubectl get ns knative-samples --show-labels 1. It deployed the helloworld-python app as a K8s Deployment and created a K8s service names helloworld-python. Verify using the following command. kubectl --namespace knative-samples get deployments helloworld-python kubectl --namespace knative-samples get svc helloworld-python 1. It created a Knative Eventing Trigger to route certain events to the helloworld-python application. Make sure that Ready=true kubectl --namespace knative-samples get trigger helloworld-python","title":"Building and deploying the sample"},{"location":"eventing/samples/helloworld/helloworld-python/#send-and-verify-cloudevents","text":"After you have deployed the application, and have verified that the namespace, sample application and trigger are ready, you can send a CloudEvent.","title":"Send and verify CloudEvents"},{"location":"eventing/samples/helloworld/helloworld-python/#send-cloudevent-to-the-broker","text":"You can send an HTTP request directly to the Knative broker if the correct CloudEvent headers are set. Deploy a curl pod and SSH into it kubectl --namespace knative-samples run curl --image = radial/busyboxplus:curl -it Run the following in the SSH terminal curl -v \"default-broker.knative-samples.svc.cluster.local\" \\ -X POST \\ -H \"Ce-Id: 536808d3-88be-4077-9d7a-a3f162705f79\" \\ -H \"Ce-specversion: 0.3\" \\ -H \"Ce-Type: dev.knative.samples.helloworld\" \\ -H \"Ce-Source: dev.knative.samples/helloworldsource\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello World from the curl pod.\"}' exit","title":"Send CloudEvent to the Broker"},{"location":"eventing/samples/helloworld/helloworld-python/#verify-that-event-is-received-by-helloworld-python-app","text":"Helloworld-python app logs the context and the msg of the above event, and replies back with another event. 1. Display helloworld-python app logs kubectl --namespace knative-samples logs -l app = helloworld-python --tail = 50 You should see something similar to: Event received. Context: Context Attributes, specversion: 0 .3 type: dev.knative.samples.helloworld source: dev.knative.samples/helloworldsource id: 536808d3-88be-4077-9d7a-a3f162705f79 time: 2019 -10-04T22:35:26.05871736Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:35:26Z knativehistory: default-kn2-trigger-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -971d4644229653483d38c46e92a959c7-92c66312e4bb39be-00 Hello World Message \"Hello World from the curl pod.\" Responded with event Validation: valid Context Attributes, specversion: 0 .2 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 37458d77-01f5-411e-a243-a459bbf79682 Data, { \"msg\" : \"Hi from Knative!\" } Try the CloudEvent attributes in the curl command and the trigger specification to understand how triggers work.","title":"Verify that event is received by helloworld-python app"},{"location":"eventing/samples/helloworld/helloworld-python/#verify-reply-from-helloworld-python-app","text":"The helloworld-python app replies with an event type type= dev.knative.samples.hifromknative , and source source=knative/eventing/samples/hello-world . The event enters the eventing mesh through the broker, and can be delivered to event sinks using a trigger Deploy a pod that receives any CloudEvent and logs the event to its output. kubectl --namespace knative-samples apply --filename - << END # event-display app deploment apiVersion: apps/v1 kind: Deployment metadata: name: event-display namespace: knative-samples spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: helloworld-python image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- # Service that exposes event-display app. # This will be the subscriber for the Trigger kind: Service apiVersion: v1 metadata: name: event-display namespace: knative-samples spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 END Create a trigger to deliver the event to the above service kubectl --namespace knative-samples apply --filename - << END apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: event-display namespace: knative-samples spec: broker: default filter: attributes: type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world subscriber: ref: apiVersion: v1 kind: Service name: event-display END Send a CloudEvent to the Broker Check the logs of event-display service kubectl --namespace knative-samples logs -l app = event-display --tail = 50 You should see something similar to: cloudevents.Event Validation: valid Context Attributes, specversion: 0 .3 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 8a7384b9-8bbe-4634-bf0f-ead07e450b2a time: 2019 -10-04T22:53:39.844943931Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:53:39Z knativehistory: default-kn2-ingress-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -4b01db030b9ea04bb150b77c8fa86509-2740816590a7604f-00 Data, { \"msg\" : \"Hi from helloworld- app!\" } Note: You could use the above approach to test your applications too.","title":"Verify reply from helloworld-python app"},{"location":"eventing/samples/helloworld/helloworld-python/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename sample-app.yaml","title":"Removing the sample app deployment"},{"location":"eventing/samples/iot-core/","text":"Binding running services to an IoT core \u00b6 This sample shows how to bind a running service to an IoT core using GCP PubSub as the event source. With minor modifications, it can be used to bind a running service to anything that sends events via GCP PubSub. Note: All commands are given relative to the root of this repository. Deployment Steps \u00b6 Environment Variables \u00b6 To make the following commands easier, we are going to set the various variables here and use them later. Variables you must Change \u00b6 export IOTCORE_PROJECT = \"s9-demo\" Variables you may Change \u00b6 export IOTCORE_REGISTRY = \"iot-demo\" export IOTCORE_DEVICE = \"iot-demo-client\" export IOTCORE_REGION = \"us-central1\" export IOTCORE_TOPIC_DATA = \"iot-demo-pubsub-topic\" export IOTCORE_TOPIC_DEVICE = \"iot-demo-device-pubsub-topic\" Prerequisites \u00b6 Kubernetes \u00b6 Have a running Kubernetes cluster with kubectl pointing at it. GCP \u00b6 Create a Google Cloud Project . Have gcloud installed and pointing at that project. Enable the Cloud Pub/Sub API on that project. gcloud services enable pubsub.googleapis.com Create the two GCP PubSub topic s. gcloud pubsub topics create $IOTCORE_TOPIC_DATA gcloud pubsub topics create $IOTCORE_TOPIC_DEVICE Setup Knative Eventing . GCP PubSub Source \u00b6 Create a GCP Service Account . Determine the Service Account to use, or create a new one. Give that Service Account the 'Pub/Sub Editor' role on your GCP project. Download a new JSON private key for that Service Account. Create two secrets with the downloaded key (one for the Source, one for the Receive Adapter): kubectl --namespace knative-sources create secret generic gcppubsub-source-key --from-file = key.json = PATH_TO_KEY_FILE.json kubectl --namespace default create secret generic google-cloud-key --from-file = key.json = PATH_TO_KEY_FILE.json Deploy the GcpPubSubSource controller as part of eventing-source's controller. kubectl apply --filename https://github.com/knative/eventing-contrib/releases/download/v0.8.2/gcppubsub.yaml Deploying \u00b6 Broker \u00b6 Install the default Broker . kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF GCP PubSub Source \u00b6 Deploy gcp-pubsub-source.yaml . sed -e \"s/PROJECT_ID/ $IOTCORE_PROJECT /\" \\ -e \"s/TOPIC_NAME/ $IOTCORE_TOPIC_DATA /\" \\ docs/eventing/samples/iot-core/gcp-pubsub-source.yaml | kubectl apply --filename - Trigger \u00b6 Even though the Source isn't completely ready yet, we can setup the Trigger for all events coming out of it. Deploy trigger.yaml . kubectl apply --filename docs/eventing/samples/iot-core/trigger.yaml This uses a very simple Knative Service to see that events are flowing. Feel free to replace it. IoT Core \u00b6 We now have everything setup on the Knative side. We will now setup the IoT Core. Create a device registry: gcloud iot registries create $IOTCORE_REGISTRY \\ --project = $IOTCORE_PROJECT \\ --region = $IOTCORE_REGION \\ --event-notification-config = topic = $IOTCORE_TOPIC_DATA \\ --state-pubsub-topic = $IOTCORE_TOPIC_DEVICE Create the certificates. openssl req -x509 -nodes -newkey rsa:2048 \\ -keyout device.key.pem \\ -out device.crt.pem \\ -days 365 \\ -subj \"/CN=unused\" curl https://pki.google.com/roots.pem > ./root-ca.pem Register a device using the generated certificates. gcloud iot devices create $IOTCORE_DEVICE \\ --project = $IOTCORE_PROJECT \\ --region = $IOTCORE_REGION \\ --registry = $IOTCORE_REGISTRY \\ --public-key path = ./device.crt.pem,type = rsa-x509-pem Running \u00b6 We now have everything installed and ready to go. We will generate events and see them in the subscriber. Run the following program to generate events: go run github.com/knative/docs/docs/eventing/samples/iot-core/generator \\ -project $IOTCORE_PROJECT \\ -region $IOTCORE_REGION \\ -registry $IOTCORE_REGISTRY \\ -device $IOTCORE_DEVICE \\ -ca \" $PWD /root-ca.pem\" \\ -key \" $PWD /device.key.pem\" \\ -src \"iot-core demo\" \\ -events 10 Inspect the logs of the subscriber: kubectl logs --selector serving.knative.dev/service = event-display -c user-container You should see something along the similar to: { \"ID\" : \"481014114648052\" , \"Data\" : \"eyJzb3VyY2VfaWQiOiJpb3QtY29yZSBkZW1vIiwiZXZlbnRfaWQiOiJlaWQtMzI3MjJiMzItZWU5Mi00YzZlLWEzOTgtNDlmYjRkYWYyNGE1IiwiZXZlbnRfdHMiOjE1NTM3MTczOTYsIm1ldHJpYyI6MC4xMzY1MjI5OH0=\" , \"Attributes\" : { \"deviceId\" : \"iot-demo-client\" , \"deviceNumId\" : \"2754785852315736\" , \"deviceRegistryId\" : \"iot-demo\" , \"deviceRegistryLocation\" : \"us-central1\" , \"projectId\" : \"s9-demo\" , \"subFolder\" : \"\" } , \"PublishTime\" : \"2019-03-27T20:09:56.685Z\" } Cleanup \u00b6 To cleanup the knative resources: Remove the GcpPubSubSource : sed -e \"s/PROJECT_ID/ $IOTCORE_PROJECT /\" \\ -e \"s/TOPIC_NAME/ $IOTCORE_TOPIC_DATA /\" \\ docs/eventing/samples/iot-core/gcp-pubsub-source.yaml | kubectl delete --filename - Remove the Trigger: kubectl delete --filename docs/eventing/samples/iot-core/trigger.yaml Remove the GcpPubSubSource controller: kubectl delete --filename https://github.com/knative/eventing-contrib/releases/download/v0.8.2/gcppubsub.yaml","title":"IoT core"},{"location":"eventing/samples/iot-core/#binding-running-services-to-an-iot-core","text":"This sample shows how to bind a running service to an IoT core using GCP PubSub as the event source. With minor modifications, it can be used to bind a running service to anything that sends events via GCP PubSub. Note: All commands are given relative to the root of this repository.","title":"Binding running services to an IoT core"},{"location":"eventing/samples/iot-core/#deployment-steps","text":"","title":"Deployment Steps"},{"location":"eventing/samples/iot-core/#environment-variables","text":"To make the following commands easier, we are going to set the various variables here and use them later.","title":"Environment Variables"},{"location":"eventing/samples/iot-core/#variables-you-must-change","text":"export IOTCORE_PROJECT = \"s9-demo\"","title":"Variables you must Change"},{"location":"eventing/samples/iot-core/#variables-you-may-change","text":"export IOTCORE_REGISTRY = \"iot-demo\" export IOTCORE_DEVICE = \"iot-demo-client\" export IOTCORE_REGION = \"us-central1\" export IOTCORE_TOPIC_DATA = \"iot-demo-pubsub-topic\" export IOTCORE_TOPIC_DEVICE = \"iot-demo-device-pubsub-topic\"","title":"Variables you may Change"},{"location":"eventing/samples/iot-core/#prerequisites","text":"","title":"Prerequisites"},{"location":"eventing/samples/iot-core/#kubernetes","text":"Have a running Kubernetes cluster with kubectl pointing at it.","title":"Kubernetes"},{"location":"eventing/samples/iot-core/#gcp","text":"Create a Google Cloud Project . Have gcloud installed and pointing at that project. Enable the Cloud Pub/Sub API on that project. gcloud services enable pubsub.googleapis.com Create the two GCP PubSub topic s. gcloud pubsub topics create $IOTCORE_TOPIC_DATA gcloud pubsub topics create $IOTCORE_TOPIC_DEVICE Setup Knative Eventing .","title":"GCP"},{"location":"eventing/samples/iot-core/#gcp-pubsub-source","text":"Create a GCP Service Account . Determine the Service Account to use, or create a new one. Give that Service Account the 'Pub/Sub Editor' role on your GCP project. Download a new JSON private key for that Service Account. Create two secrets with the downloaded key (one for the Source, one for the Receive Adapter): kubectl --namespace knative-sources create secret generic gcppubsub-source-key --from-file = key.json = PATH_TO_KEY_FILE.json kubectl --namespace default create secret generic google-cloud-key --from-file = key.json = PATH_TO_KEY_FILE.json Deploy the GcpPubSubSource controller as part of eventing-source's controller. kubectl apply --filename https://github.com/knative/eventing-contrib/releases/download/v0.8.2/gcppubsub.yaml","title":"GCP PubSub Source"},{"location":"eventing/samples/iot-core/#deploying","text":"","title":"Deploying"},{"location":"eventing/samples/iot-core/#broker","text":"Install the default Broker . kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF","title":"Broker"},{"location":"eventing/samples/iot-core/#gcp-pubsub-source_1","text":"Deploy gcp-pubsub-source.yaml . sed -e \"s/PROJECT_ID/ $IOTCORE_PROJECT /\" \\ -e \"s/TOPIC_NAME/ $IOTCORE_TOPIC_DATA /\" \\ docs/eventing/samples/iot-core/gcp-pubsub-source.yaml | kubectl apply --filename -","title":"GCP PubSub Source"},{"location":"eventing/samples/iot-core/#trigger","text":"Even though the Source isn't completely ready yet, we can setup the Trigger for all events coming out of it. Deploy trigger.yaml . kubectl apply --filename docs/eventing/samples/iot-core/trigger.yaml This uses a very simple Knative Service to see that events are flowing. Feel free to replace it.","title":"Trigger"},{"location":"eventing/samples/iot-core/#iot-core","text":"We now have everything setup on the Knative side. We will now setup the IoT Core. Create a device registry: gcloud iot registries create $IOTCORE_REGISTRY \\ --project = $IOTCORE_PROJECT \\ --region = $IOTCORE_REGION \\ --event-notification-config = topic = $IOTCORE_TOPIC_DATA \\ --state-pubsub-topic = $IOTCORE_TOPIC_DEVICE Create the certificates. openssl req -x509 -nodes -newkey rsa:2048 \\ -keyout device.key.pem \\ -out device.crt.pem \\ -days 365 \\ -subj \"/CN=unused\" curl https://pki.google.com/roots.pem > ./root-ca.pem Register a device using the generated certificates. gcloud iot devices create $IOTCORE_DEVICE \\ --project = $IOTCORE_PROJECT \\ --region = $IOTCORE_REGION \\ --registry = $IOTCORE_REGISTRY \\ --public-key path = ./device.crt.pem,type = rsa-x509-pem","title":"IoT Core"},{"location":"eventing/samples/iot-core/#running","text":"We now have everything installed and ready to go. We will generate events and see them in the subscriber. Run the following program to generate events: go run github.com/knative/docs/docs/eventing/samples/iot-core/generator \\ -project $IOTCORE_PROJECT \\ -region $IOTCORE_REGION \\ -registry $IOTCORE_REGISTRY \\ -device $IOTCORE_DEVICE \\ -ca \" $PWD /root-ca.pem\" \\ -key \" $PWD /device.key.pem\" \\ -src \"iot-core demo\" \\ -events 10 Inspect the logs of the subscriber: kubectl logs --selector serving.knative.dev/service = event-display -c user-container You should see something along the similar to: { \"ID\" : \"481014114648052\" , \"Data\" : \"eyJzb3VyY2VfaWQiOiJpb3QtY29yZSBkZW1vIiwiZXZlbnRfaWQiOiJlaWQtMzI3MjJiMzItZWU5Mi00YzZlLWEzOTgtNDlmYjRkYWYyNGE1IiwiZXZlbnRfdHMiOjE1NTM3MTczOTYsIm1ldHJpYyI6MC4xMzY1MjI5OH0=\" , \"Attributes\" : { \"deviceId\" : \"iot-demo-client\" , \"deviceNumId\" : \"2754785852315736\" , \"deviceRegistryId\" : \"iot-demo\" , \"deviceRegistryLocation\" : \"us-central1\" , \"projectId\" : \"s9-demo\" , \"subFolder\" : \"\" } , \"PublishTime\" : \"2019-03-27T20:09:56.685Z\" }","title":"Running"},{"location":"eventing/samples/iot-core/#cleanup","text":"To cleanup the knative resources: Remove the GcpPubSubSource : sed -e \"s/PROJECT_ID/ $IOTCORE_PROJECT /\" \\ -e \"s/TOPIC_NAME/ $IOTCORE_TOPIC_DATA /\" \\ docs/eventing/samples/iot-core/gcp-pubsub-source.yaml | kubectl delete --filename - Remove the Trigger: kubectl delete --filename docs/eventing/samples/iot-core/trigger.yaml Remove the GcpPubSubSource controller: kubectl delete --filename https://github.com/knative/eventing-contrib/releases/download/v0.8.2/gcppubsub.yaml","title":"Cleanup"},{"location":"eventing/samples/kafka/","text":"Apache Kafka examples \u00b6 The following examples will help you understand how to use the different Apache Kafka components for Knative. Prerequisites \u00b6 All examples require: A Kubernetes cluster with Knative Eventing v0.9+ Knative Serving v0.9+ An Apache Kafka cluster Setting up Apache Kafka \u00b6 If you want to run the Apache Kafka cluster on Kubernetes, the simplest option is to install it by using Strimzi . Create a namespace for your Apache Kafka installation, like kafka : kubectl create namespace kafka Install the Strimzi operator, like: curl -L \"https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.16.2/strimzi-cluster-operator-0.16.2.yaml\" \\ | sed 's/namespace: .*/namespace: kafka/' \\ | kubectl -n kafka apply -f - Describe the size of your Apache Kafka installation in kafka.yaml , like: apiVersion : kafka.strimzi.io/v1beta1 kind : Kafka metadata : name : my-cluster spec : kafka : version : 2.4.0 replicas : 1 listeners : plain : {} tls : {} config : offsets.topic.replication.factor : 1 transaction.state.log.replication.factor : 1 transaction.state.log.min.isr : 1 log.message.format.version : \"2.4\" storage : type : ephemeral zookeeper : replicas : 3 storage : type : ephemeral entityOperator : topicOperator : {} userOperator : {} Deploy the Apache Kafka cluster $ kubectl apply -n kafka -f kafka.yaml This will install a small, non-production, cluster of Apache Kafka. To verify your installation, check if the pods for Strimzi are all up, in the kafka namespace: $ kubectl get pods -n kafka NAME READY STATUS RESTARTS AGE my-cluster-entity-operator-65995cf856-ld2zp 3 /3 Running 0 102s my-cluster-kafka-0 2 /2 Running 0 2m8s my-cluster-zookeeper-0 2 /2 Running 0 2m39s my-cluster-zookeeper-1 2 /2 Running 0 2m49s my-cluster-zookeeper-2 2 /2 Running 0 2m59s strimzi-cluster-operator-77555d4b69-sbrt4 1 /1 Running 0 3m14s NOTE: For production ready installs check Strimzi . Installation script \u00b6 If you want to install the latest version of Strimzi, in just one step, we have a script for your convenience, which does exactly the same steps that are listed above: $ ./kafka_setup.sh Examples of Apache Kafka and Knative \u00b6 A number of different examples, showing the KafkaSource , KafkaChannel and KafkaBinding can be found here: KafkaSource to Service KafkaChannel and Broker KafkaBinding","title":"Overview"},{"location":"eventing/samples/kafka/#apache-kafka-examples","text":"The following examples will help you understand how to use the different Apache Kafka components for Knative.","title":"Apache Kafka examples"},{"location":"eventing/samples/kafka/#prerequisites","text":"All examples require: A Kubernetes cluster with Knative Eventing v0.9+ Knative Serving v0.9+ An Apache Kafka cluster","title":"Prerequisites"},{"location":"eventing/samples/kafka/#setting-up-apache-kafka","text":"If you want to run the Apache Kafka cluster on Kubernetes, the simplest option is to install it by using Strimzi . Create a namespace for your Apache Kafka installation, like kafka : kubectl create namespace kafka Install the Strimzi operator, like: curl -L \"https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.16.2/strimzi-cluster-operator-0.16.2.yaml\" \\ | sed 's/namespace: .*/namespace: kafka/' \\ | kubectl -n kafka apply -f - Describe the size of your Apache Kafka installation in kafka.yaml , like: apiVersion : kafka.strimzi.io/v1beta1 kind : Kafka metadata : name : my-cluster spec : kafka : version : 2.4.0 replicas : 1 listeners : plain : {} tls : {} config : offsets.topic.replication.factor : 1 transaction.state.log.replication.factor : 1 transaction.state.log.min.isr : 1 log.message.format.version : \"2.4\" storage : type : ephemeral zookeeper : replicas : 3 storage : type : ephemeral entityOperator : topicOperator : {} userOperator : {} Deploy the Apache Kafka cluster $ kubectl apply -n kafka -f kafka.yaml This will install a small, non-production, cluster of Apache Kafka. To verify your installation, check if the pods for Strimzi are all up, in the kafka namespace: $ kubectl get pods -n kafka NAME READY STATUS RESTARTS AGE my-cluster-entity-operator-65995cf856-ld2zp 3 /3 Running 0 102s my-cluster-kafka-0 2 /2 Running 0 2m8s my-cluster-zookeeper-0 2 /2 Running 0 2m39s my-cluster-zookeeper-1 2 /2 Running 0 2m49s my-cluster-zookeeper-2 2 /2 Running 0 2m59s strimzi-cluster-operator-77555d4b69-sbrt4 1 /1 Running 0 3m14s NOTE: For production ready installs check Strimzi .","title":"Setting up Apache Kafka"},{"location":"eventing/samples/kafka/#installation-script","text":"If you want to install the latest version of Strimzi, in just one step, we have a script for your convenience, which does exactly the same steps that are listed above: $ ./kafka_setup.sh","title":"Installation script"},{"location":"eventing/samples/kafka/#examples-of-apache-kafka-and-knative","text":"A number of different examples, showing the KafkaSource , KafkaChannel and KafkaBinding can be found here: KafkaSource to Service KafkaChannel and Broker KafkaBinding","title":"Examples of Apache Kafka and Knative"},{"location":"eventing/samples/kafka/binding/","text":"Apache Kafka Binding Example \u00b6 KafkaBinding is responsible for injecting Kafka bootstrap connection information into a Kubernetes resource that embed a PodSpec (as spec.template.spec ). This enables easy bootstrapping of a Kafka client. Create a Job that uses KafkaBinding \u00b6 In the below example a Kubernetes Job will be using the KafkaBinding to produce messages on a Kafka Topic, which will be received by the Event Display service via Kafka Source Prerequisites \u00b6 You must ensure that you meet the prerequisites listed in the Apache Kafka overview . This feature is available from Knative Eventing 0.15+ Creating a KafkaSource source CRD \u00b6 Install the KafkaSource sub-component to your Knative cluster: kubectl apply -f https://storage.googleapis.com/knative-releases/eventing-contrib/latest/kafka-source.yaml Check that the kafka-controller-manager-0 pod is running. kubectl get pods --namespace knative-sources NAME READY STATUS RESTARTS AGE kafka-controller-manager-0 1/1 Running 0 42m Create the Event Display service \u00b6 (Optional) Source code for Event Display service Get the source code of Event Display container image from here Deploy the Event Display Service via kubectl: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display $ kubectl apply --filename event-display.yaml ... service.serving.knative.dev/event-display created (Optional) Deploy the Event Display Service via kn cli: Alternatively, you can create the knative service using the kn cli like below kn service create event-display --image=gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Ensure that the Service pod is running. The pod name will be prefixed with event-display . $ kubectl get pods NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2/2 Running 0 72s ... Apache Kafka Event Source \u00b6 Modify event-source.yaml accordingly with bootstrap servers, topics, etc...: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 #note the kafka namespace topics : - logs sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source. $ kubectl apply -f event-source.yaml ... kafkasource.sources.knative.dev/kafka-source created Check that the event source pod is running. The pod name will be prefixed with kafka-source . $ kubectl get pods NAME READY STATUS RESTARTS AGE kafka-source-xlnhq-5544766765-dnl5s 1/1 Running 0 40m Kafka Binding Resource \u00b6 Create the KafkaBinding that will inject kafka bootstrap information into select Jobs : Modify kafka-binding.yaml accordingly with bootstrap servers etc...: apiVersion : bindings.knative.dev/v1beta1 kind : KafkaBinding metadata : name : kafka-binding-test spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : kafka.topic : \"logs\" bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 In this case, we will bind any Job with the labels kafka.topic: \"logs\" . Create Kubernetes Job \u00b6 Source code for kafka-publisher service Get the source code of kafka-publisher container image from here Now we will use the kafka-publisher container to send events to kafka topic when the Job runs. apiVersion : batch/v1 kind : Job metadata : labels : kafka.topic : \"logs\" name : kafka-publisher-job spec : backoffLimit : 1 completions : 1 parallelism : 1 template : metadata : annotations : sidecar.istio.io/inject : \"false\" spec : restartPolicy : Never containers : - image : docker.io/murugappans/kafka-publisher-1974f83e2ff7c8994707b5e8731528e8@sha256:fd79490514053c643617dc72a43097251fed139c966fd5d131134a0e424882de env : - name : KAFKA_TOPIC value : \"logs\" - name : KAFKA_KEY value : \"0\" - name : KAFKA_HEADERS value : \"content-type:application/json\" - name : KAFKA_VALUE value : '{\"msg\":\"This is a test!\"}' name : kafka-publisher 1. Check that the Job has run successfully. $ kubectl get jobs NAME COMPLETIONS DURATION AGE kafka-publisher-job 1/1 7s 7s Verify \u00b6 Ensure the Event Display received the message sent to it by the Event Source. $ kubectl logs --selector='serving.knative.dev/service=event-display' -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#logs subject: partition:0#1 id: partition:0/offset:1 time: 2020-05-17T19:45:02.7Z datacontenttype: application/json Extensions, kafkaheadercontenttype: application/json key: 0 traceparent: 00-f383b779f512358b24ffbf6556a6d6da-cacdbe78ef9b5ad3-00 Data, { \"msg\": \"This is a test!\" } Connecting to a TLS enabled Kafka broker \u00b6 The KafkaBinding supports TLS and SASL authentication methods. For injecting TLS authentication, please have the below files CA Certificate Client Certificate and Key These files are expected to be in pem format, if it is in other format like jks , please convert to pem. Create the certificate files as secrets in the namespace where KafkaBinding is going to be set up $ kubectl create secret generic cacert --from-file=caroot.pem secret/cacert created $ kubectl create secret tls kafka-secret --cert=certificate.pem --key=key.pem secret/key created Apply the kafkabinding-tls.yaml, change bootstrapServers accordingly. apiVersion : sources.knative.dev/v1beta1 kind : KafkaBinding metadata : name : kafka-source-with-tls spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : kafka.topic : \"logs\" net : tls : enable : true cert : secretKeyRef : key : tls.crt name : kafka-secret key : secretKeyRef : key : tls.key name : kafka-secret caCert : secretKeyRef : key : caroot.pem name : cacert consumerGroup : knative-group bootstrapServers : - my-secure-kafka-bootstrap.kafka:443","title":"Binding Example"},{"location":"eventing/samples/kafka/binding/#apache-kafka-binding-example","text":"KafkaBinding is responsible for injecting Kafka bootstrap connection information into a Kubernetes resource that embed a PodSpec (as spec.template.spec ). This enables easy bootstrapping of a Kafka client.","title":"Apache Kafka Binding Example"},{"location":"eventing/samples/kafka/binding/#create-a-job-that-uses-kafkabinding","text":"In the below example a Kubernetes Job will be using the KafkaBinding to produce messages on a Kafka Topic, which will be received by the Event Display service via Kafka Source","title":"Create a Job that uses KafkaBinding"},{"location":"eventing/samples/kafka/binding/#prerequisites","text":"You must ensure that you meet the prerequisites listed in the Apache Kafka overview . This feature is available from Knative Eventing 0.15+","title":"Prerequisites"},{"location":"eventing/samples/kafka/binding/#creating-a-kafkasource-source-crd","text":"Install the KafkaSource sub-component to your Knative cluster: kubectl apply -f https://storage.googleapis.com/knative-releases/eventing-contrib/latest/kafka-source.yaml Check that the kafka-controller-manager-0 pod is running. kubectl get pods --namespace knative-sources NAME READY STATUS RESTARTS AGE kafka-controller-manager-0 1/1 Running 0 42m","title":"Creating a KafkaSource source CRD"},{"location":"eventing/samples/kafka/binding/#create-the-event-display-service","text":"(Optional) Source code for Event Display service Get the source code of Event Display container image from here Deploy the Event Display Service via kubectl: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display $ kubectl apply --filename event-display.yaml ... service.serving.knative.dev/event-display created (Optional) Deploy the Event Display Service via kn cli: Alternatively, you can create the knative service using the kn cli like below kn service create event-display --image=gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Ensure that the Service pod is running. The pod name will be prefixed with event-display . $ kubectl get pods NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2/2 Running 0 72s ...","title":"Create the Event Display service"},{"location":"eventing/samples/kafka/binding/#apache-kafka-event-source","text":"Modify event-source.yaml accordingly with bootstrap servers, topics, etc...: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 #note the kafka namespace topics : - logs sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source. $ kubectl apply -f event-source.yaml ... kafkasource.sources.knative.dev/kafka-source created Check that the event source pod is running. The pod name will be prefixed with kafka-source . $ kubectl get pods NAME READY STATUS RESTARTS AGE kafka-source-xlnhq-5544766765-dnl5s 1/1 Running 0 40m","title":"Apache Kafka Event Source"},{"location":"eventing/samples/kafka/binding/#kafka-binding-resource","text":"Create the KafkaBinding that will inject kafka bootstrap information into select Jobs : Modify kafka-binding.yaml accordingly with bootstrap servers etc...: apiVersion : bindings.knative.dev/v1beta1 kind : KafkaBinding metadata : name : kafka-binding-test spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : kafka.topic : \"logs\" bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 In this case, we will bind any Job with the labels kafka.topic: \"logs\" .","title":"Kafka Binding Resource"},{"location":"eventing/samples/kafka/binding/#create-kubernetes-job","text":"Source code for kafka-publisher service Get the source code of kafka-publisher container image from here Now we will use the kafka-publisher container to send events to kafka topic when the Job runs. apiVersion : batch/v1 kind : Job metadata : labels : kafka.topic : \"logs\" name : kafka-publisher-job spec : backoffLimit : 1 completions : 1 parallelism : 1 template : metadata : annotations : sidecar.istio.io/inject : \"false\" spec : restartPolicy : Never containers : - image : docker.io/murugappans/kafka-publisher-1974f83e2ff7c8994707b5e8731528e8@sha256:fd79490514053c643617dc72a43097251fed139c966fd5d131134a0e424882de env : - name : KAFKA_TOPIC value : \"logs\" - name : KAFKA_KEY value : \"0\" - name : KAFKA_HEADERS value : \"content-type:application/json\" - name : KAFKA_VALUE value : '{\"msg\":\"This is a test!\"}' name : kafka-publisher 1. Check that the Job has run successfully. $ kubectl get jobs NAME COMPLETIONS DURATION AGE kafka-publisher-job 1/1 7s 7s","title":"Create Kubernetes Job"},{"location":"eventing/samples/kafka/binding/#verify","text":"Ensure the Event Display received the message sent to it by the Event Source. $ kubectl logs --selector='serving.knative.dev/service=event-display' -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#logs subject: partition:0#1 id: partition:0/offset:1 time: 2020-05-17T19:45:02.7Z datacontenttype: application/json Extensions, kafkaheadercontenttype: application/json key: 0 traceparent: 00-f383b779f512358b24ffbf6556a6d6da-cacdbe78ef9b5ad3-00 Data, { \"msg\": \"This is a test!\" }","title":"Verify"},{"location":"eventing/samples/kafka/binding/#connecting-to-a-tls-enabled-kafka-broker","text":"The KafkaBinding supports TLS and SASL authentication methods. For injecting TLS authentication, please have the below files CA Certificate Client Certificate and Key These files are expected to be in pem format, if it is in other format like jks , please convert to pem. Create the certificate files as secrets in the namespace where KafkaBinding is going to be set up $ kubectl create secret generic cacert --from-file=caroot.pem secret/cacert created $ kubectl create secret tls kafka-secret --cert=certificate.pem --key=key.pem secret/key created Apply the kafkabinding-tls.yaml, change bootstrapServers accordingly. apiVersion : sources.knative.dev/v1beta1 kind : KafkaBinding metadata : name : kafka-source-with-tls spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : kafka.topic : \"logs\" net : tls : enable : true cert : secretKeyRef : key : tls.crt name : kafka-secret key : secretKeyRef : key : tls.key name : kafka-secret caCert : secretKeyRef : key : caroot.pem name : cacert consumerGroup : knative-group bootstrapServers : - my-secure-kafka-bootstrap.kafka:443","title":"Connecting to a TLS enabled Kafka broker"},{"location":"eventing/samples/kafka/channel/","text":"Apache Kafka Channel Example \u00b6 You can install and configure the Apache Kafka CRD ( KafkaChannel ) as the default channel configuration in Knative Eventing. Prerequisites \u00b6 Ensure that you meet the prerequisites listed in the Apache Kafka overview . A Kubernetes cluster with Knative Kafka Channel installed . Creating a KafkaChannel channel CRD \u00b6 Create a new object by configuring the YAML file as follows: cat <<-EOF | kubectl apply -f - --- apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel metadata: name: my-kafka-channel spec: numPartitions: 3 replicationFactor: 1 EOF Specifying the default channel configuration \u00b6 To configure the usage of the KafkaChannel CRD as the default channel configuration , edit the default-ch-webhook ConfigMap as follows: cat <<-EOF | kubectl apply -f - --- apiVersion: v1 kind: ConfigMap metadata: name: default-ch-webhook namespace: knative-eventing data: # Configuration for defaulting channels that do not specify CRD implementations. default-ch-config: | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 EOF Creating an Apache Kafka channel using the default channel configuration \u00b6 Now that KafkaChannel is set as the default channel configuration, you can use the channels.messaging.knative.dev CRD to create a new Apache Kafka channel, using the generic Channel : cat <<-EOF | kubectl apply -f - --- apiVersion: messaging.knative.dev/v1 kind: Channel metadata: name: testchannel-one EOF Check Kafka for a testchannel topic. With Strimzi this can be done by using the command: kubectl -n kafka exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper localhost:2181 --list The result is: ... knative-messaging-kafka.default.testchannel-one ... The Apache Kafka topic that is created by the channel implementation is prefixed with knative-messaging-kafka . This indicates it is an Apache Kafka channel from Knative. It contains the name of the namespace, default in this example, followed by the actual name of the channel. Configuring the Knative broker for Apache Kafka channels \u00b6 To setup a broker that will use the new default Kafka channels, you must create a new default broker, using the command: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF This will give you two pods, such as: default-broker-filter-64658fc79f-nf596 1/1 Running 0 15m default-broker-ingress-ff79755b6-vj9jt 1/1 Running 0 15m Inside the Apache Kafka cluster you should see two new topics, such as: ... knative-messaging-kafka.default.default-kn2-ingress knative-messaging-kafka.default.default-kn2-trigger ... Creating a service and trigger to use the Apache Kafka broker \u00b6 To use the Apache Kafka based broker, let's take a look at a simple demo. Use the ApiServerSource to publish events to the broker as well as the Trigger API, which then routes events to a Knative Service . Install ksvc , using the command: kubectl apply -f 000-ksvc.yaml Install a source that publishes to the default broker kubectl apply -f 020-k8s-events.yaml Create a trigger that routes the events to the ksvc : kubectl apply -f 030-trigger.yaml Verifying your Apache Kafka channel and broker \u00b6 Now that your Eventing cluster is configured for Apache Kafka, you can verify your configuration with the following options. Receive events via Knative \u00b6 Now you can see the events in the log of the ksvc using the command: kubectl logs --selector='serving.knative.dev/service=broker-kafka-display' -c user-container Authentication against an Apache Kafka \u00b6 In production environments it is common that the Apache Kafka cluster is secured using TLS or SASL . This section shows how to confiugure the KafkaChannel to work against a protected Apache Kafka cluster, with the two supported TLS and SASL authentication methods. TLS authentication \u00b6 To use TLS authentication you must create: A CA certificate A client certificate and key NOTE: Kafka channels require these files to be in .pem format. If your files are in a different format, you must convert them to .pem . Create the certificate files as secrets in your chosen namespace: $ kubectl create secret --namespace <namespace> generic <kafka-auth-secret> \\ --from-file=ca.crt=caroot.pem \\ --from-file=user.crt=certificate.pem \\ --from-file=user.key=key.pem ``` *NOTE:* It is important to use the same keys (`ca.crt`, `user.crt` and `user.key`). Reference your secret and the namespace of the secret in the `config-kafka` ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: config-kafka namespace: knative-eventing data: bootstrapServers: authSecretName: authSecretNamespace: ``` SASL authentication \u00b6 To use SASL authentication, you will need the following information: A username and password. The type of SASL mechanism you wish to use. For example; PLAIN , SCRAM-SHA-256 or SCRAM-SHA-512 . NOTE: It is recommended to also enable TLS. If you enable this, you will also need the ca.crt certificate as described in the previous section. Create the certificate files as secrets in your chosen namespace: $ kubectl create secret --namespace <namespace> generic <kafka-auth-secret> \\ --from-file=ca.crt=caroot.pem \\ --from-literal=password=\"SecretPassword\" \\ --from-literal=saslType=\"SCRAM-SHA-512\" \\ --from-literal=user=\"my-sasl-user\" NOTE: It is important to use the same keys; user , password and saslType . SASL authentication using public CA certificates \u00b6 If you want to use SASL with public CA certificates, you must use the tls.enabled=true flag, rather than the ca.crt argument, when creating the secret. For example: $ kubectl create secret --namespace <namespace> generic <kafka-auth-secret> \\ --from-literal=tls.enabled=true \\ --from-literal=password=\"SecretPassword\" \\ --from-literal=saslType=\"SCRAM-SHA-512\" \\ --from-literal=user=\"my-sasl-user\" Reference your secret and the namespace of the secret in the config-kafka ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: config-kafka namespace: knative-eventing data: bootstrapServers: <bootstrap-servers> authSecretName: <kafka-auth-Secret> authSecretNamespace: <namespace>","title":"Channel Example"},{"location":"eventing/samples/kafka/channel/#apache-kafka-channel-example","text":"You can install and configure the Apache Kafka CRD ( KafkaChannel ) as the default channel configuration in Knative Eventing.","title":"Apache Kafka Channel Example"},{"location":"eventing/samples/kafka/channel/#prerequisites","text":"Ensure that you meet the prerequisites listed in the Apache Kafka overview . A Kubernetes cluster with Knative Kafka Channel installed .","title":"Prerequisites"},{"location":"eventing/samples/kafka/channel/#creating-a-kafkachannel-channel-crd","text":"Create a new object by configuring the YAML file as follows: cat <<-EOF | kubectl apply -f - --- apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel metadata: name: my-kafka-channel spec: numPartitions: 3 replicationFactor: 1 EOF","title":"Creating a KafkaChannel channel CRD"},{"location":"eventing/samples/kafka/channel/#specifying-the-default-channel-configuration","text":"To configure the usage of the KafkaChannel CRD as the default channel configuration , edit the default-ch-webhook ConfigMap as follows: cat <<-EOF | kubectl apply -f - --- apiVersion: v1 kind: ConfigMap metadata: name: default-ch-webhook namespace: knative-eventing data: # Configuration for defaulting channels that do not specify CRD implementations. default-ch-config: | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 EOF","title":"Specifying the default channel configuration"},{"location":"eventing/samples/kafka/channel/#creating-an-apache-kafka-channel-using-the-default-channel-configuration","text":"Now that KafkaChannel is set as the default channel configuration, you can use the channels.messaging.knative.dev CRD to create a new Apache Kafka channel, using the generic Channel : cat <<-EOF | kubectl apply -f - --- apiVersion: messaging.knative.dev/v1 kind: Channel metadata: name: testchannel-one EOF Check Kafka for a testchannel topic. With Strimzi this can be done by using the command: kubectl -n kafka exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper localhost:2181 --list The result is: ... knative-messaging-kafka.default.testchannel-one ... The Apache Kafka topic that is created by the channel implementation is prefixed with knative-messaging-kafka . This indicates it is an Apache Kafka channel from Knative. It contains the name of the namespace, default in this example, followed by the actual name of the channel.","title":"Creating an Apache Kafka channel using the default channel configuration"},{"location":"eventing/samples/kafka/channel/#configuring-the-knative-broker-for-apache-kafka-channels","text":"To setup a broker that will use the new default Kafka channels, you must create a new default broker, using the command: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF This will give you two pods, such as: default-broker-filter-64658fc79f-nf596 1/1 Running 0 15m default-broker-ingress-ff79755b6-vj9jt 1/1 Running 0 15m Inside the Apache Kafka cluster you should see two new topics, such as: ... knative-messaging-kafka.default.default-kn2-ingress knative-messaging-kafka.default.default-kn2-trigger ...","title":"Configuring the Knative broker for Apache Kafka channels"},{"location":"eventing/samples/kafka/channel/#creating-a-service-and-trigger-to-use-the-apache-kafka-broker","text":"To use the Apache Kafka based broker, let's take a look at a simple demo. Use the ApiServerSource to publish events to the broker as well as the Trigger API, which then routes events to a Knative Service . Install ksvc , using the command: kubectl apply -f 000-ksvc.yaml Install a source that publishes to the default broker kubectl apply -f 020-k8s-events.yaml Create a trigger that routes the events to the ksvc : kubectl apply -f 030-trigger.yaml","title":"Creating a service and trigger to use the Apache Kafka broker"},{"location":"eventing/samples/kafka/channel/#verifying-your-apache-kafka-channel-and-broker","text":"Now that your Eventing cluster is configured for Apache Kafka, you can verify your configuration with the following options.","title":"Verifying your Apache Kafka channel and broker"},{"location":"eventing/samples/kafka/channel/#receive-events-via-knative","text":"Now you can see the events in the log of the ksvc using the command: kubectl logs --selector='serving.knative.dev/service=broker-kafka-display' -c user-container","title":"Receive events via Knative"},{"location":"eventing/samples/kafka/channel/#authentication-against-an-apache-kafka","text":"In production environments it is common that the Apache Kafka cluster is secured using TLS or SASL . This section shows how to confiugure the KafkaChannel to work against a protected Apache Kafka cluster, with the two supported TLS and SASL authentication methods.","title":"Authentication against an Apache Kafka"},{"location":"eventing/samples/kafka/channel/#tls-authentication","text":"To use TLS authentication you must create: A CA certificate A client certificate and key NOTE: Kafka channels require these files to be in .pem format. If your files are in a different format, you must convert them to .pem . Create the certificate files as secrets in your chosen namespace: $ kubectl create secret --namespace <namespace> generic <kafka-auth-secret> \\ --from-file=ca.crt=caroot.pem \\ --from-file=user.crt=certificate.pem \\ --from-file=user.key=key.pem ``` *NOTE:* It is important to use the same keys (`ca.crt`, `user.crt` and `user.key`). Reference your secret and the namespace of the secret in the `config-kafka` ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: config-kafka namespace: knative-eventing data: bootstrapServers: authSecretName: authSecretNamespace: ```","title":"TLS authentication"},{"location":"eventing/samples/kafka/channel/#sasl-authentication","text":"To use SASL authentication, you will need the following information: A username and password. The type of SASL mechanism you wish to use. For example; PLAIN , SCRAM-SHA-256 or SCRAM-SHA-512 . NOTE: It is recommended to also enable TLS. If you enable this, you will also need the ca.crt certificate as described in the previous section. Create the certificate files as secrets in your chosen namespace: $ kubectl create secret --namespace <namespace> generic <kafka-auth-secret> \\ --from-file=ca.crt=caroot.pem \\ --from-literal=password=\"SecretPassword\" \\ --from-literal=saslType=\"SCRAM-SHA-512\" \\ --from-literal=user=\"my-sasl-user\" NOTE: It is important to use the same keys; user , password and saslType .","title":"SASL authentication"},{"location":"eventing/samples/kafka/channel/#sasl-authentication-using-public-ca-certificates","text":"If you want to use SASL with public CA certificates, you must use the tls.enabled=true flag, rather than the ca.crt argument, when creating the secret. For example: $ kubectl create secret --namespace <namespace> generic <kafka-auth-secret> \\ --from-literal=tls.enabled=true \\ --from-literal=password=\"SecretPassword\" \\ --from-literal=saslType=\"SCRAM-SHA-512\" \\ --from-literal=user=\"my-sasl-user\" Reference your secret and the namespace of the secret in the config-kafka ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: config-kafka namespace: knative-eventing data: bootstrapServers: <bootstrap-servers> authSecretName: <kafka-auth-Secret> authSecretNamespace: <namespace>","title":"SASL authentication using public CA certificates"},{"location":"eventing/samples/kafka/source/","text":"Apache Kafka Source Example \u00b6 Tutorial on how to build and deploy a KafkaSource Eventing source using a Knative Serving Service . Prerequisites \u00b6 Ensure that you meet the prerequisites listed in the Apache Kafka overview . A Kubernetes cluster with Knative Kafka Source installed . Apache Kafka Topic (Optional) \u00b6 If using Strimzi, you can set a topic modifying source/kafka-topic.yaml with your desired: Topic Cluster Name Partitions Replicas apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : knative-demo-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 Deploy the KafkaTopic $ kubectl apply -f strimzi-topic.yaml kafkatopic.kafka.strimzi.io/knative-demo-topic created Ensure the KafkaTopic is running. $ kubectl -n kafka get kafkatopics.kafka.strimzi.io NAME AGE knative-demo-topic 16s Create the Event Display service \u00b6 Download a copy of the code: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/kafka/source Build the Event Display Service ( event-display.yaml ) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Deploy the Event Display Service $ kubectl apply --filename event-display.yaml ... service.serving.knative.dev/event-display created Ensure that the Service pod is running. The pod name will be prefixed with event-display . $ kubectl get pods NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2/2 Running 0 72s ... Apache Kafka Event Source \u00b6 Modify source/event-source.yaml accordingly with bootstrap servers, topics, etc...: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source. $ kubectl apply -f event-source.yaml ... kafkasource.sources.knative.dev/kafka-source created Check that the event source pod is running. The pod name will be prefixed with kafka-source . $ kubectl get pods NAME READY STATUS RESTARTS AGE kafka-source-xlnhq-5544766765-dnl5s 1/1 Running 0 40m Ensure the Apache Kafka Event Source started with the necessary configuration. $ kubectl logs --selector='knative-eventing-source-name=kafka-source' {\"level\":\"info\",\"ts\":\"2020-05-28T10:39:42.104Z\",\"caller\":\"adapter/adapter.go:81\",\"msg\":\"Starting with config: \",\"Topics\":\".\",\"ConsumerGroup\":\"...\",\"SinkURI\":\"...\",\"Name\":\".\",\"Namespace\":\".\"} Verify \u00b6 Produce a message ( {\"msg\": \"This is a test!\"} ) to the Apache Kafka topic, like shown below: kubectl -n kafka run kafka-producer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic If you don't see a command prompt, try pressing enter. >{\"msg\": \"This is a test!\"} Check that the Apache Kafka Event Source consumed the message and sent it to its sink properly. Since these logs are captured in debug level, edit the key level of config-logging configmap in knative-sources namespace to look like this: data: loglevel.controller: info loglevel.webhook: info zap-logger-config: | { \"level\": \"debug\", \"development\": false, \"outputPaths\": [\"stdout\"], \"errorOutputPaths\": [\"stderr\"], \"encoding\": \"json\", \"encoderConfig\": { \"timeKey\": \"ts\", \"levelKey\": \"level\", \"nameKey\": \"logger\", \"callerKey\": \"caller\", \"messageKey\": \"msg\", \"stacktraceKey\": \"stacktrace\", \"lineEnding\": \"\", \"levelEncoder\": \"\", \"timeEncoder\": \"iso8601\", \"durationEncoder\": \"\", \"callerEncoder\": \"\" } } Now manually delete the kafkasource deployment and allow the kafka-controller-manager deployment running in knative-sources namespace to redeploy it. Debug level logs should be visible now. $ kubectl logs --selector='knative-eventing-source-name=kafka-source' ... {\"level\":\"debug\",\"ts\":\"2020-05-28T10:40:29.400Z\",\"caller\":\"kafka/consumer_handler.go:77\",\"msg\":\"Message claimed\",\"topic\":\".\",\"value\":\".\"} {\"level\":\"debug\",\"ts\":\"2020-05-28T10:40:31.722Z\",\"caller\":\"kafka/consumer_handler.go:89\",\"msg\":\"Message marked\",\"topic\":\".\",\"value\":\".\"} Ensure the Event Display received the message sent to it by the Event Source. $ kubectl logs --selector='serving.knative.dev/service=event-display' -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic subject: partition:0#564 id: partition:0/offset:564 time: 2020-02-10T18:10:23.861866615Z datacontenttype: application/json Extensions, key: Data, { \"msg\": \"This is a test!\" } Teardown Steps \u00b6 Remove the Apache Kafka Event Source $ kubectl delete -f source/source.yaml kafkasource.sources.knative.dev \"kafka-source\" deleted Remove the Event Display $ kubectl delete -f source/event-display.yaml service.serving.knative.dev \"event-display\" deleted Remove the Apache Kafka Event Controller $ kubectl delete -f https://storage.googleapis.com/knative-releases/eventing-contrib/latest/kafka-source.yaml serviceaccount \"kafka-controller-manager\" deleted clusterrole.rbac.authorization.k8s.io \"eventing-sources-kafka-controller\" deleted clusterrolebinding.rbac.authorization.k8s.io \"eventing-sources-kafka-controller\" deleted customresourcedefinition.apiextensions.k8s.io \"kafkasources.sources.knative.dev\" deleted service \"kafka-controller\" deleted statefulset.apps \"kafka-controller-manager\" deleted (Optional) Remove the Apache Kafka Topic $ kubectl delete -f kafka-topic.yaml kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted (Optional) Specify the key deserializer \u00b6 When KafkaSource receives a message from Kafka, it dumps the key in the Event extension called Key and dumps Kafka message headers in the extensions starting with kafkaheader . You can specify the key deserializer among four types: string (default) for UTF-8 encoded strings int for 32-bit & 64-bit signed integers float for 32-bit & 64-bit floating points byte-array for a Base64 encoded byte array To specify it, add the label kafkasources.sources.knative.dev/key-type to the KafkaSource definition like: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source labels : kafkasources.sources.knative.dev/key-type : int spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Connecting to a TLS enabled Kafka broker \u00b6 The KafkaSource supports TLS and SASL authentication methods. For enabling TLS authentication, please have the below files CA Certificate Client Certificate and Key KafkaSource expects these files to be in pem format, if it is in other format like jks, please convert to pem. Create the certificate files as secrets in the namespace where KafkaSource is going to be set up $ kubectl create secret generic cacert --from-file=caroot.pem secret/cacert created $ kubectl create secret tls kafka-secret --cert=certificate.pem --key=key.pem secret/key created Apply the KafkaSource, change bootstrapServers and topics accordingly. ```yaml apiVersion: sources.knative.dev/v1beta1 kind: KafkaSource metadata: name: kafka-source-with-tls spec: net: tls: enable: true cert: secretKeyRef: key: tls.crt name: kafka-secret key: secretKeyRef: key: tls.key name: kafka-secret caCert: secretKeyRef: key: caroot.pem name: cacert consumerGroup: knative-group bootstrapServers: my-secure-kafka-bootstrap.kafka:443 topics: knative-demo-topic sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: event-display ```","title":"Source Example"},{"location":"eventing/samples/kafka/source/#apache-kafka-source-example","text":"Tutorial on how to build and deploy a KafkaSource Eventing source using a Knative Serving Service .","title":"Apache Kafka Source Example"},{"location":"eventing/samples/kafka/source/#prerequisites","text":"Ensure that you meet the prerequisites listed in the Apache Kafka overview . A Kubernetes cluster with Knative Kafka Source installed .","title":"Prerequisites"},{"location":"eventing/samples/kafka/source/#apache-kafka-topic-optional","text":"If using Strimzi, you can set a topic modifying source/kafka-topic.yaml with your desired: Topic Cluster Name Partitions Replicas apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : knative-demo-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 Deploy the KafkaTopic $ kubectl apply -f strimzi-topic.yaml kafkatopic.kafka.strimzi.io/knative-demo-topic created Ensure the KafkaTopic is running. $ kubectl -n kafka get kafkatopics.kafka.strimzi.io NAME AGE knative-demo-topic 16s","title":"Apache Kafka Topic (Optional)"},{"location":"eventing/samples/kafka/source/#create-the-event-display-service","text":"Download a copy of the code: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/kafka/source Build the Event Display Service ( event-display.yaml ) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Deploy the Event Display Service $ kubectl apply --filename event-display.yaml ... service.serving.knative.dev/event-display created Ensure that the Service pod is running. The pod name will be prefixed with event-display . $ kubectl get pods NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2/2 Running 0 72s ...","title":"Create the Event Display service"},{"location":"eventing/samples/kafka/source/#apache-kafka-event-source","text":"Modify source/event-source.yaml accordingly with bootstrap servers, topics, etc...: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source. $ kubectl apply -f event-source.yaml ... kafkasource.sources.knative.dev/kafka-source created Check that the event source pod is running. The pod name will be prefixed with kafka-source . $ kubectl get pods NAME READY STATUS RESTARTS AGE kafka-source-xlnhq-5544766765-dnl5s 1/1 Running 0 40m Ensure the Apache Kafka Event Source started with the necessary configuration. $ kubectl logs --selector='knative-eventing-source-name=kafka-source' {\"level\":\"info\",\"ts\":\"2020-05-28T10:39:42.104Z\",\"caller\":\"adapter/adapter.go:81\",\"msg\":\"Starting with config: \",\"Topics\":\".\",\"ConsumerGroup\":\"...\",\"SinkURI\":\"...\",\"Name\":\".\",\"Namespace\":\".\"}","title":"Apache Kafka Event Source"},{"location":"eventing/samples/kafka/source/#verify","text":"Produce a message ( {\"msg\": \"This is a test!\"} ) to the Apache Kafka topic, like shown below: kubectl -n kafka run kafka-producer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic If you don't see a command prompt, try pressing enter. >{\"msg\": \"This is a test!\"} Check that the Apache Kafka Event Source consumed the message and sent it to its sink properly. Since these logs are captured in debug level, edit the key level of config-logging configmap in knative-sources namespace to look like this: data: loglevel.controller: info loglevel.webhook: info zap-logger-config: | { \"level\": \"debug\", \"development\": false, \"outputPaths\": [\"stdout\"], \"errorOutputPaths\": [\"stderr\"], \"encoding\": \"json\", \"encoderConfig\": { \"timeKey\": \"ts\", \"levelKey\": \"level\", \"nameKey\": \"logger\", \"callerKey\": \"caller\", \"messageKey\": \"msg\", \"stacktraceKey\": \"stacktrace\", \"lineEnding\": \"\", \"levelEncoder\": \"\", \"timeEncoder\": \"iso8601\", \"durationEncoder\": \"\", \"callerEncoder\": \"\" } } Now manually delete the kafkasource deployment and allow the kafka-controller-manager deployment running in knative-sources namespace to redeploy it. Debug level logs should be visible now. $ kubectl logs --selector='knative-eventing-source-name=kafka-source' ... {\"level\":\"debug\",\"ts\":\"2020-05-28T10:40:29.400Z\",\"caller\":\"kafka/consumer_handler.go:77\",\"msg\":\"Message claimed\",\"topic\":\".\",\"value\":\".\"} {\"level\":\"debug\",\"ts\":\"2020-05-28T10:40:31.722Z\",\"caller\":\"kafka/consumer_handler.go:89\",\"msg\":\"Message marked\",\"topic\":\".\",\"value\":\".\"} Ensure the Event Display received the message sent to it by the Event Source. $ kubectl logs --selector='serving.knative.dev/service=event-display' -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic subject: partition:0#564 id: partition:0/offset:564 time: 2020-02-10T18:10:23.861866615Z datacontenttype: application/json Extensions, key: Data, { \"msg\": \"This is a test!\" }","title":"Verify"},{"location":"eventing/samples/kafka/source/#teardown-steps","text":"Remove the Apache Kafka Event Source $ kubectl delete -f source/source.yaml kafkasource.sources.knative.dev \"kafka-source\" deleted Remove the Event Display $ kubectl delete -f source/event-display.yaml service.serving.knative.dev \"event-display\" deleted Remove the Apache Kafka Event Controller $ kubectl delete -f https://storage.googleapis.com/knative-releases/eventing-contrib/latest/kafka-source.yaml serviceaccount \"kafka-controller-manager\" deleted clusterrole.rbac.authorization.k8s.io \"eventing-sources-kafka-controller\" deleted clusterrolebinding.rbac.authorization.k8s.io \"eventing-sources-kafka-controller\" deleted customresourcedefinition.apiextensions.k8s.io \"kafkasources.sources.knative.dev\" deleted service \"kafka-controller\" deleted statefulset.apps \"kafka-controller-manager\" deleted (Optional) Remove the Apache Kafka Topic $ kubectl delete -f kafka-topic.yaml kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted","title":"Teardown Steps"},{"location":"eventing/samples/kafka/source/#optional-specify-the-key-deserializer","text":"When KafkaSource receives a message from Kafka, it dumps the key in the Event extension called Key and dumps Kafka message headers in the extensions starting with kafkaheader . You can specify the key deserializer among four types: string (default) for UTF-8 encoded strings int for 32-bit & 64-bit signed integers float for 32-bit & 64-bit floating points byte-array for a Base64 encoded byte array To specify it, add the label kafkasources.sources.knative.dev/key-type to the KafkaSource definition like: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source labels : kafkasources.sources.knative.dev/key-type : int spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display","title":"(Optional) Specify the key deserializer"},{"location":"eventing/samples/kafka/source/#connecting-to-a-tls-enabled-kafka-broker","text":"The KafkaSource supports TLS and SASL authentication methods. For enabling TLS authentication, please have the below files CA Certificate Client Certificate and Key KafkaSource expects these files to be in pem format, if it is in other format like jks, please convert to pem. Create the certificate files as secrets in the namespace where KafkaSource is going to be set up $ kubectl create secret generic cacert --from-file=caroot.pem secret/cacert created $ kubectl create secret tls kafka-secret --cert=certificate.pem --key=key.pem secret/key created Apply the KafkaSource, change bootstrapServers and topics accordingly. ```yaml apiVersion: sources.knative.dev/v1beta1 kind: KafkaSource metadata: name: kafka-source-with-tls spec: net: tls: enable: true cert: secretKeyRef: key: tls.crt name: kafka-secret key: secretKeyRef: key: tls.key name: kafka-secret caCert: secretKeyRef: key: caroot.pem name: cacert consumerGroup: knative-group bootstrapServers: my-secure-kafka-bootstrap.kafka:443 topics: knative-demo-topic sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: event-display ```","title":"Connecting to a TLS enabled Kafka broker"},{"location":"eventing/samples/parallel/","text":"Parallel Example \u00b6 The following examples will help you understand how to use Parallel to describe various flows. Prerequisites \u00b6 All examples require: A Kubernetes cluster with Knative Eventing Knative Serving All examples are using the default channel template . Examples \u00b6 For each of these examples below, we'll use PingSource as the source of events. We also use simple functions to perform trivial filtering, transformation and routing of the incoming events. The examples are: Parallel with multiple branches and global reply Parallel with mutually exclusive cases","title":"Overview"},{"location":"eventing/samples/parallel/#parallel-example","text":"The following examples will help you understand how to use Parallel to describe various flows.","title":"Parallel Example"},{"location":"eventing/samples/parallel/#prerequisites","text":"All examples require: A Kubernetes cluster with Knative Eventing Knative Serving All examples are using the default channel template .","title":"Prerequisites"},{"location":"eventing/samples/parallel/#examples","text":"For each of these examples below, we'll use PingSource as the source of events. We also use simple functions to perform trivial filtering, transformation and routing of the incoming events. The examples are: Parallel with multiple branches and global reply Parallel with mutually exclusive cases","title":"Examples"},{"location":"eventing/samples/parallel/multiple-branches/","text":"Multiple Cases \u00b6 We are going to create a Parallel with two branches: the first branch accepts events with a time that is is even the second branch accepts events with a time that is is odd The events produced by each branch are then sent to the event-display service. Prerequisites \u00b6 Please refer to the sample overview for the prerequisites . Create the Knative Services \u00b6 Let's first create the filter and transformer services that we will use in our Parallel. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-filter spec : template : spec : containers : - image : villardl/filter-nodejs:0.1 env : - name : FILTER value : | Math.round(Date.parse(event.time) / 60000) % 2 === 0 --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-filter spec : template : spec : containers : - image : villardl/filter-nodejs:0.1 env : - name : FILTER value : | Math.round(Date.parse(event.time) / 60000) % 2 !== 0 --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs env : - name : TRANSFORMER value : | ({\"message\": \"we are even!\"}) --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"this is odd!\"}) . kubectl create -f ./filters.yaml -f ./transformers.yaml Create the Service displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources to be created. kubectl -n default create -f ./event-display.yaml Create the Parallel \u00b6 The parallel.yaml file contains the specifications for creating the Parallel. apiVersion : flows.knative.dev/v1 kind : Parallel metadata : name : odd-even-parallel spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel branches : - filter : ref : apiVersion : serving.knative.dev/v1 kind : Service name : even-filter subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : even-transformer - filter : ref : apiVersion : serving.knative.dev/v1 kind : Service name : odd-filter subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : odd-transformer reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display kubectl create -f ./parallel.yaml Create the PingSource targeting the Parallel \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Even or odd?\"} as the data payload every minute. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/1 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Even or odd?\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Parallel name : odd-even-parallel kubectl create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every minute, it might take some time for the events to show up in the logs. Let's look at the event-display log: kubectl logs -l serving.knative.dev/service = event-display --tail = 30 -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 015a4cf4-8a43-44a9-8702-3d4171d27ba5 time: 2020 -03-03T21:24:00.0007254Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; odd-even-parallel-kn-parallel-0-kn-channel.default.svc.cluster.local traceparent: 00 -41a139bf073f3cfcba7bb7ce7f1488fc-68a891ace985221a-00 Data, { \"message\" : \"we are even!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 52e6b097-f914-4b5a-8539-165650e85bcd time: 2020 -03-03T21:23:00.0004662Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; odd-even-parallel-kn-parallel-1-kn-channel.default.svc.cluster.local traceparent: 00 -58d371410d7daf2033be226860b4ee5d-05d686ee90c3226f-00 Data, { \"message\" : \"this is odd!\" }","title":"Multiple Cases"},{"location":"eventing/samples/parallel/multiple-branches/#multiple-cases","text":"We are going to create a Parallel with two branches: the first branch accepts events with a time that is is even the second branch accepts events with a time that is is odd The events produced by each branch are then sent to the event-display service.","title":"Multiple Cases"},{"location":"eventing/samples/parallel/multiple-branches/#prerequisites","text":"Please refer to the sample overview for the prerequisites .","title":"Prerequisites"},{"location":"eventing/samples/parallel/multiple-branches/#create-the-knative-services","text":"Let's first create the filter and transformer services that we will use in our Parallel. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-filter spec : template : spec : containers : - image : villardl/filter-nodejs:0.1 env : - name : FILTER value : | Math.round(Date.parse(event.time) / 60000) % 2 === 0 --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-filter spec : template : spec : containers : - image : villardl/filter-nodejs:0.1 env : - name : FILTER value : | Math.round(Date.parse(event.time) / 60000) % 2 !== 0 --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs env : - name : TRANSFORMER value : | ({\"message\": \"we are even!\"}) --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"this is odd!\"}) . kubectl create -f ./filters.yaml -f ./transformers.yaml","title":"Create the Knative Services"},{"location":"eventing/samples/parallel/multiple-branches/#create-the-service-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources to be created. kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Sequence"},{"location":"eventing/samples/parallel/multiple-branches/#create-the-parallel","text":"The parallel.yaml file contains the specifications for creating the Parallel. apiVersion : flows.knative.dev/v1 kind : Parallel metadata : name : odd-even-parallel spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel branches : - filter : ref : apiVersion : serving.knative.dev/v1 kind : Service name : even-filter subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : even-transformer - filter : ref : apiVersion : serving.knative.dev/v1 kind : Service name : odd-filter subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : odd-transformer reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display kubectl create -f ./parallel.yaml","title":"Create the Parallel"},{"location":"eventing/samples/parallel/multiple-branches/#create-the-pingsource-targeting-the-parallel","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Even or odd?\"} as the data payload every minute. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/1 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Even or odd?\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Parallel name : odd-even-parallel kubectl create -f ./ping-source.yaml","title":"Create the PingSource targeting the Parallel"},{"location":"eventing/samples/parallel/multiple-branches/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every minute, it might take some time for the events to show up in the logs. Let's look at the event-display log: kubectl logs -l serving.knative.dev/service = event-display --tail = 30 -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 015a4cf4-8a43-44a9-8702-3d4171d27ba5 time: 2020 -03-03T21:24:00.0007254Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; odd-even-parallel-kn-parallel-0-kn-channel.default.svc.cluster.local traceparent: 00 -41a139bf073f3cfcba7bb7ce7f1488fc-68a891ace985221a-00 Data, { \"message\" : \"we are even!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 52e6b097-f914-4b5a-8539-165650e85bcd time: 2020 -03-03T21:23:00.0004662Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; odd-even-parallel-kn-parallel-1-kn-channel.default.svc.cluster.local traceparent: 00 -58d371410d7daf2033be226860b4ee5d-05d686ee90c3226f-00 Data, { \"message\" : \"this is odd!\" }","title":"Inspecting the results"},{"location":"eventing/samples/parallel/mutual-exclusivity/","text":"Mutual Exclusive Cases \u00b6 In this example, we are going to see how we can create a Parallel with mutually exclusive branches. This example is the same as the multiple branches example except that we are now going to rely on the Knative switch function to provide a soft mutual exclusivity guarantee. NOTE: this example must be deployed in the default namespace. Prerequisites \u00b6 Please refer to the sample overview for the prerequisites . Create the Knative Services \u00b6 Let's first create the switcher and transformer services that we will use in our Parallel. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : me-even-odd-switcher spec : template : spec : containers : - image : villardl/switcher-nodejs:0.1 env : - name : EXPRESSION value : Math.round(Date.parse(event.time) / 60000) % 2 - name : CASES value : '[0, 1]' --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"we are even!\"}) --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"this is odd!\"}) . kubectl create -f ./switcher.yaml -f ./transformers.yaml Create the Service displaying the events created by Parallel \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display kubectl -n default create -f ./event-display.yaml Create the Parallel object \u00b6 The parallel.yaml file contains the specifications for creating the Parallel object. apiVersion : flows.knative.dev/v1 kind : Parallel metadata : name : me-odd-even-parallel spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel branches : - filter : uri : \"http://me-even-odd-switcher.default.svc.cluster.local/0\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-even-transformer - filter : uri : \"http://me-even-odd-switcher.default.svc.cluster.local/1\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-odd-transformer reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-event-display kubectl create -f ./parallel.yaml Create the PingSource targeting the Parallel object \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Even or odd?\"} as the data payload every minute. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : me-ping-source spec : schedule : \"*/1 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Even or odd?\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Parallel name : me-odd-even-parallel kubectl create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the me-event-display pods. Note that since we set the PingSource to emit every minute, it might take some time for the events to show up in the logs. Let's look at the me-event-display log: kubectl logs -l serving.knative.dev/service = me-event-display --tail = 50 -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/me-ping-source id: 0b0db15c-9b36-4388-8eaa-8c23a9dc2707 time: 2020 -03-03T21:30:00.0007292Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: me-odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; me-odd-even-parallel-kn-parallel-0-kn-channel.default.svc.cluster.local traceparent: 00 -e8b17109cd21d4fa66a86633b5a614c7-ba96d220fe13211c-00 Data, { \"message\" : \"we are even!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/me-ping-source id: 321170d1-dea7-4b18-9290-28adb1de089b time: 2020 -03-03T21:31:00.0007847Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: me-odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; me-odd-even-parallel-kn-parallel-1-kn-channel.default.svc.cluster.local traceparent: 00 -78d8b044d23c85789f0a13fd3679ac24-1d69ddde56d620c7-00 Data, { \"message\" : \"this is odd!\" }","title":"Mutual Exclusivity"},{"location":"eventing/samples/parallel/mutual-exclusivity/#mutual-exclusive-cases","text":"In this example, we are going to see how we can create a Parallel with mutually exclusive branches. This example is the same as the multiple branches example except that we are now going to rely on the Knative switch function to provide a soft mutual exclusivity guarantee. NOTE: this example must be deployed in the default namespace.","title":"Mutual Exclusive Cases"},{"location":"eventing/samples/parallel/mutual-exclusivity/#prerequisites","text":"Please refer to the sample overview for the prerequisites .","title":"Prerequisites"},{"location":"eventing/samples/parallel/mutual-exclusivity/#create-the-knative-services","text":"Let's first create the switcher and transformer services that we will use in our Parallel. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : me-even-odd-switcher spec : template : spec : containers : - image : villardl/switcher-nodejs:0.1 env : - name : EXPRESSION value : Math.round(Date.parse(event.time) / 60000) % 2 - name : CASES value : '[0, 1]' --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"we are even!\"}) --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"this is odd!\"}) . kubectl create -f ./switcher.yaml -f ./transformers.yaml","title":"Create the Knative Services"},{"location":"eventing/samples/parallel/mutual-exclusivity/#create-the-service-displaying-the-events-created-by-parallel","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Parallel"},{"location":"eventing/samples/parallel/mutual-exclusivity/#create-the-parallel-object","text":"The parallel.yaml file contains the specifications for creating the Parallel object. apiVersion : flows.knative.dev/v1 kind : Parallel metadata : name : me-odd-even-parallel spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel branches : - filter : uri : \"http://me-even-odd-switcher.default.svc.cluster.local/0\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-even-transformer - filter : uri : \"http://me-even-odd-switcher.default.svc.cluster.local/1\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-odd-transformer reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-event-display kubectl create -f ./parallel.yaml","title":"Create the Parallel object"},{"location":"eventing/samples/parallel/mutual-exclusivity/#create-the-pingsource-targeting-the-parallel-object","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Even or odd?\"} as the data payload every minute. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : me-ping-source spec : schedule : \"*/1 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Even or odd?\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Parallel name : me-odd-even-parallel kubectl create -f ./ping-source.yaml","title":"Create the PingSource targeting the Parallel object"},{"location":"eventing/samples/parallel/mutual-exclusivity/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the me-event-display pods. Note that since we set the PingSource to emit every minute, it might take some time for the events to show up in the logs. Let's look at the me-event-display log: kubectl logs -l serving.knative.dev/service = me-event-display --tail = 50 -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/me-ping-source id: 0b0db15c-9b36-4388-8eaa-8c23a9dc2707 time: 2020 -03-03T21:30:00.0007292Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: me-odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; me-odd-even-parallel-kn-parallel-0-kn-channel.default.svc.cluster.local traceparent: 00 -e8b17109cd21d4fa66a86633b5a614c7-ba96d220fe13211c-00 Data, { \"message\" : \"we are even!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/me-ping-source id: 321170d1-dea7-4b18-9290-28adb1de089b time: 2020 -03-03T21:31:00.0007847Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: me-odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; me-odd-even-parallel-kn-parallel-1-kn-channel.default.svc.cluster.local traceparent: 00 -78d8b044d23c85789f0a13fd3679ac24-1d69ddde56d620c7-00 Data, { \"message\" : \"this is odd!\" }","title":"Inspecting the results"},{"location":"eventing/sink/","text":"Sink \u00b6 A sink is an Addressable resource that acts as a link between the Eventing mesh and an entity or system. We can connect any source to a sink, such as PingSource and KafkaSink objects: apiVersion : sources.knative.dev/v1beta1 kind : PingSource metadata : name : test-ping-source spec : schedule : \"*/1 * * * *\" jsonData : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : my-kafka-sink We can connect a Trigger object to a sink, so that we can filter events, before sending them to a sink: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : my-kafka-sink Setting up a Custom Resource to be used as a sink \u00b6 To allow a Custom Resource to act as a sink for events, there are two things needed: 1. Make the resource Addressable \u00b6 To make a Custom Resource Addressable, it needs to contain a status.address.url . More information about Addressable resources . 2. Create an addressable-resolver ClusterRole \u00b6 An addressable-resolver ClusterRole is needed in order to obtain the necessary RBAC rules for the sink to receive events. For example, we can create a kafkasinks-addressable-resolver ClusterRole to allow get , list , and watch access to kafkasinks and kafkasinks/status ```yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kafkasinks-addressable-resolver labels: kafka.eventing.knative.dev/release: devel duck.knative.dev/addressable: \"true\" Do not use this role directly. These rules will be added to the \"addressable-resolver\" role. \u00b6 rules: - apiGroups: - eventing.knative.dev resources: - kafkasinks - kafkasinks/status verbs: - get - list - watch ``` Knative Sinks \u00b6 Name Maintainer Description KafkaSink Knative Send events to a Kafka topic RedisSink Knative Send events to a Redis Stream","title":"Overview"},{"location":"eventing/sink/#sink","text":"A sink is an Addressable resource that acts as a link between the Eventing mesh and an entity or system. We can connect any source to a sink, such as PingSource and KafkaSink objects: apiVersion : sources.knative.dev/v1beta1 kind : PingSource metadata : name : test-ping-source spec : schedule : \"*/1 * * * *\" jsonData : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : my-kafka-sink We can connect a Trigger object to a sink, so that we can filter events, before sending them to a sink: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : my-kafka-sink","title":"Sink"},{"location":"eventing/sink/#setting-up-a-custom-resource-to-be-used-as-a-sink","text":"To allow a Custom Resource to act as a sink for events, there are two things needed:","title":"Setting up a Custom Resource to be used as a sink"},{"location":"eventing/sink/#1-make-the-resource-addressable","text":"To make a Custom Resource Addressable, it needs to contain a status.address.url . More information about Addressable resources .","title":"1. Make the resource Addressable"},{"location":"eventing/sink/#2-create-an-addressable-resolver-clusterrole","text":"An addressable-resolver ClusterRole is needed in order to obtain the necessary RBAC rules for the sink to receive events. For example, we can create a kafkasinks-addressable-resolver ClusterRole to allow get , list , and watch access to kafkasinks and kafkasinks/status ```yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: kafkasinks-addressable-resolver labels: kafka.eventing.knative.dev/release: devel duck.knative.dev/addressable: \"true\"","title":"2. Create an addressable-resolver ClusterRole"},{"location":"eventing/sink/#do-not-use-this-role-directly-these-rules-will-be-added-to-the-addressable-resolver-role","text":"rules: - apiGroups: - eventing.knative.dev resources: - kafkasinks - kafkasinks/status verbs: - get - list - watch ```","title":"Do not use this role directly. These rules will be added to the \"addressable-resolver\" role."},{"location":"eventing/sink/#knative-sinks","text":"Name Maintainer Description KafkaSink Knative Send events to a Kafka topic RedisSink Knative Send events to a Redis Stream","title":"Knative Sinks"},{"location":"eventing/sink/kafka-sink/","text":"Apache Kafka Sink \u00b6 This page shows how to install and configure Apache Kafka Sink. Prerequisites \u00b6 Installing Eventing using YAML files . Installation \u00b6 Install the Kafka controller: kubectl apply --filename https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Sink data plane: kubectl apply --filename https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Verify that kafka-controller and kafka-sink-receiver are running: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-sink-receiver 1 /1 1 1 5s Kafka Sink \u00b6 A KafkaSink object looks like this: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 Security \u00b6 Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the KafkaSink spec, we can reference a Secret : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 auth : secret : ref : name : my_secret The Secret my_secret must exist in the same namespace of the KafkaSink , in this case: default . Note: Certificates and keys must be in PEM format . Authentication using SASL \u00b6 Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice. Authentication using SASL without encryption \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Authentication using SASL and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Encryption using SSL without client authentication \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true Authentication and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> NOTE: ca.crt can be omitted to fallback to use system's root CA set. Kafka Producer configurations \u00b6 A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. Knative exposes all available Kafka Producer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-sink-data-plane config map in the knative-eventing namespace. Documentation for the settings available in this config map is available on the Apache Kafka website , in particular, Producer configurations . Enable debug logging for data plane components \u00b6 To enable debug logging for data plane components change the logging level to DEBUG in the kafka-config-logging config map. Apply the following kafka-config-logging config map: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-sink-receiver : kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver Additional information \u00b6 To report bugs or add feature requests, open an issue in the eventing-kafka-broker repository .","title":"Apache Kafka Sink"},{"location":"eventing/sink/kafka-sink/#apache-kafka-sink","text":"This page shows how to install and configure Apache Kafka Sink.","title":"Apache Kafka Sink"},{"location":"eventing/sink/kafka-sink/#prerequisites","text":"Installing Eventing using YAML files .","title":"Prerequisites"},{"location":"eventing/sink/kafka-sink/#installation","text":"Install the Kafka controller: kubectl apply --filename https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Sink data plane: kubectl apply --filename https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Verify that kafka-controller and kafka-sink-receiver are running: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-sink-receiver 1 /1 1 1 5s","title":"Installation"},{"location":"eventing/sink/kafka-sink/#kafka-sink","text":"A KafkaSink object looks like this: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092","title":"Kafka Sink"},{"location":"eventing/sink/kafka-sink/#security","text":"Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the KafkaSink spec, we can reference a Secret : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 auth : secret : ref : name : my_secret The Secret my_secret must exist in the same namespace of the KafkaSink , in this case: default . Note: Certificates and keys must be in PEM format .","title":"Security"},{"location":"eventing/sink/kafka-sink/#authentication-using-sasl","text":"Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice.","title":"Authentication using SASL"},{"location":"eventing/sink/kafka-sink/#authentication-using-sasl-without-encryption","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL without encryption"},{"location":"eventing/sink/kafka-sink/#authentication-using-sasl-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL and encryption using SSL"},{"location":"eventing/sink/kafka-sink/#encryption-using-ssl-without-client-authentication","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true","title":"Encryption using SSL without client authentication"},{"location":"eventing/sink/kafka-sink/#authentication-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> NOTE: ca.crt can be omitted to fallback to use system's root CA set.","title":"Authentication and encryption using SSL"},{"location":"eventing/sink/kafka-sink/#kafka-producer-configurations","text":"A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. Knative exposes all available Kafka Producer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-sink-data-plane config map in the knative-eventing namespace. Documentation for the settings available in this config map is available on the Apache Kafka website , in particular, Producer configurations .","title":"Kafka Producer configurations"},{"location":"eventing/sink/kafka-sink/#enable-debug-logging-for-data-plane-components","text":"To enable debug logging for data plane components change the logging level to DEBUG in the kafka-config-logging config map. Apply the following kafka-config-logging config map: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-sink-receiver : kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver","title":"Enable debug logging for data plane components"},{"location":"eventing/sink/kafka-sink/#additional-information","text":"To report bugs or add feature requests, open an issue in the eventing-kafka-broker repository .","title":"Additional information"},{"location":"eventing/sources/","text":"Event sources \u00b6 An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink . A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source. Event sources are created by instantiating a CR from a Source object. The Source object defines the arguments and parameters needed to instantiate a CR. All Sources are part of the sources category. kubectl You can list existing event sources on your cluster by entering the command: kubectl get sources kn You can list existing event sources on your cluster by entering the kn command: kn source list Knative Sources \u00b6 Name API Version Maintainer Description APIServerSource v1 Knative Brings Kubernetes API server events into Knative. The APIServerSource fires a new event each time a Kubernetes resource is created, updated or deleted. AWS SQS v1alpha1 Knative Brings AWS Simple Queue Service messages into Knative. The AwsSqsSource fires a new event each time an event is published on an AWS SQS topic . Apache Camel v1alpha1 Knative Enables use of Apache Camel components for pushing events into Knative. A CamelSource is an event source that can represent any existing Apache Camel component , that provides a consumer side, and enables publishing events to an addressable endpoint. Each Camel endpoint has the form of a URI where the scheme is the ID of the component to use. CamelSource requires Camel-K to be installed into the current namespace. See the CamelSource example. Apache CouchDB v1alpha1 Knative Brings Apache CouchDB messages into Knative. Apache Kafka v1beta1 Knative Brings Apache Kafka messages into Knative. The KafkaSource reads events from an Apache Kafka Cluster, and passes these events to a sink so that they can be consumed. See the Kafka Source example for more details. Container Source v1 Knative The ContainerSource will instantiate container image(s) that can generate events until the ContainerSource is deleted. This may be used, for example, to poll an FTP server for new files or generate events at a set time interval. Given a spec.template with at least a container image specified, ContainerSource will keep a Pod running with the specified image(s). K_SINK (destination address) and KE_CE_OVERRIDES (JSON CloudEvents attributes) environment variables are injected into the running image(s). It is used by multiple other Sources as underlying infrastructure. Refer to the Container Source example for more details. GitHub v1alpha1 Knative Registers for events of the specified types on the specified GitHub organization or repository, and brings those events into Knative. The GitHubSource fires a new event for selected GitHub event types . See the GitHub Source example for more details. GitLab v1alpha1 Knative Registers for events of the specified types on the specified GitLab repository, and brings those events into Knative. The GitLabSource creates a webhooks for specified event types , listens for incoming events, and passes them to a consumer. See the GitLab Source example for more details. Heartbeats N/A Knative Uses an in-memory timer to produce events at the specified interval. PingSource v1beta2 Knative Produces events with a fixed payload on a specified Cron schedule. See the Ping Source example for more details. RabbitMQ Active development None Brings RabbitMQ messages into Knative. SinkBinding v1 Knative The SinkBinding can be used to author new event sources using any of the familiar compute abstractions that Kubernetes makes available (e.g. Deployment, Job, DaemonSet, StatefulSet), or Knative abstractions (e.g. Service, Configuration). SinkBinding provides a framework for injecting K_SINK (destination address) and K_CE_OVERRIDES (JSON cloudevents attributes) environment variables into any Kubernetes resource which has a spec.template that looks like a Pod (aka PodSpecable). See the SinkBinding example for more details. WebSocket N/A Knative Opens a WebSocket to the specified source and packages each received message as a Knative event. Third-Party Sources \u00b6 Name API Version Maintainer Description Auto Container Source Proof of Concept None AutoContainerSource is a controller that allows the Source CRDs without needing a controller. It notices CRDs with a specific label and starts controlling resources of that type. It utilizes Container Source as underlying infrastructure. Amazon CloudWatch Supported TriggerMesh Collects metrics from Amazon CloudWatch . Amazon CloudWatch Logs Supported TriggerMesh Subscribes to log events from an Amazon CloudWatch Logs stream. Amazon CodeCommit Supported TriggerMesh Registers for events emitted by an Amazon CodeCommit source code repository. Amazon Cognito Identity Supported TriggerMesh Registers for events from Amazon Cognito identity pools. Amazon Cognito User Supported TriggerMesh Registers for events from Amazon Cognito user pools. Amazon DynamoDB Supported TriggerMesh Reads records from an Amazon DynamoDB stream. Amazon Kinesis Supported TriggerMesh Reads records from an Amazon Kinesis stream. Amazon SNS Supported TriggerMesh Subscribes to messages from an Amazon SNS topic. Amazon SQS Supported TriggerMesh Consumes messages from an Amazon SQS queue. BitBucket Proof of Concept None Registers for events of the specified types on the specified BitBucket organization/repository. Brings those events into Knative. CloudAuditLogsSource v1 Google Registers for events of the specified types on the specified Google Cloud Audit Logs . Brings those events into Knative. Refer to the CloudAuditLogsSource example for more details. CloudPubSubSource v1 Google Brings Cloud Pub/Sub messages into Knative. The CloudPubSubSource fires a new event each time a message is published on a Google Cloud Platform PubSub topic . See the CloudPubSubSource example for more details. CloudSchedulerSource v1 Google Create, update, and delete Google Cloud Scheduler Jobs. When those jobs are triggered, receive the event inside Knative. See the CloudSchedulerSource example for further details. CloudStorageSource v1 Google Registers for events of the specified types on the specified Google Cloud Storage bucket and optional object prefix. Brings those events into Knative. See the CloudStorageSource example. DockerHubSource v1alpha1 None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. FTP / SFTP Proof of concept None Watches for files being uploaded into a FTP/SFTP and generates events for those. GitHub Issue Comments Proof of Concept None Polls a specific GitHub issue for new comments. Heartbeat Proof of Concept None Uses an in-memory timer to produce events as the specified interval. Uses AutoContainerSource for underlying infrastructure. Konnek Active Development None Retrieves events from cloud platforms (like AWS and GCP) and transforms them into CloudEvents for consumption in Knative. K8s Proof of Concept None Brings Kubernetes cluster events into Knative. Uses AutoContainerSource for underlying infrastructure. RedisSource v1alpha1 None Brings Redis Stream into Knative. Slack v1alpha1 TriggerMesh Subscribes to events from Slack. VMware Active Development None Brings vSphere events into Knative. Zendesk v1alpha1 TriggerMesh Subscribes to events from Zendesk. Additional resources \u00b6 For information about creating your own Source type, see the tutorial on writing a Source with a Receive Adapter . If your code needs to send events as part of its business logic and doesn't fit the model of a Source, consider feeding events directly to a Broker . For more information about using kn Source related commands, see the kn source reference documentation .","title":"Overview"},{"location":"eventing/sources/#event-sources","text":"An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink . A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source. Event sources are created by instantiating a CR from a Source object. The Source object defines the arguments and parameters needed to instantiate a CR. All Sources are part of the sources category. kubectl You can list existing event sources on your cluster by entering the command: kubectl get sources kn You can list existing event sources on your cluster by entering the kn command: kn source list","title":"Event sources"},{"location":"eventing/sources/#knative-sources","text":"Name API Version Maintainer Description APIServerSource v1 Knative Brings Kubernetes API server events into Knative. The APIServerSource fires a new event each time a Kubernetes resource is created, updated or deleted. AWS SQS v1alpha1 Knative Brings AWS Simple Queue Service messages into Knative. The AwsSqsSource fires a new event each time an event is published on an AWS SQS topic . Apache Camel v1alpha1 Knative Enables use of Apache Camel components for pushing events into Knative. A CamelSource is an event source that can represent any existing Apache Camel component , that provides a consumer side, and enables publishing events to an addressable endpoint. Each Camel endpoint has the form of a URI where the scheme is the ID of the component to use. CamelSource requires Camel-K to be installed into the current namespace. See the CamelSource example. Apache CouchDB v1alpha1 Knative Brings Apache CouchDB messages into Knative. Apache Kafka v1beta1 Knative Brings Apache Kafka messages into Knative. The KafkaSource reads events from an Apache Kafka Cluster, and passes these events to a sink so that they can be consumed. See the Kafka Source example for more details. Container Source v1 Knative The ContainerSource will instantiate container image(s) that can generate events until the ContainerSource is deleted. This may be used, for example, to poll an FTP server for new files or generate events at a set time interval. Given a spec.template with at least a container image specified, ContainerSource will keep a Pod running with the specified image(s). K_SINK (destination address) and KE_CE_OVERRIDES (JSON CloudEvents attributes) environment variables are injected into the running image(s). It is used by multiple other Sources as underlying infrastructure. Refer to the Container Source example for more details. GitHub v1alpha1 Knative Registers for events of the specified types on the specified GitHub organization or repository, and brings those events into Knative. The GitHubSource fires a new event for selected GitHub event types . See the GitHub Source example for more details. GitLab v1alpha1 Knative Registers for events of the specified types on the specified GitLab repository, and brings those events into Knative. The GitLabSource creates a webhooks for specified event types , listens for incoming events, and passes them to a consumer. See the GitLab Source example for more details. Heartbeats N/A Knative Uses an in-memory timer to produce events at the specified interval. PingSource v1beta2 Knative Produces events with a fixed payload on a specified Cron schedule. See the Ping Source example for more details. RabbitMQ Active development None Brings RabbitMQ messages into Knative. SinkBinding v1 Knative The SinkBinding can be used to author new event sources using any of the familiar compute abstractions that Kubernetes makes available (e.g. Deployment, Job, DaemonSet, StatefulSet), or Knative abstractions (e.g. Service, Configuration). SinkBinding provides a framework for injecting K_SINK (destination address) and K_CE_OVERRIDES (JSON cloudevents attributes) environment variables into any Kubernetes resource which has a spec.template that looks like a Pod (aka PodSpecable). See the SinkBinding example for more details. WebSocket N/A Knative Opens a WebSocket to the specified source and packages each received message as a Knative event.","title":"Knative Sources"},{"location":"eventing/sources/#third-party-sources","text":"Name API Version Maintainer Description Auto Container Source Proof of Concept None AutoContainerSource is a controller that allows the Source CRDs without needing a controller. It notices CRDs with a specific label and starts controlling resources of that type. It utilizes Container Source as underlying infrastructure. Amazon CloudWatch Supported TriggerMesh Collects metrics from Amazon CloudWatch . Amazon CloudWatch Logs Supported TriggerMesh Subscribes to log events from an Amazon CloudWatch Logs stream. Amazon CodeCommit Supported TriggerMesh Registers for events emitted by an Amazon CodeCommit source code repository. Amazon Cognito Identity Supported TriggerMesh Registers for events from Amazon Cognito identity pools. Amazon Cognito User Supported TriggerMesh Registers for events from Amazon Cognito user pools. Amazon DynamoDB Supported TriggerMesh Reads records from an Amazon DynamoDB stream. Amazon Kinesis Supported TriggerMesh Reads records from an Amazon Kinesis stream. Amazon SNS Supported TriggerMesh Subscribes to messages from an Amazon SNS topic. Amazon SQS Supported TriggerMesh Consumes messages from an Amazon SQS queue. BitBucket Proof of Concept None Registers for events of the specified types on the specified BitBucket organization/repository. Brings those events into Knative. CloudAuditLogsSource v1 Google Registers for events of the specified types on the specified Google Cloud Audit Logs . Brings those events into Knative. Refer to the CloudAuditLogsSource example for more details. CloudPubSubSource v1 Google Brings Cloud Pub/Sub messages into Knative. The CloudPubSubSource fires a new event each time a message is published on a Google Cloud Platform PubSub topic . See the CloudPubSubSource example for more details. CloudSchedulerSource v1 Google Create, update, and delete Google Cloud Scheduler Jobs. When those jobs are triggered, receive the event inside Knative. See the CloudSchedulerSource example for further details. CloudStorageSource v1 Google Registers for events of the specified types on the specified Google Cloud Storage bucket and optional object prefix. Brings those events into Knative. See the CloudStorageSource example. DockerHubSource v1alpha1 None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. FTP / SFTP Proof of concept None Watches for files being uploaded into a FTP/SFTP and generates events for those. GitHub Issue Comments Proof of Concept None Polls a specific GitHub issue for new comments. Heartbeat Proof of Concept None Uses an in-memory timer to produce events as the specified interval. Uses AutoContainerSource for underlying infrastructure. Konnek Active Development None Retrieves events from cloud platforms (like AWS and GCP) and transforms them into CloudEvents for consumption in Knative. K8s Proof of Concept None Brings Kubernetes cluster events into Knative. Uses AutoContainerSource for underlying infrastructure. RedisSource v1alpha1 None Brings Redis Stream into Knative. Slack v1alpha1 TriggerMesh Subscribes to events from Slack. VMware Active Development None Brings vSphere events into Knative. Zendesk v1alpha1 TriggerMesh Subscribes to events from Zendesk.","title":"Third-Party Sources"},{"location":"eventing/sources/#additional-resources","text":"For information about creating your own Source type, see the tutorial on writing a Source with a Receive Adapter . If your code needs to send events as part of its business logic and doesn't fit the model of a Source, consider feeding events directly to a Broker . For more information about using kn Source related commands, see the kn source reference documentation .","title":"Additional resources"},{"location":"eventing/sources/containersource/","text":"ContainerSource \u00b6 ContainerSource will start a container image which will generate events under certain situations and send messages to a sink URI. It also can be an easy way to support your own event sources in Knative. This guide shows how to configure ContainerSource as an event source for functions and summarizes guidelines for creating your own event source as a ContainerSource. Prerequisites \u00b6 Install ko Set KO_DOCKER_REPO (e.g. gcr.io/[gcloud-project] or docker.io/<username> ) Authenticated with your KO_DOCKER_REPO Install docker Installation \u00b6 The ContainerSource source type is enabled by default when you install Knative Eventing. Example \u00b6 This example shows how the heartbeats container sends events to the Event Display Service. Preparing the heartbeats image \u00b6 Knative event-sources has a sample of heartbeats event source. You could clone the source code by git clone -b \"v0.23.0\" https://github.com/knative/eventing.git And then build a heartbeats image and publish to your image repo with ko publish ko://knative.dev/eventing/cmd/heartbeats Creating a namespace \u00b6 Create a new namespace called containersource-example by entering the following command: kubectl create namespace containersource-example Creating the Event Display Service \u00b6 In order to verify ContainerSource is working, we will create a Event Display Service that dumps incoming messages to its log. kubectl -n containersource-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: event-display spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: event-display spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Creating the ContainerSource using the heartbeats image \u00b6 In order to run the heartbeats container as an event source, you have to create a concrete ContainerSource with specific arguments and environment settings. Be sure to replace heartbeats_image_uri with a valid uri for your heartbeats image you published in the previous step. Note that arguments and environment variables are set and will be passed to the container. kubectl -n containersource-example apply -f - << EOF apiVersion: sources.knative.dev/v1 kind: ContainerSource metadata: name: test-heartbeats spec: template: spec: containers: # This corresponds to a heartbeats image uri you build and publish in the previous step # e.g. gcr.io/[gcloud-project]/knative.dev/eventing/cmd/heartbeats - image: <heartbeats_image_uri> name: heartbeats args: - --period=1 env: - name: POD_NAME value: \"mypod\" - name: POD_NAMESPACE value: \"event-test\" sink: ref: apiVersion: v1 kind: Service name: event-display EOF Verify \u00b6 View the logs for the event-display event consumer by entering the following command: kubectl -n containersource-example logs -l app = event-display --tail = 200 This returns the Attributes and Data of the events that the ContainerSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" } Cleanup \u00b6 Delete the containersource-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace containersource-example Reference Documentation \u00b6 See the ContainerSource specification . Contact \u00b6 For any inquiries about this source, please reach out on to the Knative users group .","title":"ContainerSource"},{"location":"eventing/sources/containersource/#containersource","text":"ContainerSource will start a container image which will generate events under certain situations and send messages to a sink URI. It also can be an easy way to support your own event sources in Knative. This guide shows how to configure ContainerSource as an event source for functions and summarizes guidelines for creating your own event source as a ContainerSource.","title":"ContainerSource"},{"location":"eventing/sources/containersource/#prerequisites","text":"Install ko Set KO_DOCKER_REPO (e.g. gcr.io/[gcloud-project] or docker.io/<username> ) Authenticated with your KO_DOCKER_REPO Install docker","title":"Prerequisites"},{"location":"eventing/sources/containersource/#installation","text":"The ContainerSource source type is enabled by default when you install Knative Eventing.","title":"Installation"},{"location":"eventing/sources/containersource/#example","text":"This example shows how the heartbeats container sends events to the Event Display Service.","title":"Example"},{"location":"eventing/sources/containersource/#preparing-the-heartbeats-image","text":"Knative event-sources has a sample of heartbeats event source. You could clone the source code by git clone -b \"v0.23.0\" https://github.com/knative/eventing.git And then build a heartbeats image and publish to your image repo with ko publish ko://knative.dev/eventing/cmd/heartbeats","title":"Preparing the heartbeats image"},{"location":"eventing/sources/containersource/#creating-a-namespace","text":"Create a new namespace called containersource-example by entering the following command: kubectl create namespace containersource-example","title":"Creating a namespace"},{"location":"eventing/sources/containersource/#creating-the-event-display-service","text":"In order to verify ContainerSource is working, we will create a Event Display Service that dumps incoming messages to its log. kubectl -n containersource-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: event-display spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: event-display spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF","title":"Creating the Event Display Service"},{"location":"eventing/sources/containersource/#creating-the-containersource-using-the-heartbeats-image","text":"In order to run the heartbeats container as an event source, you have to create a concrete ContainerSource with specific arguments and environment settings. Be sure to replace heartbeats_image_uri with a valid uri for your heartbeats image you published in the previous step. Note that arguments and environment variables are set and will be passed to the container. kubectl -n containersource-example apply -f - << EOF apiVersion: sources.knative.dev/v1 kind: ContainerSource metadata: name: test-heartbeats spec: template: spec: containers: # This corresponds to a heartbeats image uri you build and publish in the previous step # e.g. gcr.io/[gcloud-project]/knative.dev/eventing/cmd/heartbeats - image: <heartbeats_image_uri> name: heartbeats args: - --period=1 env: - name: POD_NAME value: \"mypod\" - name: POD_NAMESPACE value: \"event-test\" sink: ref: apiVersion: v1 kind: Service name: event-display EOF","title":"Creating the ContainerSource using the heartbeats image"},{"location":"eventing/sources/containersource/#verify","text":"View the logs for the event-display event consumer by entering the following command: kubectl -n containersource-example logs -l app = event-display --tail = 200 This returns the Attributes and Data of the events that the ContainerSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" }","title":"Verify"},{"location":"eventing/sources/containersource/#cleanup","text":"Delete the containersource-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace containersource-example","title":"Cleanup"},{"location":"eventing/sources/containersource/#reference-documentation","text":"See the ContainerSource specification .","title":"Reference Documentation"},{"location":"eventing/sources/containersource/#contact","text":"For any inquiries about this source, please reach out on to the Knative users group .","title":"Contact"},{"location":"eventing/sources/sinkbinding/","text":"Sink binding \u00b6 The SinkBinding custom object supports decoupling event production from delivery addressing. You can use sink binding to connect Kubernetes resources that embed a PodSpec and want to produce events, such as an event source, to an addressable Kubernetes object that can receive events, also known as an event sink . Sink binding can be used to create new event sources using any of the familiar compute objects that Kubernetes makes available. For example, Deployment , Job , DaemonSet , or StatefulSet objects, or Knative abstractions, such as Service or Configuration objects, can be used. Sink binding injects environment variables into the PodTemplateSpec of the event sink, so that the application code does not need to interact directly with the Kubernetes API to locate the event destination. Sink binding operates in one of two modes; Inclusion or Exclusion . You can set the mode by modifying the SINK_BINDING_SELECTION_MODE of the eventing-webhook deployment accordingly. The mode determines the default scope of the webhook. By default, the webhook is set to exclusion mode, which means that any namespace that does not have the label bindings.knative.dev/exclude: true will be subject to mutation evalutation. If SINK_BINDING_SELECTION_MODE is set to inclusion , only the resources in a namespace labelled with bindings.knative.dev/include: true will be considered. In inclusion mode, any SinkBinding resource created will automatically label the subject namespace with bindings.knative.dev/include: true for inclusion in the potential environment variable inclusions. Getting started \u00b6 The following procedures show how you can create a sink binding and connect it to a service and event source in your cluster. Creating a namespace \u00b6 Create a namespace called sinkbinding-example : kubectl create namespace sinkbinding-example Creating a Knative service \u00b6 Create a Knative service if you do not have an existing event sink that you want to connect to the sink binding. Prerequisites \u00b6 You must have Knative Serving installed on your cluster. Optional: If you want to use kn commands with sink binding, you must install the kn CLI. Procedure \u00b6 Create a Knative service: kn kn service create hello --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --env RESPONSE = \"Hello Serverless!\" yaml Copy the sample YAML into a service.yaml file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Apply the file: kubectl apply --filename service.yaml Creating a cron job \u00b6 Create a cron job if you do not have an existing event source that you want to connect to the sink binding. Create a CronJob object: Copy the sample YAML into a cronjob.yaml file: apiVersion : batch/v1beta1 kind : CronJob metadata : name : heartbeat-cron spec : # Run every minute schedule : \"*/1 * * * *\" jobTemplate : metadata : labels : app : heartbeat-cron spec : template : spec : restartPolicy : Never containers : - name : single-heartbeat image : gcr.io/knative-releases/knative.dev/eventing/cmd/heartbeats args : - --period=1 env : - name : ONE_SHOT value : \"true\" - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace Apply the file: kubectl apply --filename heartbeats-source.yaml Cloning a sample heartbeat cron job \u00b6 Knative event-contrib contains a sample heartbeats event source. Prerequisites \u00b6 Ensure that ko publish is set up correctly: KO_DOCKER_REPO must be set. For example, gcr.io/[gcloud-project] or docker.io/<username> . You must have authenticated with your KO_DOCKER_REPO . Procedure \u00b6 Clone the event-contib repository: $ git clone -b \"v0.23.0\" https://github.com/knative/eventing.git Build a heartbeats image, and publish the image to your image repository: $ ko publish knative.dev/eventing/cmd/heartbeats Creating a SinkBinding object \u00b6 Create a SinkBinding object that directs events from your cron job to the event sink. Prerequisites \u00b6 You must have Knative Eventing installed on your cluster. Optional: If you want to use kn commands with sink binding, you must install the kn CLI. Procedure \u00b6 Create a sink binding: kn kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" yaml Copy the sample YAML into a cronjob.yaml file: apiVersion : sources.knative.dev/v1alpha1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : app : heartbeat-cron sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Apply the file: kubectl apply --filename heartbeats-source.yaml Verification steps \u00b6 Verify that a message was sent to the Knative eventing system by looking at the event-display service logs: kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m Observe the lines showing the request headers and body of the event message, sent by the heartbeats source to the display function: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687 time: 2020 -02-19T13:47:10.41428688Z datacontenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\" : 1 , \"label\" : \"\" } Cleanup \u00b6 Delete the sinkbinding-example namespace and all of its resources from your cluster: kubectl delete namespace sinkbinding-example","title":"Sink binding"},{"location":"eventing/sources/sinkbinding/#sink-binding","text":"The SinkBinding custom object supports decoupling event production from delivery addressing. You can use sink binding to connect Kubernetes resources that embed a PodSpec and want to produce events, such as an event source, to an addressable Kubernetes object that can receive events, also known as an event sink . Sink binding can be used to create new event sources using any of the familiar compute objects that Kubernetes makes available. For example, Deployment , Job , DaemonSet , or StatefulSet objects, or Knative abstractions, such as Service or Configuration objects, can be used. Sink binding injects environment variables into the PodTemplateSpec of the event sink, so that the application code does not need to interact directly with the Kubernetes API to locate the event destination. Sink binding operates in one of two modes; Inclusion or Exclusion . You can set the mode by modifying the SINK_BINDING_SELECTION_MODE of the eventing-webhook deployment accordingly. The mode determines the default scope of the webhook. By default, the webhook is set to exclusion mode, which means that any namespace that does not have the label bindings.knative.dev/exclude: true will be subject to mutation evalutation. If SINK_BINDING_SELECTION_MODE is set to inclusion , only the resources in a namespace labelled with bindings.knative.dev/include: true will be considered. In inclusion mode, any SinkBinding resource created will automatically label the subject namespace with bindings.knative.dev/include: true for inclusion in the potential environment variable inclusions.","title":"Sink binding"},{"location":"eventing/sources/sinkbinding/#getting-started","text":"The following procedures show how you can create a sink binding and connect it to a service and event source in your cluster.","title":"Getting started"},{"location":"eventing/sources/sinkbinding/#creating-a-namespace","text":"Create a namespace called sinkbinding-example : kubectl create namespace sinkbinding-example","title":"Creating a namespace"},{"location":"eventing/sources/sinkbinding/#creating-a-knative-service","text":"Create a Knative service if you do not have an existing event sink that you want to connect to the sink binding.","title":"Creating a Knative service"},{"location":"eventing/sources/sinkbinding/#prerequisites","text":"You must have Knative Serving installed on your cluster. Optional: If you want to use kn commands with sink binding, you must install the kn CLI.","title":"Prerequisites"},{"location":"eventing/sources/sinkbinding/#procedure","text":"Create a Knative service: kn kn service create hello --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --env RESPONSE = \"Hello Serverless!\" yaml Copy the sample YAML into a service.yaml file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Apply the file: kubectl apply --filename service.yaml","title":"Procedure"},{"location":"eventing/sources/sinkbinding/#creating-a-cron-job","text":"Create a cron job if you do not have an existing event source that you want to connect to the sink binding. Create a CronJob object: Copy the sample YAML into a cronjob.yaml file: apiVersion : batch/v1beta1 kind : CronJob metadata : name : heartbeat-cron spec : # Run every minute schedule : \"*/1 * * * *\" jobTemplate : metadata : labels : app : heartbeat-cron spec : template : spec : restartPolicy : Never containers : - name : single-heartbeat image : gcr.io/knative-releases/knative.dev/eventing/cmd/heartbeats args : - --period=1 env : - name : ONE_SHOT value : \"true\" - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace Apply the file: kubectl apply --filename heartbeats-source.yaml","title":"Creating a cron job"},{"location":"eventing/sources/sinkbinding/#cloning-a-sample-heartbeat-cron-job","text":"Knative event-contrib contains a sample heartbeats event source.","title":"Cloning a sample heartbeat cron job"},{"location":"eventing/sources/sinkbinding/#prerequisites_1","text":"Ensure that ko publish is set up correctly: KO_DOCKER_REPO must be set. For example, gcr.io/[gcloud-project] or docker.io/<username> . You must have authenticated with your KO_DOCKER_REPO .","title":"Prerequisites"},{"location":"eventing/sources/sinkbinding/#procedure_1","text":"Clone the event-contib repository: $ git clone -b \"v0.23.0\" https://github.com/knative/eventing.git Build a heartbeats image, and publish the image to your image repository: $ ko publish knative.dev/eventing/cmd/heartbeats","title":"Procedure"},{"location":"eventing/sources/sinkbinding/#creating-a-sinkbinding-object","text":"Create a SinkBinding object that directs events from your cron job to the event sink.","title":"Creating a SinkBinding object"},{"location":"eventing/sources/sinkbinding/#prerequisites_2","text":"You must have Knative Eventing installed on your cluster. Optional: If you want to use kn commands with sink binding, you must install the kn CLI.","title":"Prerequisites"},{"location":"eventing/sources/sinkbinding/#procedure_2","text":"Create a sink binding: kn kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" yaml Copy the sample YAML into a cronjob.yaml file: apiVersion : sources.knative.dev/v1alpha1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : app : heartbeat-cron sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Apply the file: kubectl apply --filename heartbeats-source.yaml","title":"Procedure"},{"location":"eventing/sources/sinkbinding/#verification-steps","text":"Verify that a message was sent to the Knative eventing system by looking at the event-display service logs: kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m Observe the lines showing the request headers and body of the event message, sent by the heartbeats source to the display function: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687 time: 2020 -02-19T13:47:10.41428688Z datacontenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\" : 1 , \"label\" : \"\" }","title":"Verification steps"},{"location":"eventing/sources/sinkbinding/#cleanup","text":"Delete the sinkbinding-example namespace and all of its resources from your cluster: kubectl delete namespace sinkbinding-example","title":"Cleanup"},{"location":"eventing/sources/apache-camel-source/","text":"Apache Camel source \u00b6 These samples show how to configure Camel Sources. These event sources are highly dynamic and allow you to generate events from a variety of systems (cloud platforms, social networks, datastores, message brokers, legacy systems, etc.), leveraging all the 300+ components provided by Apache Camel . All Camel Sources use Apache Camel K as the runtime engine. Prerequisites \u00b6 Install Knative Serving and Eventing . Install the Apache Camel K Operator in any namespace where you want to run Camel sources. The preferred version that is compatible with Camel sources is Camel K v1.0.0-M4 . Installation instructions are provided in the Apache Camel K Manual . Documentation includes specific instructions for common Kubernetes environments, including development clusters. Install the Camel Source from the camel.yaml in the Knative Eventing Camel release page : kubectl apply --filename camel.yaml Create Test Resources \u00b6 All the CamelSource examples use some test resources for the purpose of displaying the generated events. The following resources need to be created: a simple Knative event display service that prints incoming events to its log an in-memory channel named camel-test that buffers events created by the event source a subscription to direct events from the test channel to the event display service Deploy the display_resources.yaml : kubectl apply --filename display_resources.yaml Run a Timer CamelSource \u00b6 The samples directory contains some sample sources that can be used to generate events. The simplest example of CamelSource , that does not require additional configuration, is the timer source. The timer source periodically generates \"Hello world!\" events and forwards them to the provided destination. If you want, you can customize the source behavior using options available in the Apache Camel documentation for the timer component . All Camel components are documented in the Apache Camel Website . Install the timer CamelSource from source: kubectl apply -f source_timer.yaml Verify that the published events were sent into the Knative eventing system by looking at what is downstream of the CamelSource . kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container If you have deployed the timer source, you should see new log lines appearing every 3 seconds. Run a MQTT CamelSource \u00b6 One of the 300+ Camel components that you can leverage is Camel-Paho , based on the Eclipse Paho open source project. A source based on Paho (like the provided MQTT CamelSource ) allows to bridge any MQTT broker to a Knative resource, automatically converting IoT messages to Cloudevents. To use the MQTT source, you need a MQTT broker running and reachable from your cluster. For example, it's possible to run a Mosquito MQTT Broker for testing purposes. First, edit the MQTT CamelSource and put the correct address of the MQTT broker in the brokerUrl field. You also need to provide the name of the topic that you want to subscribe to: just change paho:mytopic to match the topic that you want to use. You can also scale this source out, in order to obtain more throughput, by changing the value of the replicas field. By default it creates 2 replicas for demonstration purposes. To reduce noise in the event display, you can remove all previously created CamelSources from the namespace: kubectl delete camelsource --all Install the mqtt CamelSource : kubectl apply -f source_mqtt.yaml You can now send MQTT messages to your broker using your favourite client (you can even use Camel K for sending test events). Each message you send to the MQTT broker will be printed by the event display as a Cloudevent. You can verify that your messages reach the event display by checking its logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container Run a Telegram CamelSource \u00b6 Another useful component available with Camel is the Telegram component. It can be used to forward messages of a Telegram chat into Knative channels as events. Before using the provided Telegram CamelSource example, you need to follow the instructions on the Telegram website for creating a Telegram Bot . The quickest way to create a bot is to contact the Bot Father , another Telegram Bot, using your preferred Telegram client (mobile or web). After you create the bot, you will receive an authorization token that is needed for the source to work. First, edit the telegram CamelSource and put the authorization token, replacing the <put-your-token-here> placeholder. To reduce noise in the event display, you can remove all previously created CamelSources from the namespace: kubectl delete camelsource --all Install the telegram CamelSource : kubectl apply -f source_telegram.yaml Now, you can contact your bot with any Telegram client. Each message you send to the bot will be printed by the event display as a Cloudevent. You can verify that your messages reach the event display by checking its logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container Run an HTTP Poller CamelSource \u00b6 CamelSources are not limited to using a single Camel component. For example, you can combine the Camel Timer component with the Camel HTTP component to periodically fetch an external API, transform the result into a Cloudevent and forward it to a given destination. The example will retrieve a static JSON file from a remote URL, but you can edit the HTTP poller CamelSource to add your own API. If you have previously deployed other CamelSources, to reduce noise in the event display, you can remove them all from the namespace: kubectl delete camelsource --all Install the HTTP poller CamelSource : kubectl apply -f source_http_poller.yaml The event display will show some JSON data periodically pulled from the external REST API. To check the logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container","title":"Camel source"},{"location":"eventing/sources/apache-camel-source/#apache-camel-source","text":"These samples show how to configure Camel Sources. These event sources are highly dynamic and allow you to generate events from a variety of systems (cloud platforms, social networks, datastores, message brokers, legacy systems, etc.), leveraging all the 300+ components provided by Apache Camel . All Camel Sources use Apache Camel K as the runtime engine.","title":"Apache Camel source"},{"location":"eventing/sources/apache-camel-source/#prerequisites","text":"Install Knative Serving and Eventing . Install the Apache Camel K Operator in any namespace where you want to run Camel sources. The preferred version that is compatible with Camel sources is Camel K v1.0.0-M4 . Installation instructions are provided in the Apache Camel K Manual . Documentation includes specific instructions for common Kubernetes environments, including development clusters. Install the Camel Source from the camel.yaml in the Knative Eventing Camel release page : kubectl apply --filename camel.yaml","title":"Prerequisites"},{"location":"eventing/sources/apache-camel-source/#create-test-resources","text":"All the CamelSource examples use some test resources for the purpose of displaying the generated events. The following resources need to be created: a simple Knative event display service that prints incoming events to its log an in-memory channel named camel-test that buffers events created by the event source a subscription to direct events from the test channel to the event display service Deploy the display_resources.yaml : kubectl apply --filename display_resources.yaml","title":"Create Test Resources"},{"location":"eventing/sources/apache-camel-source/#run-a-timer-camelsource","text":"The samples directory contains some sample sources that can be used to generate events. The simplest example of CamelSource , that does not require additional configuration, is the timer source. The timer source periodically generates \"Hello world!\" events and forwards them to the provided destination. If you want, you can customize the source behavior using options available in the Apache Camel documentation for the timer component . All Camel components are documented in the Apache Camel Website . Install the timer CamelSource from source: kubectl apply -f source_timer.yaml Verify that the published events were sent into the Knative eventing system by looking at what is downstream of the CamelSource . kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container If you have deployed the timer source, you should see new log lines appearing every 3 seconds.","title":"Run a Timer CamelSource"},{"location":"eventing/sources/apache-camel-source/#run-a-mqtt-camelsource","text":"One of the 300+ Camel components that you can leverage is Camel-Paho , based on the Eclipse Paho open source project. A source based on Paho (like the provided MQTT CamelSource ) allows to bridge any MQTT broker to a Knative resource, automatically converting IoT messages to Cloudevents. To use the MQTT source, you need a MQTT broker running and reachable from your cluster. For example, it's possible to run a Mosquito MQTT Broker for testing purposes. First, edit the MQTT CamelSource and put the correct address of the MQTT broker in the brokerUrl field. You also need to provide the name of the topic that you want to subscribe to: just change paho:mytopic to match the topic that you want to use. You can also scale this source out, in order to obtain more throughput, by changing the value of the replicas field. By default it creates 2 replicas for demonstration purposes. To reduce noise in the event display, you can remove all previously created CamelSources from the namespace: kubectl delete camelsource --all Install the mqtt CamelSource : kubectl apply -f source_mqtt.yaml You can now send MQTT messages to your broker using your favourite client (you can even use Camel K for sending test events). Each message you send to the MQTT broker will be printed by the event display as a Cloudevent. You can verify that your messages reach the event display by checking its logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container","title":"Run a MQTT CamelSource"},{"location":"eventing/sources/apache-camel-source/#run-a-telegram-camelsource","text":"Another useful component available with Camel is the Telegram component. It can be used to forward messages of a Telegram chat into Knative channels as events. Before using the provided Telegram CamelSource example, you need to follow the instructions on the Telegram website for creating a Telegram Bot . The quickest way to create a bot is to contact the Bot Father , another Telegram Bot, using your preferred Telegram client (mobile or web). After you create the bot, you will receive an authorization token that is needed for the source to work. First, edit the telegram CamelSource and put the authorization token, replacing the <put-your-token-here> placeholder. To reduce noise in the event display, you can remove all previously created CamelSources from the namespace: kubectl delete camelsource --all Install the telegram CamelSource : kubectl apply -f source_telegram.yaml Now, you can contact your bot with any Telegram client. Each message you send to the bot will be printed by the event display as a Cloudevent. You can verify that your messages reach the event display by checking its logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container","title":"Run a Telegram CamelSource"},{"location":"eventing/sources/apache-camel-source/#run-an-http-poller-camelsource","text":"CamelSources are not limited to using a single Camel component. For example, you can combine the Camel Timer component with the Camel HTTP component to periodically fetch an external API, transform the result into a Cloudevent and forward it to a given destination. The example will retrieve a static JSON file from a remote URL, but you can edit the HTTP poller CamelSource to add your own API. If you have previously deployed other CamelSources, to reduce noise in the event display, you can remove them all from the namespace: kubectl delete camelsource --all Install the HTTP poller CamelSource : kubectl apply -f source_http_poller.yaml The event display will show some JSON data periodically pulled from the external REST API. To check the logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container","title":"Run an HTTP Poller CamelSource"},{"location":"eventing/sources/apiserversource/","text":"API server source \u00b6 The API server source is a Knative Eventing Kubernetes custom resource that listens for Kubernetes events and forwards received events to a sink. The API server source is part of the core Knative Eventing component, and is provided by default when Knative Eventing is installed. Multiple instances of an APIServerSource object can be created by users.","title":"Overview"},{"location":"eventing/sources/apiserversource/#api-server-source","text":"The API server source is a Knative Eventing Kubernetes custom resource that listens for Kubernetes events and forwards received events to a sink. The API server source is part of the core Knative Eventing component, and is provided by default when Knative Eventing is installed. Multiple instances of an APIServerSource object can be created by users.","title":"API server source"},{"location":"eventing/sources/apiserversource/getting-started/","text":"Getting started \u00b6 Prerequisites \u00b6 Before you can create an API server source, you must install Knative Eventing and the kubectl CLI tool. Create an API server source \u00b6 Optional: Create a namespace for the API server source instance: kubectl create namespace <namespace> where; - <namespace> is the name of the namespace that you want to create. Creating a namespace for your API server source and related components allows you to view changes and events for this workflow more easily, since these are isolated from the many other components that may exist in your default namespace. It also makes removing the source easier, since you can simply delete the namespace to remove all of the resources. Create a service account: kubectl create -f - <<EOF apiVersion : v1 kind : ServiceAccount metadata : name : <service-account> namespace : <namespace> EOF where; - <service-account> is the name of the service account that you want to create. - <namespace> is the namespace that you created in step 1 above. Create a cluster role: kubectl create -f - <<EOF apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : <cluster-role> rules : - apiGroups : - \"\" resources : - events verbs : - get - list - watch EOF where; - <cluster-role> is the name of the cluster role that you want to create. Create a cluster role binding: kubectl create -f - <<EOF apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : <cluster-role-binding> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : <cluster-role> subjects : - kind : ServiceAccount name : <service-account> namespace : <namespace> EOF where; - <cluster-role-binding> is the name of the cluster role binding that you want to create. - <cluster-role> is the name of the cluster role that you created in step 3 above. - <service-account> is the name of the service account that you created in step 2 above. - <namespace> is the name of the namespace that you created in step 1 above. Create an ApiServerSource object: YAML kubectl create -f - <<EOF apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : serviceAccountName : <service-account> mode : Resource resources : - apiVersion : v1 kind : Event sink : ref : apiVersion : v1 kind : Service name : <sink> EOF where; - <apiserversource> is the name of the source that you want to create. - <namespace> is the name of the namespace that you created in step 1 above. - <service-account> is the name of the service account that you created in step 2 above. - <sink> is the name of the Knative service that you want to use as a sink. A service is used here as an example, however you can use any supported PodSpecable object by updating the kind from Service to another object type. kn kn source apiserver create <apiserversource> \\ --namespace <namespace> \\ --mode \"Resource\" \\ --resource \"Event:v1\" \\ --service-account <service-account> \\ --sink <sink> where; - <apiserversource> is the name of the source that you want to create. - <namespace> is the name of the namespace that you created in step 1 above. - <service-account> is the name of the service account that you created in step 2 above. - <sink> is the name of the PodSpecable object that you want to use as a sink. Create events by launching a test pod in your namespace: kubectl run busybox --image = busybox --namespace = <namespace> --restart = Never -- ls where; - <namespace> is the name of the namespace that you created in step 1 above. Delete the test pod: kubectl --namespace = <namespace> delete pod busybox where; - <namespace> is the name of the namespace that you created in step 1 above. View the logs to verify that Kubernetes events were sent to the Knative Eventing system: kubectl logs --namespace = <namespace> -l app = <sink> --tail = 100 where; - <namespace> is the name of the namespace that you created in step 1 above. - <sink> is the name of the PodSpecable object that you used as a sink in step 5 above. Example log output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020 -07-28T19:14:54.719501054Z datacontenttype: application/json Extensions, kind: Event name: busybox.1626008649e617e3 namespace: apiserversource-example Data, { \"apiVersion\" : \"v1\" , \"count\" : 1 , \"eventTime\" : null, \"firstTimestamp\" : \"2020-07-28T19:14:54Z\" , \"involvedObject\" : { \"apiVersion\" : \"v1\" , \"fieldPath\" : \"spec.containers{busybox}\" , \"kind\" : \"Pod\" , \"name\" : \"busybox\" , \"namespace\" : \"apiserversource-example\" , \"resourceVersion\" : \"28987493\" , \"uid\" : \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" } , \"kind\" : \"Event\" , \"lastTimestamp\" : \"2020-07-28T19:14:54Z\" , \"message\" : \"Started container\" , \"metadata\" : { \"creationTimestamp\" : \"2020-07-28T19:14:54Z\" , \"name\" : \"busybox.1626008649e617e3\" , \"namespace\" : \"default\" , \"resourceVersion\" : \"506088\" , \"selfLink\" : \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\" , \"uid\" : \"2005af47-737b-11e9-a6c5-42010a8a00ed\" } , \"reason\" : \"Started\" , \"reportingComponent\" : \"\" , \"reportingInstance\" : \"\" , \"source\" : { \"component\" : \"kubelet\" , \"host\" : \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" } , \"type\" : \"Normal\" } Delete the API server source \u00b6 Deleting the namespace removes the API server source and all of the related resources that were created in this namespace: kubectl delete namespace <namespace> where; - <namespace> is the name of the namespace that you created in step 1 above.","title":"Getting started"},{"location":"eventing/sources/apiserversource/getting-started/#getting-started","text":"","title":"Getting started"},{"location":"eventing/sources/apiserversource/getting-started/#prerequisites","text":"Before you can create an API server source, you must install Knative Eventing and the kubectl CLI tool.","title":"Prerequisites"},{"location":"eventing/sources/apiserversource/getting-started/#create-an-api-server-source","text":"Optional: Create a namespace for the API server source instance: kubectl create namespace <namespace> where; - <namespace> is the name of the namespace that you want to create. Creating a namespace for your API server source and related components allows you to view changes and events for this workflow more easily, since these are isolated from the many other components that may exist in your default namespace. It also makes removing the source easier, since you can simply delete the namespace to remove all of the resources. Create a service account: kubectl create -f - <<EOF apiVersion : v1 kind : ServiceAccount metadata : name : <service-account> namespace : <namespace> EOF where; - <service-account> is the name of the service account that you want to create. - <namespace> is the namespace that you created in step 1 above. Create a cluster role: kubectl create -f - <<EOF apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : <cluster-role> rules : - apiGroups : - \"\" resources : - events verbs : - get - list - watch EOF where; - <cluster-role> is the name of the cluster role that you want to create. Create a cluster role binding: kubectl create -f - <<EOF apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : <cluster-role-binding> roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : <cluster-role> subjects : - kind : ServiceAccount name : <service-account> namespace : <namespace> EOF where; - <cluster-role-binding> is the name of the cluster role binding that you want to create. - <cluster-role> is the name of the cluster role that you created in step 3 above. - <service-account> is the name of the service account that you created in step 2 above. - <namespace> is the name of the namespace that you created in step 1 above. Create an ApiServerSource object: YAML kubectl create -f - <<EOF apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : serviceAccountName : <service-account> mode : Resource resources : - apiVersion : v1 kind : Event sink : ref : apiVersion : v1 kind : Service name : <sink> EOF where; - <apiserversource> is the name of the source that you want to create. - <namespace> is the name of the namespace that you created in step 1 above. - <service-account> is the name of the service account that you created in step 2 above. - <sink> is the name of the Knative service that you want to use as a sink. A service is used here as an example, however you can use any supported PodSpecable object by updating the kind from Service to another object type. kn kn source apiserver create <apiserversource> \\ --namespace <namespace> \\ --mode \"Resource\" \\ --resource \"Event:v1\" \\ --service-account <service-account> \\ --sink <sink> where; - <apiserversource> is the name of the source that you want to create. - <namespace> is the name of the namespace that you created in step 1 above. - <service-account> is the name of the service account that you created in step 2 above. - <sink> is the name of the PodSpecable object that you want to use as a sink. Create events by launching a test pod in your namespace: kubectl run busybox --image = busybox --namespace = <namespace> --restart = Never -- ls where; - <namespace> is the name of the namespace that you created in step 1 above. Delete the test pod: kubectl --namespace = <namespace> delete pod busybox where; - <namespace> is the name of the namespace that you created in step 1 above. View the logs to verify that Kubernetes events were sent to the Knative Eventing system: kubectl logs --namespace = <namespace> -l app = <sink> --tail = 100 where; - <namespace> is the name of the namespace that you created in step 1 above. - <sink> is the name of the PodSpecable object that you used as a sink in step 5 above. Example log output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020 -07-28T19:14:54.719501054Z datacontenttype: application/json Extensions, kind: Event name: busybox.1626008649e617e3 namespace: apiserversource-example Data, { \"apiVersion\" : \"v1\" , \"count\" : 1 , \"eventTime\" : null, \"firstTimestamp\" : \"2020-07-28T19:14:54Z\" , \"involvedObject\" : { \"apiVersion\" : \"v1\" , \"fieldPath\" : \"spec.containers{busybox}\" , \"kind\" : \"Pod\" , \"name\" : \"busybox\" , \"namespace\" : \"apiserversource-example\" , \"resourceVersion\" : \"28987493\" , \"uid\" : \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" } , \"kind\" : \"Event\" , \"lastTimestamp\" : \"2020-07-28T19:14:54Z\" , \"message\" : \"Started container\" , \"metadata\" : { \"creationTimestamp\" : \"2020-07-28T19:14:54Z\" , \"name\" : \"busybox.1626008649e617e3\" , \"namespace\" : \"default\" , \"resourceVersion\" : \"506088\" , \"selfLink\" : \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\" , \"uid\" : \"2005af47-737b-11e9-a6c5-42010a8a00ed\" } , \"reason\" : \"Started\" , \"reportingComponent\" : \"\" , \"reportingInstance\" : \"\" , \"source\" : { \"component\" : \"kubelet\" , \"host\" : \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" } , \"type\" : \"Normal\" }","title":"Create an API server source"},{"location":"eventing/sources/apiserversource/getting-started/#delete-the-api-server-source","text":"Deleting the namespace removes the API server source and all of the related resources that were created in this namespace: kubectl delete namespace <namespace> where; - <namespace> is the name of the namespace that you created in step 1 above.","title":"Delete the API server source"},{"location":"eventing/sources/creating-event-sources/","text":"Creating an event source \u00b6 You can create your own event source for use with Knative Eventing components by using the following methods: Build an event source in Javascript, and implement it using a ContainerSource or SinkBinding. By creating your own event source controller, receiver adapter, and custom resource definition (CRD).","title":"Overview"},{"location":"eventing/sources/creating-event-sources/#creating-an-event-source","text":"You can create your own event source for use with Knative Eventing components by using the following methods: Build an event source in Javascript, and implement it using a ContainerSource or SinkBinding. By creating your own event source controller, receiver adapter, and custom resource definition (CRD).","title":"Creating an event source"},{"location":"eventing/sources/creating-event-sources/writing-event-source/","text":"Creating an event source by using the sample event source \u00b6 This guide explains how to create your own event source for Knative Eventing by using a sample repository , and explains the key concepts behind each required component. Documentation for the default Knative event sources can be used as an additional reference. After completing the provided tutorial, you will have created a basic event source controller and a receive adapter. Events can be viewed by using the event_display Knative service. Prerequisites \u00b6 You are familiar with Kubernetes and Go development. You have installed Git. You have installed Go. Clone the sample source . Optional: Install the ko CLI tool. Install the kubectl CLI tool. Set up minikube . Steps \u00b6 Separation of Concerns API Definition Controller Reconciler Receive Adapter Example YAML Moving the event source to the knative-sandbox organization","title":"Overview"},{"location":"eventing/sources/creating-event-sources/writing-event-source/#creating-an-event-source-by-using-the-sample-event-source","text":"This guide explains how to create your own event source for Knative Eventing by using a sample repository , and explains the key concepts behind each required component. Documentation for the default Knative event sources can be used as an additional reference. After completing the provided tutorial, you will have created a basic event source controller and a receive adapter. Events can be viewed by using the event_display Knative service.","title":"Creating an event source by using the sample event source"},{"location":"eventing/sources/creating-event-sources/writing-event-source/#prerequisites","text":"You are familiar with Kubernetes and Go development. You have installed Git. You have installed Go. Clone the sample source . Optional: Install the ko CLI tool. Install the kubectl CLI tool. Set up minikube .","title":"Prerequisites"},{"location":"eventing/sources/creating-event-sources/writing-event-source/#steps","text":"Separation of Concerns API Definition Controller Reconciler Receive Adapter Example YAML Moving the event source to the knative-sandbox organization","title":"Steps"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/","text":"Topics \u00b6 What are the personas and critical paths? Contributor: implement a new source with minimal k8s overhead (don't have to learn controller/k8s internals) Operator: easily install Sources and verify that they are \"safe\" Developer: easily discover what Sources they can pull from on this cluster Developer: easily configure a Source based on existing knowledge of other Sources. Separation of concerns \u00b6 Contributor: \u00b6 Receive Adapter (RA) - process that receives incoming events. Implement CloudEvent binding interfaces, cloudevent's go sdk provides libraries for standard access to configure interfaces as needed. Passing configuration from the Source CRD YAML, that the controller needs to configure the Receive Adapter Source library (provided by Knative): \u00b6 Controller runtime (this is what we share via injection) incorporates protocol specific config into \"generic controller\" CRD. Identifying specific event characteristics (i.e. value of interest, relevant metadata, etc) to pass along to the serverless system Propagating events internally to the system (i.e. cloudevents) Theory \u00b6 Quick Introduction to Knative Eventing Sources A Knative Source is Kubernetes Custom Resource that generates or imports an event and pushes that event to another endpoint on the cluster via a CloudEvents . The specification for Knative Eventing Sources contains a number of requirements that together define a well-behaved Knative Source. To achieve this, there are several separations of concerns that we have to keep in mind: 1. A controller to run our Event Source and reconcile the underlying Receive Adapter deployments 2. A \"receive adapter\" which generates or imports the actual events 3. A series of identifying characteristics for our event 4. Transporting a valid event to the serverless system for further processing There are also two different classes of developer to consider: 1. A \"contributor\" knows about the foreign protocol but is not a Knative expert. 2. Knative Eventing expert knows how Knative Eventing components are implemented, configured and deployed, but is not an expert in all the foreign protocols that sources may implement. These two roles will often not be the same person. We want to confine the job of the \"contributor\" to implementing the Receive Adapter , and specifying what configuration their adapter needs to connect, subscribe, or do whatever it does. The Knative Eventing developer exposes the protocol configuration as part of the Source CRD , and the controller passes any required configuration (which may include resolved data like URLs) to the Receive Adapter . API Resources required: KubeClientSet.Appsv1.Deployment (Inherited via the Eventing base reconciler) Used to deploy the Receive Adapter for \"importing\" events EventingClientSet.EventingV1Alpha1 (Inherited via the Eventing base reconciler) Used to interact with Events within the Knative system SourceClientSet.SourcesV1Alpha1 Used for source \u2014 in this case, samplesource \u2014 specific config and translated to the underlying deployment (via the inherited KubeClientSet) To ease writing a new event source, the eventing subsystem has offloaded several core functionalities (via injection) to the eventing-sources-controller . Fig 1. - Via shared Knative Dependency Injection Specifically, the clientset , cache , informers , and listers can all be generated and shared. Thus, they can be generated, imported, and assigned to the underlying reconciler when creating a new controller source implementation: import ( // ... sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\" samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\" ) // ... func NewController(ctx context.Context, cmw configmap.Watcher) *controller.Impl { sampleSourceInformer := samplesourceinformer.Get(ctx) r := &Reconciler{ // ... samplesourceClientSet: sampleSourceClient.Get(ctx), samplesourceLister: sampleSourceInformer.Lister(), // ... } Sample source's update-codegen.sh have the configuration to have the required things above generated and injected: # Generation ${ CODEGEN_PKG } /generate-groups.sh \"deepcopy,client,informer,lister\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt # Injection ${ KNATIVE_CODEGEN_PKG } /hack/generate-knative.sh \"injection\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt File Layout & Hierarchy: cmd/controller/main.go - Pass source\u2019s NewController implementation to the shared main cmd/receive_adapter/main.go - Translate resource variables to underlying adapter struct (to eventually be passed into the serverless system) pkg/reconciler/sample/controller.go - NewController implementation to pass to sharedmain pkg/reconciler/sample/samplesource.go - reconciliation functions for the receive adapter pkg/apis/samples/VERSION/samplesource_types.go - schema for the underlying api types (variables to be defined in the resource yaml) pkg/apis/samples/VERSION/samplesource_lifecycle.go - status updates for the source\u2019s reconciliation details Source ready Sink provided Deployed Eventtype Provided K8s Resources Correct pkg/adapter/adapter.go - receive_adapter functions supporting translation of events to CloudEvents","title":"Design of an Event Source"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/#topics","text":"What are the personas and critical paths? Contributor: implement a new source with minimal k8s overhead (don't have to learn controller/k8s internals) Operator: easily install Sources and verify that they are \"safe\" Developer: easily discover what Sources they can pull from on this cluster Developer: easily configure a Source based on existing knowledge of other Sources.","title":"Topics"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/#separation-of-concerns","text":"","title":"Separation of concerns"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/#contributor","text":"Receive Adapter (RA) - process that receives incoming events. Implement CloudEvent binding interfaces, cloudevent's go sdk provides libraries for standard access to configure interfaces as needed. Passing configuration from the Source CRD YAML, that the controller needs to configure the Receive Adapter","title":"Contributor:"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/#source-library-provided-by-knative","text":"Controller runtime (this is what we share via injection) incorporates protocol specific config into \"generic controller\" CRD. Identifying specific event characteristics (i.e. value of interest, relevant metadata, etc) to pass along to the serverless system Propagating events internally to the system (i.e. cloudevents)","title":"Source library (provided by Knative):"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/#theory","text":"Quick Introduction to Knative Eventing Sources A Knative Source is Kubernetes Custom Resource that generates or imports an event and pushes that event to another endpoint on the cluster via a CloudEvents . The specification for Knative Eventing Sources contains a number of requirements that together define a well-behaved Knative Source. To achieve this, there are several separations of concerns that we have to keep in mind: 1. A controller to run our Event Source and reconcile the underlying Receive Adapter deployments 2. A \"receive adapter\" which generates or imports the actual events 3. A series of identifying characteristics for our event 4. Transporting a valid event to the serverless system for further processing There are also two different classes of developer to consider: 1. A \"contributor\" knows about the foreign protocol but is not a Knative expert. 2. Knative Eventing expert knows how Knative Eventing components are implemented, configured and deployed, but is not an expert in all the foreign protocols that sources may implement. These two roles will often not be the same person. We want to confine the job of the \"contributor\" to implementing the Receive Adapter , and specifying what configuration their adapter needs to connect, subscribe, or do whatever it does. The Knative Eventing developer exposes the protocol configuration as part of the Source CRD , and the controller passes any required configuration (which may include resolved data like URLs) to the Receive Adapter . API Resources required: KubeClientSet.Appsv1.Deployment (Inherited via the Eventing base reconciler) Used to deploy the Receive Adapter for \"importing\" events EventingClientSet.EventingV1Alpha1 (Inherited via the Eventing base reconciler) Used to interact with Events within the Knative system SourceClientSet.SourcesV1Alpha1 Used for source \u2014 in this case, samplesource \u2014 specific config and translated to the underlying deployment (via the inherited KubeClientSet) To ease writing a new event source, the eventing subsystem has offloaded several core functionalities (via injection) to the eventing-sources-controller . Fig 1. - Via shared Knative Dependency Injection Specifically, the clientset , cache , informers , and listers can all be generated and shared. Thus, they can be generated, imported, and assigned to the underlying reconciler when creating a new controller source implementation: import ( // ... sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\" samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\" ) // ... func NewController(ctx context.Context, cmw configmap.Watcher) *controller.Impl { sampleSourceInformer := samplesourceinformer.Get(ctx) r := &Reconciler{ // ... samplesourceClientSet: sampleSourceClient.Get(ctx), samplesourceLister: sampleSourceInformer.Lister(), // ... } Sample source's update-codegen.sh have the configuration to have the required things above generated and injected: # Generation ${ CODEGEN_PKG } /generate-groups.sh \"deepcopy,client,informer,lister\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt # Injection ${ KNATIVE_CODEGEN_PKG } /hack/generate-knative.sh \"injection\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt File Layout & Hierarchy: cmd/controller/main.go - Pass source\u2019s NewController implementation to the shared main cmd/receive_adapter/main.go - Translate resource variables to underlying adapter struct (to eventually be passed into the serverless system) pkg/reconciler/sample/controller.go - NewController implementation to pass to sharedmain pkg/reconciler/sample/samplesource.go - reconciliation functions for the receive adapter pkg/apis/samples/VERSION/samplesource_types.go - schema for the underlying api types (variables to be defined in the resource yaml) pkg/apis/samples/VERSION/samplesource_lifecycle.go - status updates for the source\u2019s reconciliation details Source ready Sink provided Deployed Eventtype Provided K8s Resources Correct pkg/adapter/adapter.go - receive_adapter functions supporting translation of events to CloudEvents","title":"Theory"},{"location":"eventing/sources/creating-event-sources/writing-event-source/02-lifecycle-and-types/","text":"Sample Source Lifecycle and Types \u00b6 API Definition \u00b6 Define the types required in the resource\u2019s schema in pkg/apis/samples/v1alpha1/samplesource_types.go This includes the fields that will be required in the resource yaml as well as what will be referenced in the controller using the source\u2019s clientset and API // +genclient // +genreconciler // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // +k8s:openapi-gen=true type SampleSource struct { metav1 . TypeMeta `json:\",inline\"` // +optional metav1 . ObjectMeta `json:\"metadata,omitempty\"` // Spec holds the desired state of the SampleSource (from the client). Spec SampleSourceSpec `json:\"spec\"` // Status communicates the observed state of the SampleSource (from the controller). // +optional Status SampleSourceStatus `json:\"status,omitempty\"` } // SampleSourceSpec holds the desired state of the SampleSource (from the client). type SampleSourceSpec struct { // inherits duck/v1 SourceSpec, which currently provides: // * Sink - a reference to an object that will resolve to a domain name or // a URI directly to use as the sink. // * CloudEventOverrides - defines overrides to control the output format // and modifications of the event sent to the sink. duckv1 . SourceSpec `json:\",inline\"` // ServiceAccountName holds the name of the Kubernetes service account // as which the underlying K8s resources should be run. If unspecified // this will default to the \"default\" service account for the namespace // in which the SampleSource exists. // +optional ServiceAccountName string `json:\"serviceAccountName,omitempty\"` // Interval is the time interval between events. // // The string format is a sequence of decimal numbers, each with optional // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified // this will default to \"10s\". Interval string `json:\"interval\"` } // SampleSourceStatus communicates the observed state of the SampleSource (from the controller). type SampleSourceStatus struct { duckv1 . Status `json:\",inline\"` // SinkURI is the current active sink URI that has been configured // for the SampleSource. // +optional SinkURI * apis . URL `json:\"sinkUri,omitempty\"` } Define the lifecycle that will be reflected in the status and SinkURI fields const ( // SampleConditionReady has status True when the SampleSource is ready to send events. SampleConditionReady = apis . ConditionReady // ... ) Define the functions that will be called from the Reconciler functions to set the lifecycle conditions. This is typically done in pkg/apis/samples/VERSION/samplesource_lifecycle.go // InitializeConditions sets relevant unset conditions to Unknown state. func ( s * SampleSourceStatus ) InitializeConditions () { SampleCondSet . Manage ( s ). InitializeConditions () } ... // MarkSink sets the condition that the source has a sink configured. func ( s * SampleSourceStatus ) MarkSink ( uri * apis . URL ) { s . SinkURI = uri if len ( uri . String ()) > 0 { SampleCondSet . Manage ( s ). MarkTrue ( SampleConditionSinkProvided ) } else { SampleCondSet . Manage ( s ). MarkUnknown ( SampleConditionSinkProvided , \"SinkEmpty\" , \"Sink has resolved to empty.%s\" , \"\" ) } } // MarkNoSink sets the condition that the source does not have a sink configured. func ( s * SampleSourceStatus ) MarkNoSink ( reason , messageFormat string , messageA ... interface {}) { SampleCondSet . Manage ( s ). MarkFalse ( SampleConditionSinkProvided , reason , messageFormat , messageA ... ) }","title":"Lifecycle and Types"},{"location":"eventing/sources/creating-event-sources/writing-event-source/02-lifecycle-and-types/#sample-source-lifecycle-and-types","text":"","title":"Sample Source Lifecycle and Types"},{"location":"eventing/sources/creating-event-sources/writing-event-source/02-lifecycle-and-types/#api-definition","text":"Define the types required in the resource\u2019s schema in pkg/apis/samples/v1alpha1/samplesource_types.go This includes the fields that will be required in the resource yaml as well as what will be referenced in the controller using the source\u2019s clientset and API // +genclient // +genreconciler // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // +k8s:openapi-gen=true type SampleSource struct { metav1 . TypeMeta `json:\",inline\"` // +optional metav1 . ObjectMeta `json:\"metadata,omitempty\"` // Spec holds the desired state of the SampleSource (from the client). Spec SampleSourceSpec `json:\"spec\"` // Status communicates the observed state of the SampleSource (from the controller). // +optional Status SampleSourceStatus `json:\"status,omitempty\"` } // SampleSourceSpec holds the desired state of the SampleSource (from the client). type SampleSourceSpec struct { // inherits duck/v1 SourceSpec, which currently provides: // * Sink - a reference to an object that will resolve to a domain name or // a URI directly to use as the sink. // * CloudEventOverrides - defines overrides to control the output format // and modifications of the event sent to the sink. duckv1 . SourceSpec `json:\",inline\"` // ServiceAccountName holds the name of the Kubernetes service account // as which the underlying K8s resources should be run. If unspecified // this will default to the \"default\" service account for the namespace // in which the SampleSource exists. // +optional ServiceAccountName string `json:\"serviceAccountName,omitempty\"` // Interval is the time interval between events. // // The string format is a sequence of decimal numbers, each with optional // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified // this will default to \"10s\". Interval string `json:\"interval\"` } // SampleSourceStatus communicates the observed state of the SampleSource (from the controller). type SampleSourceStatus struct { duckv1 . Status `json:\",inline\"` // SinkURI is the current active sink URI that has been configured // for the SampleSource. // +optional SinkURI * apis . URL `json:\"sinkUri,omitempty\"` } Define the lifecycle that will be reflected in the status and SinkURI fields const ( // SampleConditionReady has status True when the SampleSource is ready to send events. SampleConditionReady = apis . ConditionReady // ... ) Define the functions that will be called from the Reconciler functions to set the lifecycle conditions. This is typically done in pkg/apis/samples/VERSION/samplesource_lifecycle.go // InitializeConditions sets relevant unset conditions to Unknown state. func ( s * SampleSourceStatus ) InitializeConditions () { SampleCondSet . Manage ( s ). InitializeConditions () } ... // MarkSink sets the condition that the source has a sink configured. func ( s * SampleSourceStatus ) MarkSink ( uri * apis . URL ) { s . SinkURI = uri if len ( uri . String ()) > 0 { SampleCondSet . Manage ( s ). MarkTrue ( SampleConditionSinkProvided ) } else { SampleCondSet . Manage ( s ). MarkUnknown ( SampleConditionSinkProvided , \"SinkEmpty\" , \"Sink has resolved to empty.%s\" , \"\" ) } } // MarkNoSink sets the condition that the source does not have a sink configured. func ( s * SampleSourceStatus ) MarkNoSink ( reason , messageFormat string , messageA ... interface {}) { SampleCondSet . Manage ( s ). MarkFalse ( SampleConditionSinkProvided , reason , messageFormat , messageA ... ) }","title":"API Definition"},{"location":"eventing/sources/creating-event-sources/writing-event-source/03-controller/","text":"Controller Implementation \u00b6 cmd/controller \u00b6 Pass the new controller implementation to the shared main import ( // The set of controllers this controller process runs. \"knative.dev/sample-source/pkg/reconciler/sample\" // This defines the shared main for injected controllers. \"knative.dev/pkg/injection/sharedmain\" ) func main () { sharedmain . Main ( \"sample-source-controller\" , sample . NewController ) } Define the NewController implementation, it will be passed a configmap.Watcher , as well as a context which the injected listers will use for the reconciler struct arguments func NewController ( ctx context . Context , cmw configmap . Watcher , ) * controller . Impl { // ... deploymentInformer := deploymentinformer . Get ( ctx ) sinkBindingInformer := sinkbindinginformer . Get ( ctx ) sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { dr : & reconciler . DeploymentReconciler { KubeClientSet : kubeclient . Get ( ctx )}, sbr : & reconciler . SinkBindingReconciler { EventingClientSet : eventingclient . Get ( ctx )}, // Config accessor takes care of tracing/config/logging config propagation to the receive adapter configAccessor : reconcilersource . WatchConfigurations ( ctx , \"sample-source\" , cmw ), } The base reconciler is imported from the knative.dev/pkg dependency: import ( // ... reconcilersource \"knative.dev/eventing/pkg/reconciler/source\" // ... ) Ensure the correct informers have EventHandlers filtered to them sampleSourceInformer . Informer (). AddEventHandler ( controller . HandleAll ( impl . Enqueue )) Controller for the SampleSource uses Deployment and SinkBinding resources to deploy and also bind the event source and the receive adapter. Also ensure the informers are set up correctly for these secondary resources deploymentInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), }) sinkBindingInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), })","title":"Controller Implemetation"},{"location":"eventing/sources/creating-event-sources/writing-event-source/03-controller/#controller-implementation","text":"","title":"Controller Implementation"},{"location":"eventing/sources/creating-event-sources/writing-event-source/03-controller/#cmdcontroller","text":"Pass the new controller implementation to the shared main import ( // The set of controllers this controller process runs. \"knative.dev/sample-source/pkg/reconciler/sample\" // This defines the shared main for injected controllers. \"knative.dev/pkg/injection/sharedmain\" ) func main () { sharedmain . Main ( \"sample-source-controller\" , sample . NewController ) } Define the NewController implementation, it will be passed a configmap.Watcher , as well as a context which the injected listers will use for the reconciler struct arguments func NewController ( ctx context . Context , cmw configmap . Watcher , ) * controller . Impl { // ... deploymentInformer := deploymentinformer . Get ( ctx ) sinkBindingInformer := sinkbindinginformer . Get ( ctx ) sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { dr : & reconciler . DeploymentReconciler { KubeClientSet : kubeclient . Get ( ctx )}, sbr : & reconciler . SinkBindingReconciler { EventingClientSet : eventingclient . Get ( ctx )}, // Config accessor takes care of tracing/config/logging config propagation to the receive adapter configAccessor : reconcilersource . WatchConfigurations ( ctx , \"sample-source\" , cmw ), } The base reconciler is imported from the knative.dev/pkg dependency: import ( // ... reconcilersource \"knative.dev/eventing/pkg/reconciler/source\" // ... ) Ensure the correct informers have EventHandlers filtered to them sampleSourceInformer . Informer (). AddEventHandler ( controller . HandleAll ( impl . Enqueue )) Controller for the SampleSource uses Deployment and SinkBinding resources to deploy and also bind the event source and the receive adapter. Also ensure the informers are set up correctly for these secondary resources deploymentInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), }) sinkBindingInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), })","title":"cmd/controller"},{"location":"eventing/sources/creating-event-sources/writing-event-source/04-reconciler/","text":"Reconciler Implementation and Design \u00b6 Reconciler Functionality \u00b6 General steps the reconciliation process needs to cover: 1. Update the ObservedGeneration and initialize the Status conditions (as defined in samplesource_lifecycle.go and samplesource_types.go ) src . Status . InitializeConditions () src . Status . ObservedGeneration = src . Generation 2. Create/reconcile the Receive Adapter (detailed below) 3. If successful, update the Status and MarkDeployed src . Status . PropagateDeploymentAvailability ( ra ) 4. Create/reconcile the SinkBinding for the Receive Adapter targeting the Sink (detailed below) 5. MarkSink with the result src . Status . MarkSink ( sb . Status . SinkURI ) 6. Return a new reconciler event stating that the process is done return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SampleSourceReconciled\" , \"SampleSource reconciled: \\\"%s/%s\\\"\" , namespace , name ) Reconcile/Create The Receive Adapter \u00b6 As part of the source reconciliation, we have to create and deploy (and update if necessary) the underlying receive adapter. Verify the specified kubernetes resources are valid, and update the Status accordingly Assemble the ReceiveAdapterArgs raArgs := resources . ReceiveAdapterArgs { EventSource : src . Namespace + \"/\" + src . Name , Image : r . ReceiveAdapterImage , Source : src , Labels : resources . Labels ( src . Name ), AdditionalEnvs : r . configAccessor . ToEnvVars (), // Grab config envs for tracing/logging/metrics } NB The exact arguments may change based on functional requirements Create the underlying deployment from the arguments provided, matching pod templates, labels, owner references, etc as needed to fill out the deployment Example: pkg/reconciler/sample/resources/receive_adapter.go Fetch the existing receive adapter deployment namespace := owner . GetObjectMeta (). GetNamespace () ra , err := r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) Otherwise, create the deployment ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Create ( expected ) Check if the expected vs existing spec is different, and update the deployment if required } else if r . podSpecImageSync ( expected . Spec . Template . Spec , ra . Spec . Template . Spec ) { ra . Spec . Template . Spec = expected . Spec . Template . Spec if ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Update ( ra ); err != nil { return ra , err } If updated, record the event return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"DeploymentUpdated\" , \"updated deployment: \\\"%s/%s\\\"\" , namespace , name ) Reconcile/Create The SinkBinding \u00b6 Instead of directly giving the details of the sink to the receive adapter, use a SinkBinding to bind the receive adapter with the sink. Steps here are almost the same with the Deployment reconciliation above, but it is for another resource, SinkBinding . Create a Reference for the receive adapter deployment. This deployment will be SinkBinding 's source: tracker . Reference { APIVersion : appsv1 . SchemeGroupVersion . String (), Kind : \"Deployment\" , Namespace : ra . Namespace , Name : ra . Name , } Fetch the existing SinkBinding namespace := owner . GetObjectMeta (). GetNamespace () sb , err := r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If it doesn't exist, create it sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Create ( expected ) Check if the expected vs existing spec is different, and update the SinkBinding if required else if r . specChanged ( sb . Spec , expected . Spec ) { sb . Spec = expected . Spec if sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Update ( sb ); err != nil { return sb , err } If updated, record the event return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SinkBindingUpdated\" , \"updated SinkBinding: \\\"%s/%s\\\"\" , namespace , name )","title":"Reconciler Implementation"},{"location":"eventing/sources/creating-event-sources/writing-event-source/04-reconciler/#reconciler-implementation-and-design","text":"","title":"Reconciler Implementation and Design"},{"location":"eventing/sources/creating-event-sources/writing-event-source/04-reconciler/#reconciler-functionality","text":"General steps the reconciliation process needs to cover: 1. Update the ObservedGeneration and initialize the Status conditions (as defined in samplesource_lifecycle.go and samplesource_types.go ) src . Status . InitializeConditions () src . Status . ObservedGeneration = src . Generation 2. Create/reconcile the Receive Adapter (detailed below) 3. If successful, update the Status and MarkDeployed src . Status . PropagateDeploymentAvailability ( ra ) 4. Create/reconcile the SinkBinding for the Receive Adapter targeting the Sink (detailed below) 5. MarkSink with the result src . Status . MarkSink ( sb . Status . SinkURI ) 6. Return a new reconciler event stating that the process is done return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SampleSourceReconciled\" , \"SampleSource reconciled: \\\"%s/%s\\\"\" , namespace , name )","title":"Reconciler Functionality"},{"location":"eventing/sources/creating-event-sources/writing-event-source/04-reconciler/#reconcilecreate-the-receive-adapter","text":"As part of the source reconciliation, we have to create and deploy (and update if necessary) the underlying receive adapter. Verify the specified kubernetes resources are valid, and update the Status accordingly Assemble the ReceiveAdapterArgs raArgs := resources . ReceiveAdapterArgs { EventSource : src . Namespace + \"/\" + src . Name , Image : r . ReceiveAdapterImage , Source : src , Labels : resources . Labels ( src . Name ), AdditionalEnvs : r . configAccessor . ToEnvVars (), // Grab config envs for tracing/logging/metrics } NB The exact arguments may change based on functional requirements Create the underlying deployment from the arguments provided, matching pod templates, labels, owner references, etc as needed to fill out the deployment Example: pkg/reconciler/sample/resources/receive_adapter.go Fetch the existing receive adapter deployment namespace := owner . GetObjectMeta (). GetNamespace () ra , err := r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) Otherwise, create the deployment ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Create ( expected ) Check if the expected vs existing spec is different, and update the deployment if required } else if r . podSpecImageSync ( expected . Spec . Template . Spec , ra . Spec . Template . Spec ) { ra . Spec . Template . Spec = expected . Spec . Template . Spec if ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Update ( ra ); err != nil { return ra , err } If updated, record the event return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"DeploymentUpdated\" , \"updated deployment: \\\"%s/%s\\\"\" , namespace , name )","title":"Reconcile/Create The Receive Adapter"},{"location":"eventing/sources/creating-event-sources/writing-event-source/04-reconciler/#reconcilecreate-the-sinkbinding","text":"Instead of directly giving the details of the sink to the receive adapter, use a SinkBinding to bind the receive adapter with the sink. Steps here are almost the same with the Deployment reconciliation above, but it is for another resource, SinkBinding . Create a Reference for the receive adapter deployment. This deployment will be SinkBinding 's source: tracker . Reference { APIVersion : appsv1 . SchemeGroupVersion . String (), Kind : \"Deployment\" , Namespace : ra . Namespace , Name : ra . Name , } Fetch the existing SinkBinding namespace := owner . GetObjectMeta (). GetNamespace () sb , err := r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If it doesn't exist, create it sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Create ( expected ) Check if the expected vs existing spec is different, and update the SinkBinding if required else if r . specChanged ( sb . Spec , expected . Spec ) { sb . Spec = expected . Spec if sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Update ( sb ); err != nil { return sb , err } If updated, record the event return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SinkBindingUpdated\" , \"updated SinkBinding: \\\"%s/%s\\\"\" , namespace , name )","title":"Reconcile/Create The SinkBinding"},{"location":"eventing/sources/creating-event-sources/writing-event-source/05-receive-adapter/","text":"Receive Adapter Implementation and Design \u00b6 Receive Adapter cmd \u00b6 Similar to the controller, we'll need an injection based main.go similar to the controller under cmd/receiver_adapter/main.go // This Adapter generates events at a regular interval. package main import ( \"knative.dev/eventing/pkg/adapter\" myadapter \"knative.dev/sample-source/pkg/adapter\" ) func main () { adapter . Main ( \"sample-source\" , myadapter . NewEnv , myadapter . NewAdapter ) } Defining NewAdapter implementation and Start function \u00b6 The adapter's pkg implementation consists of two main functions; A NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {} call, which creates the new adapter with passed variables via the EnvConfigAccessor . The created adapter will be passed the cloudevents client (which is where the events are forwarded to). This is sometimes referred to as a sink, or ceClient in the Knative ecosystem. The return value is a reference to the adapter as defined by the adapter's local struct. In our sample-source 's case; // Adapter generates events at a regular interval. type Adapter struct { logger * zap . Logger interval time . Duration nextID int client cloudevents . Client } Start function implemented as an interface to the adapter struct. func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { stopCh is the signal to stop the Adapter. Otherwise the role of the function is to process the next event. In the case of the sample-source , it creates an event to forward to the specified cloudevent sink/client every X interval, as specified by the loaded EnvConfigAccessor (loaded via the resource yaml). func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { a . logger . Infow ( \"Starting heartbeat\" , zap . String ( \"interval\" , a . interval . String ())) for { select { case <- time . After ( a . interval ): event := a . newEvent () a . logger . Infow ( \"Sending new event\" , zap . String ( \"event\" , event . String ())) if result := a . client . Send ( context . Background (), event ); ! cloudevents . IsACK ( result ) { a . logger . Infow ( \"failed to send event\" , zap . String ( \"event\" , event . String ()), zap . Error ( result )) // We got an error but it could be transient, try again next interval. continue } case <- stopCh : a . logger . Info ( \"Shutting down...\" ) return nil } } }","title":"Receive Adapter Implementation"},{"location":"eventing/sources/creating-event-sources/writing-event-source/05-receive-adapter/#receive-adapter-implementation-and-design","text":"","title":"Receive Adapter Implementation and Design"},{"location":"eventing/sources/creating-event-sources/writing-event-source/05-receive-adapter/#receive-adapter-cmd","text":"Similar to the controller, we'll need an injection based main.go similar to the controller under cmd/receiver_adapter/main.go // This Adapter generates events at a regular interval. package main import ( \"knative.dev/eventing/pkg/adapter\" myadapter \"knative.dev/sample-source/pkg/adapter\" ) func main () { adapter . Main ( \"sample-source\" , myadapter . NewEnv , myadapter . NewAdapter ) }","title":"Receive Adapter cmd"},{"location":"eventing/sources/creating-event-sources/writing-event-source/05-receive-adapter/#defining-newadapter-implementation-and-start-function","text":"The adapter's pkg implementation consists of two main functions; A NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {} call, which creates the new adapter with passed variables via the EnvConfigAccessor . The created adapter will be passed the cloudevents client (which is where the events are forwarded to). This is sometimes referred to as a sink, or ceClient in the Knative ecosystem. The return value is a reference to the adapter as defined by the adapter's local struct. In our sample-source 's case; // Adapter generates events at a regular interval. type Adapter struct { logger * zap . Logger interval time . Duration nextID int client cloudevents . Client } Start function implemented as an interface to the adapter struct. func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { stopCh is the signal to stop the Adapter. Otherwise the role of the function is to process the next event. In the case of the sample-source , it creates an event to forward to the specified cloudevent sink/client every X interval, as specified by the loaded EnvConfigAccessor (loaded via the resource yaml). func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { a . logger . Infow ( \"Starting heartbeat\" , zap . String ( \"interval\" , a . interval . String ())) for { select { case <- time . After ( a . interval ): event := a . newEvent () a . logger . Infow ( \"Sending new event\" , zap . String ( \"event\" , event . String ())) if result := a . client . Send ( context . Background (), event ); ! cloudevents . IsACK ( result ) { a . logger . Infow ( \"failed to send event\" , zap . String ( \"event\" , event . String ()), zap . Error ( result )) // We got an error but it could be transient, try again next interval. continue } case <- stopCh : a . logger . Info ( \"Shutting down...\" ) return nil } } }","title":"Defining NewAdapter implementation and Start function"},{"location":"eventing/sources/creating-event-sources/writing-event-source/06-yaml/","text":"Publishing to your Kubernetes cluster \u00b6 Run the sample source locally \u00b6 Start a minikube cluster. If you already have a Kubernetes cluster running, you can skip this step. The cluster must be 1.15+ minikube start Setup ko to use the minikube docker instance and local registry eval $( minikube docker-env ) export KO_DOCKER_REPO = ko.local Apply the CRD and configuration yaml ko apply -f config Once the sample-source-controller-manager is running in the knative-samples namespace, you can apply the example.yaml to connect our sample-source every 10s directly to a ksvc . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : knative-samples spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : samples.knative.dev/v1alpha1 kind : SampleSource metadata : name : sample-source namespace : knative-samples spec : interval : \"10s\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display ko apply -f example.yaml Once reconciled, you can confirm the ksvc is outputting valid cloudevents every 10s to align with our specified interval. % kubectl -n knative-samples logs -l serving.knative.dev/service = event-display -c user-container -f \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: d4619592-363e-4a41-82d1-b1586c390e24 time: 2019-12-17T01:31:10.795588888Z datacontenttype: application/json Data, { \"Sequence\": 0, \"Heartbeat\": \"10s\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: db2edad0-06bc-4234-b9e1-7ea3955841d6 time: 2019-12-17T01:31:20.825969504Z datacontenttype: application/json Data, { \"Sequence\": 1, \"Heartbeat\": \"10s\" }","title":"Publishing to your cluster"},{"location":"eventing/sources/creating-event-sources/writing-event-source/06-yaml/#publishing-to-your-kubernetes-cluster","text":"","title":"Publishing to your Kubernetes cluster"},{"location":"eventing/sources/creating-event-sources/writing-event-source/06-yaml/#run-the-sample-source-locally","text":"Start a minikube cluster. If you already have a Kubernetes cluster running, you can skip this step. The cluster must be 1.15+ minikube start Setup ko to use the minikube docker instance and local registry eval $( minikube docker-env ) export KO_DOCKER_REPO = ko.local Apply the CRD and configuration yaml ko apply -f config Once the sample-source-controller-manager is running in the knative-samples namespace, you can apply the example.yaml to connect our sample-source every 10s directly to a ksvc . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : knative-samples spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : samples.knative.dev/v1alpha1 kind : SampleSource metadata : name : sample-source namespace : knative-samples spec : interval : \"10s\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display ko apply -f example.yaml Once reconciled, you can confirm the ksvc is outputting valid cloudevents every 10s to align with our specified interval. % kubectl -n knative-samples logs -l serving.knative.dev/service = event-display -c user-container -f \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: d4619592-363e-4a41-82d1-b1586c390e24 time: 2019-12-17T01:31:10.795588888Z datacontenttype: application/json Data, { \"Sequence\": 0, \"Heartbeat\": \"10s\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: db2edad0-06bc-4234-b9e1-7ea3955841d6 time: 2019-12-17T01:31:20.825969504Z datacontenttype: application/json Data, { \"Sequence\": 1, \"Heartbeat\": \"10s\" }","title":"Run the sample source locally"},{"location":"eventing/sources/creating-event-sources/writing-event-source/07-knative-sandbox/","text":"Moving the event source to knative-sandbox \u00b6 If you would like to move your source over to the knative-sandbox organization follow the instructions to create a sandbox repository .","title":"Moving to knative-sandbox"},{"location":"eventing/sources/creating-event-sources/writing-event-source/07-knative-sandbox/#moving-the-event-source-to-knative-sandbox","text":"If you would like to move your source over to the knative-sandbox organization follow the instructions to create a sandbox repository .","title":"Moving the event source to knative-sandbox"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/","text":"Writing an event source using Javascript \u00b6 This tutorial provides instructions to build an event source in Javascript and implement it with a ContainerSource or SinkBinding. Using a ContainerSource is a simple way to turn any dispatcher container into a Knative event source. Using SinkBinding provides a framework for injecting environment variables into any Kubernetes resource that has a spec.template and is PodSpecable . ContainerSource and SinkBinding both work by injecting environment variables to an application. Injected environment variables at minimum contain the URL of a sink that will receive events. Bootstrapping \u00b6 Create the project and add the dependencies: npm init npm install cloudevents-sdk@2.0.1 --save NOTE: Due to this bug , you must use version 2.0.1 of the Javascript SDK or newer. Using ContainerSource \u00b6 A ContainerSource creates a container for your event source image and manages this container. The sink URL to post the events will be made available to the application through the K_SINK environment variable by the ContainerSource. Example \u00b6 The following example event source emits an event to the sink every 1000 milliseconds: // File - index.js const { CloudEvent , HTTPEmitter } = require ( \"cloudevents-sdk\" ); let sinkUrl = process . env [ 'K_SINK' ]; console . log ( \"Sink URL is \" + sinkUrl ); let emitter = new HTTPEmitter ({ url : sinkUrl }); let eventIndex = 0 ; setInterval ( function () { console . log ( \"Emitting event #\" + ++ eventIndex ); let myevent = new CloudEvent ({ source : \"urn:event:from:my-api/resource/123\" , type : \"your.event.source.type\" , id : \"your-event-id\" , dataContentType : \"application/json\" , data : { \"hello\" : \"World \" + eventIndex }, }); // Emit the event emitter . send ( myevent ) . then ( response => { // Treat the response console . log ( \"Event posted successfully\" ); console . log ( response . data ); }) . catch ( err => { // Deal with errors console . log ( \"Error during event post\" ); console . error ( err ); }); }, 1000 ); # File - Dockerfile FROM node:10 WORKDIR /usr/src/app COPY package*.json ./ RUN npm install COPY . . EXPOSE 8080 CMD [ \"node\" , \"index.js\" ] The example code uses Binary mode for CloudEvents. To employ structured code, change let binding = new v1.BinaryHTTPEmitter(config); to let binding = new v1.StructuredHTTPEmitter(config); . Binary mode is used in most cases because: - It is faster in terms of serialization and deserialization. - It works better with CloudEvent-aware proxies, such as Knative Channels, and can simply check the header instead of parsing the payload. Procedure \u00b6 Build and push the image: docker build . -t path/to/image/registry/node-knative-heartbeat-source:v1 docker push path/to/image/registry/node-knative-heartbeat-source:v1 Create the event display service which logs any CloudEvents posted to it: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : docker.io/aliok/event_display-864884f202126ec3150c5fcef437d90c@sha256:93cb4dcda8fee80a1f68662ae6bf20301471b046ede628f3c3f94f39752fbe08 Create the ContainerSource object: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : path/to/image/registry/node-knative-heartbeat-source:v1 name : heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Check the logs of the event display service. You will see a new message is pushed every second: $ kubectl logs -l serving.knative.dev/service = event-display -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" } Optional: If you are interested in seeing what is injected into the event source as a K_SINK , you can check the logs: $ kubectl logs test-heartbeats-deployment-7575c888c7-85w5t Sink URL is http://event-display.default.svc.cluster.local Emitting event #1 Emitting event #2 Event posted successfully Event posted successfully Using SinkBinding \u00b6 SinkBinding does not create any containers. It injects the sink information to an already existing Kubernetes resources. This is a flexible approach as you can use any Kubernetes PodSpecable object as an event source, such as Deployment, Job, or Knative services. Procedure \u00b6 Create an event display service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : docker.io/aliok/event_display-864884f202126ec3150c5fcef437d90c@sha256:93cb4dcda8fee80a1f68662ae6bf20301471b046ede628f3c3f94f39752fbe08 Create a Kubernetes deployment that runs the event source: apiVersion : apps/v1 kind : Deployment metadata : name : node-heartbeats-deployment labels : app : node-heartbeats spec : replicas : 2 selector : matchLabels : app : node-heartbeats template : metadata : labels : app : node-heartbeats spec : containers : - name : node-heartbeats image : path/to/image/registry/node-knative-heartbeat-source:v1 ports : - containerPort : 8080 Because the SinkBinding has not yet been created, you will see an error message, because the K_SINK environment variable is not yet injected: $ kubectl logs node-heartbeats-deployment-9ffbb644b-llkzk Sink URL is undefined Emitting event #1 Error during event post TypeError [ ERR_INVALID_ARG_TYPE ] : The \"url\" argument must be of type string. Received type undefined Create the SinkBinding object: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-node-heartbeat spec : subject : apiVersion : apps/v1 kind : Deployment selector : matchLabels : app : node-heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display You will see the pods are recreated and this time the K_SINK environment variable is injected. Also note that since the replicas is set to 2, there will be 2 pods that are posting events to the sink. $ kubectl logs event-display-dpplv-deployment-67c9949cf9-bvjvk -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" }","title":"Writing an event source using Javascript"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/#writing-an-event-source-using-javascript","text":"This tutorial provides instructions to build an event source in Javascript and implement it with a ContainerSource or SinkBinding. Using a ContainerSource is a simple way to turn any dispatcher container into a Knative event source. Using SinkBinding provides a framework for injecting environment variables into any Kubernetes resource that has a spec.template and is PodSpecable . ContainerSource and SinkBinding both work by injecting environment variables to an application. Injected environment variables at minimum contain the URL of a sink that will receive events.","title":"Writing an event source using Javascript"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/#bootstrapping","text":"Create the project and add the dependencies: npm init npm install cloudevents-sdk@2.0.1 --save NOTE: Due to this bug , you must use version 2.0.1 of the Javascript SDK or newer.","title":"Bootstrapping"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/#using-containersource","text":"A ContainerSource creates a container for your event source image and manages this container. The sink URL to post the events will be made available to the application through the K_SINK environment variable by the ContainerSource.","title":"Using ContainerSource"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/#example","text":"The following example event source emits an event to the sink every 1000 milliseconds: // File - index.js const { CloudEvent , HTTPEmitter } = require ( \"cloudevents-sdk\" ); let sinkUrl = process . env [ 'K_SINK' ]; console . log ( \"Sink URL is \" + sinkUrl ); let emitter = new HTTPEmitter ({ url : sinkUrl }); let eventIndex = 0 ; setInterval ( function () { console . log ( \"Emitting event #\" + ++ eventIndex ); let myevent = new CloudEvent ({ source : \"urn:event:from:my-api/resource/123\" , type : \"your.event.source.type\" , id : \"your-event-id\" , dataContentType : \"application/json\" , data : { \"hello\" : \"World \" + eventIndex }, }); // Emit the event emitter . send ( myevent ) . then ( response => { // Treat the response console . log ( \"Event posted successfully\" ); console . log ( response . data ); }) . catch ( err => { // Deal with errors console . log ( \"Error during event post\" ); console . error ( err ); }); }, 1000 ); # File - Dockerfile FROM node:10 WORKDIR /usr/src/app COPY package*.json ./ RUN npm install COPY . . EXPOSE 8080 CMD [ \"node\" , \"index.js\" ] The example code uses Binary mode for CloudEvents. To employ structured code, change let binding = new v1.BinaryHTTPEmitter(config); to let binding = new v1.StructuredHTTPEmitter(config); . Binary mode is used in most cases because: - It is faster in terms of serialization and deserialization. - It works better with CloudEvent-aware proxies, such as Knative Channels, and can simply check the header instead of parsing the payload.","title":"Example"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/#procedure","text":"Build and push the image: docker build . -t path/to/image/registry/node-knative-heartbeat-source:v1 docker push path/to/image/registry/node-knative-heartbeat-source:v1 Create the event display service which logs any CloudEvents posted to it: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : docker.io/aliok/event_display-864884f202126ec3150c5fcef437d90c@sha256:93cb4dcda8fee80a1f68662ae6bf20301471b046ede628f3c3f94f39752fbe08 Create the ContainerSource object: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : path/to/image/registry/node-knative-heartbeat-source:v1 name : heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Check the logs of the event display service. You will see a new message is pushed every second: $ kubectl logs -l serving.knative.dev/service = event-display -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" } Optional: If you are interested in seeing what is injected into the event source as a K_SINK , you can check the logs: $ kubectl logs test-heartbeats-deployment-7575c888c7-85w5t Sink URL is http://event-display.default.svc.cluster.local Emitting event #1 Emitting event #2 Event posted successfully Event posted successfully","title":"Procedure"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/#using-sinkbinding","text":"SinkBinding does not create any containers. It injects the sink information to an already existing Kubernetes resources. This is a flexible approach as you can use any Kubernetes PodSpecable object as an event source, such as Deployment, Job, or Knative services.","title":"Using SinkBinding"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/#procedure_1","text":"Create an event display service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : docker.io/aliok/event_display-864884f202126ec3150c5fcef437d90c@sha256:93cb4dcda8fee80a1f68662ae6bf20301471b046ede628f3c3f94f39752fbe08 Create a Kubernetes deployment that runs the event source: apiVersion : apps/v1 kind : Deployment metadata : name : node-heartbeats-deployment labels : app : node-heartbeats spec : replicas : 2 selector : matchLabels : app : node-heartbeats template : metadata : labels : app : node-heartbeats spec : containers : - name : node-heartbeats image : path/to/image/registry/node-knative-heartbeat-source:v1 ports : - containerPort : 8080 Because the SinkBinding has not yet been created, you will see an error message, because the K_SINK environment variable is not yet injected: $ kubectl logs node-heartbeats-deployment-9ffbb644b-llkzk Sink URL is undefined Emitting event #1 Error during event post TypeError [ ERR_INVALID_ARG_TYPE ] : The \"url\" argument must be of type string. Received type undefined Create the SinkBinding object: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-node-heartbeat spec : subject : apiVersion : apps/v1 kind : Deployment selector : matchLabels : app : node-heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display You will see the pods are recreated and this time the K_SINK environment variable is injected. Also note that since the replicas is set to 2, there will be 2 pods that are posting events to the sink. $ kubectl logs event-display-dpplv-deployment-67c9949cf9-bvjvk -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" }","title":"Procedure"},{"location":"eventing/sources/ping-source/","text":"PingSource \u00b6 A PingSource is an event source that produces events with a fixed payload on a specified cron schedule. The following example shows how you can configure a PingSource as an event source that sends events every minute to a Knative service named event-display that is used as a sink. Before you begin \u00b6 To create a PingSource, you must install Knative Eventing . The PingSource event source type is enabled by default when you install Knative Eventing. Optional: You can use either kubectl or kn commands to create components such as a sink and PingSource. Optional: You can use either kubectl or kail for logging during the verification step in this procedure. Procedure \u00b6 Optional: Create a new namespace called pingsource-example by entering the following command: kubectl create namespace pingsource-example Creating a namespace for the PingSource example allows you to isolate the components created by this demo, so that it is easier for you to view changes and remove components when you are finished. To verify that the PingSource is working correctly, create an example sink in the pingsource-example namespace that dumps incoming messages to a log, by entering the command: kubectl kubectl -n pingsource-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: event-display spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: event-display spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Create a PingSource that sends an event containing {\"message\": \"Hello world!\"} every minute, by entering the command: YAML kubectl create -n pingsource-example -f - <<EOF apiVersion: sources.knative.dev/v1 kind: PingSource metadata: name: test-ping-source spec: schedule: \"*/1 * * * *\" contentType: \"application/json\" data: '{\"message\": \"Hello world!\"}' sink: ref: apiVersion: v1 kind: Service name: event-display EOF kn kn source ping create test-ping-source \\ --namespace pingsource-example \\ --schedule \"*/1 * * * *\" \\ --data '{\"message\": \"Hello world!\"}' \\ --sink http://event-display.pingsource-example.svc.cluster.local Optional: Create a PingSource that sends binary data. If you want to send binary data in an event, this cannot be directly serialized in YAML. However, you can use dataBase64 in place of data in the PingSource spec to carry a data payload that is base64 encoded. To create a PingSource that uses base64 encoded data, enter the command: kubectl -n pingsource-example apply -f - <<EOF apiVersion: sources.knative.dev/v1 kind: PingSource metadata: name: test-ping-source-binary spec: schedule: \"*/1 * * * *\" contentType: \"text/plain\" dataBase64: \"ZGF0YQ==\" sink: ref: apiVersion: v1 kind: Service name: event-display EOF View the logs for the event-display event consumer by entering the following command: kubectl kubectl -n pingsource-example logs -l app = event-display --tail = 100 kail kail -l serving.knative.dev/service = event-display -c user-container --since = 10m This returns the Attributes and Data of the events that the PingSource sent to the event-display service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8 time: 2021 -03-25T19:41:00.444508332Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" } If you created a PingSource that sends binary data, you will also see output similar to the following: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source-binary id: ddd7bad2-9b6a-42a7-8f9b-b64494a6ce43 time: 2021 -03-25T19:38:00.455013472Z datacontenttype: text/plain Data, data Optional: You can delete the pingsource-example namespace and all related resources from your cluster by entering the following command: kubectl delete namespace pingsource-example Optional: You can also delete the PingSource instance only by entering the following command: kubectl kubectl delete pingsources.sources.knative.dev test-ping-source kn kn source ping delete test-ping-source kubectl: binary data PingSource kubectl delete pingsources.sources.knative.dev test-ping-source-binary kn: binary data PingSource kn source ping delete test-ping-source-binary Optional: Delete the event-display service: kubectl kubectl delete service.serving.knative.dev event-display kn kn service delete event-display","title":"PingSource"},{"location":"eventing/sources/ping-source/#pingsource","text":"A PingSource is an event source that produces events with a fixed payload on a specified cron schedule. The following example shows how you can configure a PingSource as an event source that sends events every minute to a Knative service named event-display that is used as a sink.","title":"PingSource"},{"location":"eventing/sources/ping-source/#before-you-begin","text":"To create a PingSource, you must install Knative Eventing . The PingSource event source type is enabled by default when you install Knative Eventing. Optional: You can use either kubectl or kn commands to create components such as a sink and PingSource. Optional: You can use either kubectl or kail for logging during the verification step in this procedure.","title":"Before you begin"},{"location":"eventing/sources/ping-source/#procedure","text":"Optional: Create a new namespace called pingsource-example by entering the following command: kubectl create namespace pingsource-example Creating a namespace for the PingSource example allows you to isolate the components created by this demo, so that it is easier for you to view changes and remove components when you are finished. To verify that the PingSource is working correctly, create an example sink in the pingsource-example namespace that dumps incoming messages to a log, by entering the command: kubectl kubectl -n pingsource-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: event-display spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: event-display spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Create a PingSource that sends an event containing {\"message\": \"Hello world!\"} every minute, by entering the command: YAML kubectl create -n pingsource-example -f - <<EOF apiVersion: sources.knative.dev/v1 kind: PingSource metadata: name: test-ping-source spec: schedule: \"*/1 * * * *\" contentType: \"application/json\" data: '{\"message\": \"Hello world!\"}' sink: ref: apiVersion: v1 kind: Service name: event-display EOF kn kn source ping create test-ping-source \\ --namespace pingsource-example \\ --schedule \"*/1 * * * *\" \\ --data '{\"message\": \"Hello world!\"}' \\ --sink http://event-display.pingsource-example.svc.cluster.local Optional: Create a PingSource that sends binary data. If you want to send binary data in an event, this cannot be directly serialized in YAML. However, you can use dataBase64 in place of data in the PingSource spec to carry a data payload that is base64 encoded. To create a PingSource that uses base64 encoded data, enter the command: kubectl -n pingsource-example apply -f - <<EOF apiVersion: sources.knative.dev/v1 kind: PingSource metadata: name: test-ping-source-binary spec: schedule: \"*/1 * * * *\" contentType: \"text/plain\" dataBase64: \"ZGF0YQ==\" sink: ref: apiVersion: v1 kind: Service name: event-display EOF View the logs for the event-display event consumer by entering the following command: kubectl kubectl -n pingsource-example logs -l app = event-display --tail = 100 kail kail -l serving.knative.dev/service = event-display -c user-container --since = 10m This returns the Attributes and Data of the events that the PingSource sent to the event-display service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8 time: 2021 -03-25T19:41:00.444508332Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" } If you created a PingSource that sends binary data, you will also see output similar to the following: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source-binary id: ddd7bad2-9b6a-42a7-8f9b-b64494a6ce43 time: 2021 -03-25T19:38:00.455013472Z datacontenttype: text/plain Data, data Optional: You can delete the pingsource-example namespace and all related resources from your cluster by entering the following command: kubectl delete namespace pingsource-example Optional: You can also delete the PingSource instance only by entering the following command: kubectl kubectl delete pingsources.sources.knative.dev test-ping-source kn kn source ping delete test-ping-source kubectl: binary data PingSource kubectl delete pingsources.sources.knative.dev test-ping-source-binary kn: binary data PingSource kn source ping delete test-ping-source-binary Optional: Delete the event-display service: kubectl kubectl delete service.serving.knative.dev event-display kn kn service delete event-display","title":"Procedure"},{"location":"eventing/sugar/","text":"Knative Eventing Sugar Controller \u00b6 Knative Eventing Sugar Controller will react to special labels and annotations to produce or control eventing resources in a cluster or namespace. This allows cluster operators and developers to focus on creating fewer resources, and the underlying eventing infrastructure is created on-demand, and cleaned up when no longer needed. Installing \u00b6 The following command installs the Eventing Sugar Controller: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml Automatic Broker Creation \u00b6 One way to create a Broker is to manually apply a resource to a cluster using the default settings: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default namespace: default EOF There might be cases where automated Broker creation is desirable, such as on namespace creation, or on Trigger creation. The Sugar controller enables those use-cases: When a Namespace is labeled with eventing.knative.dev/injection=enabled , the sugar controller will create a default Broker named \"default\" in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller will create a Broker named by that Trigger in the Trigger's Namespace. When a Broker is deleted and the above labels or annotations are in-use, the Sugar Controller will automatically recreate a default Broker. Namespace Examples \u00b6 Creating a \"default\" Broker when creating a Namespace: kubectl apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: example labels: eventing.knative.dev/injection: enabled EOF To automatically create a Broker after a namespace exists, label the Namespace: kubectl label namespace default eventing.knative.dev/injection = enabled If the Broker named \"default\" already exists in the Namespace, the Sugar Controller will do nothing. Trigger Examples \u00b6 Create a Broker named by a Trigger ( spec.broker ) in the Trigger's Namespace: kubectl apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-sugar namespace: hello annotations: eventing.knative.dev/injection: enabled spec: broker: sugar subscriber: ref: apiVersion: v1 kind: Service name: event-display EOF Note : If the named Broker already exists, the Sugar controller will do nothing, and the Trigger will not own the existing Broker. This will make a Broker called \"sugar\" in the Namespace \"hello\", and attempt to send events to the \"event-display\" service. If the Broker of the given name already exists in the Namespace, the Sugar Controller will do nothing.","title":"Sugar Controller"},{"location":"eventing/sugar/#knative-eventing-sugar-controller","text":"Knative Eventing Sugar Controller will react to special labels and annotations to produce or control eventing resources in a cluster or namespace. This allows cluster operators and developers to focus on creating fewer resources, and the underlying eventing infrastructure is created on-demand, and cleaned up when no longer needed.","title":"Knative Eventing Sugar Controller"},{"location":"eventing/sugar/#installing","text":"The following command installs the Eventing Sugar Controller: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml","title":"Installing"},{"location":"eventing/sugar/#automatic-broker-creation","text":"One way to create a Broker is to manually apply a resource to a cluster using the default settings: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default namespace: default EOF There might be cases where automated Broker creation is desirable, such as on namespace creation, or on Trigger creation. The Sugar controller enables those use-cases: When a Namespace is labeled with eventing.knative.dev/injection=enabled , the sugar controller will create a default Broker named \"default\" in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller will create a Broker named by that Trigger in the Trigger's Namespace. When a Broker is deleted and the above labels or annotations are in-use, the Sugar Controller will automatically recreate a default Broker.","title":"Automatic Broker Creation"},{"location":"eventing/sugar/#namespace-examples","text":"Creating a \"default\" Broker when creating a Namespace: kubectl apply -f - <<EOF apiVersion: v1 kind: Namespace metadata: name: example labels: eventing.knative.dev/injection: enabled EOF To automatically create a Broker after a namespace exists, label the Namespace: kubectl label namespace default eventing.knative.dev/injection = enabled If the Broker named \"default\" already exists in the Namespace, the Sugar Controller will do nothing.","title":"Namespace Examples"},{"location":"eventing/sugar/#trigger-examples","text":"Create a Broker named by a Trigger ( spec.broker ) in the Trigger's Namespace: kubectl apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-sugar namespace: hello annotations: eventing.knative.dev/injection: enabled spec: broker: sugar subscriber: ref: apiVersion: v1 kind: Service name: event-display EOF Note : If the named Broker already exists, the Sugar controller will do nothing, and the Trigger will not own the existing Broker. This will make a Broker called \"sugar\" in the Namespace \"hello\", and attempt to send events to the \"event-display\" service. If the Broker of the given name already exists in the Namespace, the Sugar Controller will do nothing.","title":"Trigger Examples"},{"location":"getting-started/csantana-getting-started-eventing/","text":"Getting Started with Knative Eventing \u00b6 After you install Knative Eventing, you can create, send, and verify events. This guide shows how you can use a basic workflow for managing events. Before you start to manage events, you must create the objects needed to transport the events. Creating a Knative Eventing namespace \u00b6 Namespaces are used to group together and organize your Knative resources. Create a new namespace called event-example by entering the following command: kubectl create namespace event-example Adding a broker to the namespace \u00b6 The broker allows you to route events to different event sinks or consumers. Add a broker named default to your namespace by entering the following command: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: broker metadata: name: default namespace: event-example EOF Verify that the broker is working correctly, by entering the following command: kubectl -n event-example get broker default This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If READY is False , wait a few moments and then run the command again. If you continue to receive the False status, see the Debugging Guide to troubleshoot the issue. Creating event consumers \u00b6 In this step, you create two event consumers, hello-display and goodbye-display , to demonstrate how you can configure your event producers to target a specific consumer. To deploy the hello-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: hello-display spec: replicas: 1 selector: matchLabels: &labels app: hello-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: hello-display spec: selector: app: hello-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF To deploy the goodbye-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: goodbye-display spec: replicas: 1 selector: matchLabels: &labels app: goodbye-display template: metadata: labels: *labels spec: containers: - name: event-display # Source code: https://github.com/knative/eventing-contrib/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: goodbye-display spec: selector: app: goodbye-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Verify that the event consumers are working by entering the following command: kubectl -n event-example get deployments hello-display goodbye-display This lists the hello-display and goodbye-display consumers that you deployed: NAME READY UP-TO-DATE AVAILABLE AGE hello-display 1/1 1 1 26s goodbye-display 1/1 1 1 16s The number of replicas in the READY column should match the number of replicas in the AVAILABLE column. If the numbers do not match, see the Debugging Guide to troubleshoot the issue. Creating triggers \u00b6 A trigger defines the events that each event consumer receives. Brokers use triggers to forward events to the correct consumers. Each trigger can specify a filter that enables selection of relevant events based on the Cloud Event context attributes. Create a trigger by entering the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-display spec: broker: default filter: attributes: type: greeting subscriber: ref: apiVersion: v1 kind: Service name: hello-display EOF The command creates a trigger that sends all events of type greeting to your event consumer named hello-display . To add a second trigger, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: goodbye-display spec: broker: default filter: attributes: source: sendoff subscriber: ref: apiVersion: v1 kind: Service name: goodbye-display EOF The command creates a trigger that sends all events of source sendoff to your event consumer named goodbye-display . Verify that the triggers are working correctly by running the following command: kubectl -n event-example get triggers This returns the hello-display and goodbye-display triggers that you created: NAME READY REASON BROKER SUBSCRIBER_URI AGE goodbye-display True default http://goodbye-display.event-example.svc.cluster.local/ 9s hello-display True default http://hello-display.event-example.svc.cluster.local/ 16s If the triggers are correctly configured, they will be ready and pointing to the correct broker ( default ) and SUBSCRIBER_URI . The SUBSCRIBER_URI has a value similar to triggerName.namespaceName.svc.cluster.local . The exact value depends on the broker implementation. If this value looks incorrect, see the Debugging Guide to troubleshoot the issue. Creating a pod as an event producer \u00b6 This guide uses curl commands to manually send individual events as HTTP requests to the broker, and demonstrate how these events are received by the correct event consumer. The broker can only be accessed from within the cluster where Knative Eventing is installed. You must create a pod within that cluster to act as an event producer that will execute the curl commands. To create a pod, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: v1 kind: Pod metadata: labels: run: curl name: curl spec: containers: # This could be any image that we can SSH into and has curl. - image: radial/busyboxplus:curl imagePullPolicy: IfNotPresent name: curl resources: {} stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true EOF Sending events to the broker \u00b6 SSH into the pod by running the following command: kubectl -n event-example attach curl -it You will see a prompt similar to the following: Defaulting container name to curl. Use 'kubectl describe pod/ -n event-example' to see all of the containers in this pod. If you don't see a command prompt, try pressing enter. [ root@curl:/ ]$ Make a HTTP request to the broker. To show the various types of events you can send, you will make three requests: To make the first request, which creates an event that has the type greeting , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: not-sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative!\"}' When the broker receives your event, hello-display will activate and send it to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the second request, which creates an event that has the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: not-greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Goodbye Knative!\"}' When the broker receives your event, goodbye-display will activate and send the event to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the third request, which creates an event that has the type greeting and the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative! Goodbye Knative!\"}' When the broker receives your event, hello-display and goodbye-display will activate and send the event to the event consumers of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT Exit SSH by typing exit into the command prompt. You have sent two events to the hello-display event consumer and two events to the goodbye-display event consumer (note that say-hello-goodbye activates the trigger conditions for both hello-display and goodbye-display ). You will verify that these events were received correctly in the next section. Verifying that events were received \u00b6 After you send the events, verify that the events were received by the correct subscribers. Look at the logs for the hello-display event consumer by entering the following command: kubectl -n event-example logs -l app=hello-display --tail=100 This returns the Attributes and Data of the events you sent to hello-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: not-sendoff id: say-hello time: 2019-05-20T17:59:43.81718488Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" } Look at the logs for the goodbye-display event consumer by entering the following command: kubectl -n event-example logs -l app=goodbye-display --tail=100 This returns the Attributes and Data of the events you sent to goodbye-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: not-greeting source: sendoff id: say-goodbye time: 2019-05-20T17:59:49.044926148Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Goodbye Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" } Cleaning up example resources \u00b6 You can delete the event-example namespace and its associated resources from your cluster if you do not plan to use it again in the future. Delete the event-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace event-example","title":"Getting Started with Knative Eventing"},{"location":"getting-started/csantana-getting-started-eventing/#getting-started-with-knative-eventing","text":"After you install Knative Eventing, you can create, send, and verify events. This guide shows how you can use a basic workflow for managing events. Before you start to manage events, you must create the objects needed to transport the events.","title":"Getting Started with Knative Eventing"},{"location":"getting-started/csantana-getting-started-eventing/#creating-a-knative-eventing-namespace","text":"Namespaces are used to group together and organize your Knative resources. Create a new namespace called event-example by entering the following command: kubectl create namespace event-example","title":"Creating a Knative Eventing namespace"},{"location":"getting-started/csantana-getting-started-eventing/#adding-a-broker-to-the-namespace","text":"The broker allows you to route events to different event sinks or consumers. Add a broker named default to your namespace by entering the following command: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: broker metadata: name: default namespace: event-example EOF Verify that the broker is working correctly, by entering the following command: kubectl -n event-example get broker default This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If READY is False , wait a few moments and then run the command again. If you continue to receive the False status, see the Debugging Guide to troubleshoot the issue.","title":"Adding a broker to the namespace"},{"location":"getting-started/csantana-getting-started-eventing/#creating-event-consumers","text":"In this step, you create two event consumers, hello-display and goodbye-display , to demonstrate how you can configure your event producers to target a specific consumer. To deploy the hello-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: hello-display spec: replicas: 1 selector: matchLabels: &labels app: hello-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: hello-display spec: selector: app: hello-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF To deploy the goodbye-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: goodbye-display spec: replicas: 1 selector: matchLabels: &labels app: goodbye-display template: metadata: labels: *labels spec: containers: - name: event-display # Source code: https://github.com/knative/eventing-contrib/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: goodbye-display spec: selector: app: goodbye-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Verify that the event consumers are working by entering the following command: kubectl -n event-example get deployments hello-display goodbye-display This lists the hello-display and goodbye-display consumers that you deployed: NAME READY UP-TO-DATE AVAILABLE AGE hello-display 1/1 1 1 26s goodbye-display 1/1 1 1 16s The number of replicas in the READY column should match the number of replicas in the AVAILABLE column. If the numbers do not match, see the Debugging Guide to troubleshoot the issue.","title":"Creating event consumers"},{"location":"getting-started/csantana-getting-started-eventing/#creating-triggers","text":"A trigger defines the events that each event consumer receives. Brokers use triggers to forward events to the correct consumers. Each trigger can specify a filter that enables selection of relevant events based on the Cloud Event context attributes. Create a trigger by entering the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-display spec: broker: default filter: attributes: type: greeting subscriber: ref: apiVersion: v1 kind: Service name: hello-display EOF The command creates a trigger that sends all events of type greeting to your event consumer named hello-display . To add a second trigger, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: goodbye-display spec: broker: default filter: attributes: source: sendoff subscriber: ref: apiVersion: v1 kind: Service name: goodbye-display EOF The command creates a trigger that sends all events of source sendoff to your event consumer named goodbye-display . Verify that the triggers are working correctly by running the following command: kubectl -n event-example get triggers This returns the hello-display and goodbye-display triggers that you created: NAME READY REASON BROKER SUBSCRIBER_URI AGE goodbye-display True default http://goodbye-display.event-example.svc.cluster.local/ 9s hello-display True default http://hello-display.event-example.svc.cluster.local/ 16s If the triggers are correctly configured, they will be ready and pointing to the correct broker ( default ) and SUBSCRIBER_URI . The SUBSCRIBER_URI has a value similar to triggerName.namespaceName.svc.cluster.local . The exact value depends on the broker implementation. If this value looks incorrect, see the Debugging Guide to troubleshoot the issue.","title":"Creating triggers"},{"location":"getting-started/csantana-getting-started-eventing/#creating-a-pod-as-an-event-producer","text":"This guide uses curl commands to manually send individual events as HTTP requests to the broker, and demonstrate how these events are received by the correct event consumer. The broker can only be accessed from within the cluster where Knative Eventing is installed. You must create a pod within that cluster to act as an event producer that will execute the curl commands. To create a pod, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: v1 kind: Pod metadata: labels: run: curl name: curl spec: containers: # This could be any image that we can SSH into and has curl. - image: radial/busyboxplus:curl imagePullPolicy: IfNotPresent name: curl resources: {} stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true EOF","title":"Creating a pod as an event producer"},{"location":"getting-started/csantana-getting-started-eventing/#sending-events-to-the-broker","text":"SSH into the pod by running the following command: kubectl -n event-example attach curl -it You will see a prompt similar to the following: Defaulting container name to curl. Use 'kubectl describe pod/ -n event-example' to see all of the containers in this pod. If you don't see a command prompt, try pressing enter. [ root@curl:/ ]$ Make a HTTP request to the broker. To show the various types of events you can send, you will make three requests: To make the first request, which creates an event that has the type greeting , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: not-sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative!\"}' When the broker receives your event, hello-display will activate and send it to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the second request, which creates an event that has the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: not-greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Goodbye Knative!\"}' When the broker receives your event, goodbye-display will activate and send the event to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the third request, which creates an event that has the type greeting and the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative! Goodbye Knative!\"}' When the broker receives your event, hello-display and goodbye-display will activate and send the event to the event consumers of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT Exit SSH by typing exit into the command prompt. You have sent two events to the hello-display event consumer and two events to the goodbye-display event consumer (note that say-hello-goodbye activates the trigger conditions for both hello-display and goodbye-display ). You will verify that these events were received correctly in the next section.","title":"Sending events to the broker"},{"location":"getting-started/csantana-getting-started-eventing/#verifying-that-events-were-received","text":"After you send the events, verify that the events were received by the correct subscribers. Look at the logs for the hello-display event consumer by entering the following command: kubectl -n event-example logs -l app=hello-display --tail=100 This returns the Attributes and Data of the events you sent to hello-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: not-sendoff id: say-hello time: 2019-05-20T17:59:43.81718488Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" } Look at the logs for the goodbye-display event consumer by entering the following command: kubectl -n event-example logs -l app=goodbye-display --tail=100 This returns the Attributes and Data of the events you sent to goodbye-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: not-greeting source: sendoff id: say-goodbye time: 2019-05-20T17:59:49.044926148Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Goodbye Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" }","title":"Verifying that events were received"},{"location":"getting-started/csantana-getting-started-eventing/#cleaning-up-example-resources","text":"You can delete the event-example namespace and its associated resources from your cluster if you do not plan to use it again in the future. Delete the event-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace event-example","title":"Cleaning up example resources"},{"location":"getting-started/first-autoscale/","text":"Scaling to Zero \u00b6 Remember those super powers we talked about? One of Knative Serving's powers is automatic scaling or simply \"autoscaling\" out-of-the-box! This means your Knative Service will only \"spin up\" your application to perform its job (in this case, saying \"Hello world!\") if it is needed; otherwise, it will \"scale to zero\" by spinning down and waiting for a new request to come in. What about scaling up to meet increased demand? Knative Autoscaling also allows you to easily configure your service to scale up (horizontal autoscaling) to meet increased demand as well as control the number of instances that spin up using \"concurrency limits and other options,\" but that's beyond the scope of this tutorial. Let's see this in action! We're going to peek under the hood at the Pod in Kubernetes where our Knative Service is running to watch our \"Hello world!\" Service scale up and down. Run your Knative Service \u00b6 Let's run our \"Hello world!\" Service just one more time. This time, try the Knative Service URL in your browser http://hello.default.127.0.0.1.nip.io , or you can use your terminal with curl . curl http://hello.default.127.0.0.1.nip.io You can watch the pods and see how they scale to zero after traffic stops going to the URL. kubectl get pod -l serving.knative.dev/service = hello -w Expected output: NAME READY STATUS hello-world 2 /2 Running hello-world 2 /2 Terminating hello-world 1 /2 Terminating hello-world 0 /2 Terminating Scale up your Knative Service \u00b6 Try to rerun the Knative Service in your browser http://hello.default.127.0.0.1.nip.io , and you will see a new pod running again. Expected output: NAME READY STATUS hello-world 0 /2 Pending hello-world 0 /2 ContainerCreating hello-world 1 /2 Running hello-world 2 /2 Running Exit the watch command with Ctrl+c . Some people call this Serverless Want to go deeper on Autoscaling? Interested in getting in the weeds with Knative Autoscaling? Check out the autoscaling page for concepts, samples, and more!","title":"Scaling to Zero"},{"location":"getting-started/first-autoscale/#scaling-to-zero","text":"Remember those super powers we talked about? One of Knative Serving's powers is automatic scaling or simply \"autoscaling\" out-of-the-box! This means your Knative Service will only \"spin up\" your application to perform its job (in this case, saying \"Hello world!\") if it is needed; otherwise, it will \"scale to zero\" by spinning down and waiting for a new request to come in. What about scaling up to meet increased demand? Knative Autoscaling also allows you to easily configure your service to scale up (horizontal autoscaling) to meet increased demand as well as control the number of instances that spin up using \"concurrency limits and other options,\" but that's beyond the scope of this tutorial. Let's see this in action! We're going to peek under the hood at the Pod in Kubernetes where our Knative Service is running to watch our \"Hello world!\" Service scale up and down.","title":"Scaling to Zero"},{"location":"getting-started/first-autoscale/#run-your-knative-service","text":"Let's run our \"Hello world!\" Service just one more time. This time, try the Knative Service URL in your browser http://hello.default.127.0.0.1.nip.io , or you can use your terminal with curl . curl http://hello.default.127.0.0.1.nip.io You can watch the pods and see how they scale to zero after traffic stops going to the URL. kubectl get pod -l serving.knative.dev/service = hello -w Expected output: NAME READY STATUS hello-world 2 /2 Running hello-world 2 /2 Terminating hello-world 1 /2 Terminating hello-world 0 /2 Terminating","title":"Run your Knative Service"},{"location":"getting-started/first-autoscale/#scale-up-your-knative-service","text":"Try to rerun the Knative Service in your browser http://hello.default.127.0.0.1.nip.io , and you will see a new pod running again. Expected output: NAME READY STATUS hello-world 0 /2 Pending hello-world 0 /2 ContainerCreating hello-world 1 /2 Running hello-world 2 /2 Running Exit the watch command with Ctrl+c . Some people call this Serverless Want to go deeper on Autoscaling? Interested in getting in the weeds with Knative Autoscaling? Check out the autoscaling page for concepts, samples, and more!","title":"Scale up your Knative Service"},{"location":"getting-started/first-broker/","text":"Sources, Brokers, Triggers, Sinks, oh my! \u00b6 For the purposes of this tutorial, let's keep it simple. You will focus on four powerful Eventing components: Source , Trigger , Broker , and Sink . Let's take a look at how these components interact: Source 1 and Source 2 are transmitting some data (1's and 2's) to the Broker, which then gets filtered by Triggers to the desired Sink. Component Basic Definition Source A Kubernetes Custom Resource which emits events to the Broker. Broker A \"hub\" for events in your infrastructure; a central location to send events for delivery. Trigger Acts as a filter for events entering the broker, can be configured with desired event attributes. Sink A destination for events. A note on Sources and Sinks A Knative Service can act as both a Source and a Sink for events, and for good reason. You may want to consume events from the Broker and send modified events back to the Broker , as you would in any pipeline use-case. CloudEvents \u00b6 Knative Eventing uses CloudEvents send information back and forth between your Services and these components. What are CloudEvents? For our purposes, the only thing you need to know about CloudEvents are: CloudEvents follow the CloudEvents 1.0 Specification , with required and optional attributes. CloudEvents can be \"emitted\" by almost anything and can be transported to anywhere in your deployment. CloudEvents can carry some attributes (things like id , source , type , etc) as well as data payloads (JSON, plaintext, reference to data that lives elsewhere, etc). To find out more about CloudEvents, check out the CloudEvents website ! Examining the Broker \u00b6 As part of the KonK install, you should have an in-memory Broker already installed. kn broker list Expected Output NAME URL AGE CONDITIONS READY REASON example-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/default 5m 5 OK / 5 True Warning In-Memory Brokers are for development use only and must not be used in a production deployment. What other components exist in Knative Eventing? If you want to find out more about the different components of Knative Eventing, like Channels, Sequences, Parallels, etc. check out \"Eventing Components.\"","title":"Sources, Brokers, Triggers, Sinks"},{"location":"getting-started/first-broker/#sources-brokers-triggers-sinks-oh-my","text":"For the purposes of this tutorial, let's keep it simple. You will focus on four powerful Eventing components: Source , Trigger , Broker , and Sink . Let's take a look at how these components interact: Source 1 and Source 2 are transmitting some data (1's and 2's) to the Broker, which then gets filtered by Triggers to the desired Sink. Component Basic Definition Source A Kubernetes Custom Resource which emits events to the Broker. Broker A \"hub\" for events in your infrastructure; a central location to send events for delivery. Trigger Acts as a filter for events entering the broker, can be configured with desired event attributes. Sink A destination for events. A note on Sources and Sinks A Knative Service can act as both a Source and a Sink for events, and for good reason. You may want to consume events from the Broker and send modified events back to the Broker , as you would in any pipeline use-case.","title":"Sources, Brokers, Triggers, Sinks, oh my!"},{"location":"getting-started/first-broker/#cloudevents","text":"Knative Eventing uses CloudEvents send information back and forth between your Services and these components. What are CloudEvents? For our purposes, the only thing you need to know about CloudEvents are: CloudEvents follow the CloudEvents 1.0 Specification , with required and optional attributes. CloudEvents can be \"emitted\" by almost anything and can be transported to anywhere in your deployment. CloudEvents can carry some attributes (things like id , source , type , etc) as well as data payloads (JSON, plaintext, reference to data that lives elsewhere, etc). To find out more about CloudEvents, check out the CloudEvents website !","title":"CloudEvents"},{"location":"getting-started/first-broker/#examining-the-broker","text":"As part of the KonK install, you should have an in-memory Broker already installed. kn broker list Expected Output NAME URL AGE CONDITIONS READY REASON example-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/default 5m 5 OK / 5 True Warning In-Memory Brokers are for development use only and must not be used in a production deployment. What other components exist in Knative Eventing? If you want to find out more about the different components of Knative Eventing, like Channels, Sequences, Parallels, etc. check out \"Eventing Components.\"","title":"Examining the Broker"},{"location":"getting-started/first-service/","text":"Deploying your first Knative Service \u00b6 Tip Hit n / . on your keyboard to move forward in the tutorial. Use p / , to go back at any time. In this tutorial, we are going to deploy a \"Hello world\" Service! This service will accept an environment variable, TARGET , and print \" Hello ${TARGET}! .\" For those of you familiar with other source-to-url tools, this may seem familiar. However, since our \"Hello world\" Service is being deployed as a Knative Service, it gets some super powers (scale-to-zero, traffic-splitting) out of the box . Knative Service: \"Hello world!\" \u00b6 kn kn service create hello \\ --image gcr.io/knative-samples/helloworld-go \\ --port 8080 \\ --env TARGET = World \\ --revision-name = world YAML apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: metadata: name: world spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"World\" Once you've created your YAML file (named something like \"hello.yaml\"): kubectl apply -f hello.yaml After Knative has successfully created your service, you should see the following: Service hello created to latest revision 'hello-world' is available at URL: http://hello.default.127.0.0.1.nip.io Why did I pass in revision-name ? Note that the name \"world\" which you passed in as \"revision-name\" is being used to create the Revision 's name ( latest revision \"hello-world\"... ). This will help you to more easily identify this particular Revision , but don't worry, we'll talk more about Revisions later. Run your Knative Service \u00b6 curl http://hello.default.127.0.0.1.nip.io Expected output: Hello World! Congratulations , you've just created your first Knative Service!","title":"First Knative Service"},{"location":"getting-started/first-service/#deploying-your-first-knative-service","text":"Tip Hit n / . on your keyboard to move forward in the tutorial. Use p / , to go back at any time. In this tutorial, we are going to deploy a \"Hello world\" Service! This service will accept an environment variable, TARGET , and print \" Hello ${TARGET}! .\" For those of you familiar with other source-to-url tools, this may seem familiar. However, since our \"Hello world\" Service is being deployed as a Knative Service, it gets some super powers (scale-to-zero, traffic-splitting) out of the box .","title":"Deploying your first Knative Service"},{"location":"getting-started/first-service/#knative-service-hello-world","text":"kn kn service create hello \\ --image gcr.io/knative-samples/helloworld-go \\ --port 8080 \\ --env TARGET = World \\ --revision-name = world YAML apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: metadata: name: world spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"World\" Once you've created your YAML file (named something like \"hello.yaml\"): kubectl apply -f hello.yaml After Knative has successfully created your service, you should see the following: Service hello created to latest revision 'hello-world' is available at URL: http://hello.default.127.0.0.1.nip.io Why did I pass in revision-name ? Note that the name \"world\" which you passed in as \"revision-name\" is being used to create the Revision 's name ( latest revision \"hello-world\"... ). This will help you to more easily identify this particular Revision , but don't worry, we'll talk more about Revisions later.","title":"Knative Service: \"Hello world!\""},{"location":"getting-started/first-service/#run-your-knative-service","text":"curl http://hello.default.127.0.0.1.nip.io Expected output: Hello World! Congratulations , you've just created your first Knative Service!","title":"Run your Knative Service"},{"location":"getting-started/first-source/","text":"In this tutorial, you will be using the CloudEvents Player to showcase the core concepts of Knative Eventing. By the end of this tutorial, you should have something that looks like this: Figure 6.6 from Knative in Action Here, the CloudEvents Player is acting as both a Source and a Sink for CloudEvents. Creating your first Source \u00b6 The CloudEvents Player acts as a Source for CloudEvents by intaking the URL of our Broker as an environment variable ( BROKER_URL ) and sending CloudEvents via the UI. Create the CloudEvents Player Service: kn kn service create cloudevents-player \\ --image ruromero/cloudevents-player:latest \\ --env BROKER_URL = http://broker-ingress.knative-eventing.svc.cluster.local/default/default YAML //TODO Expected Output Service 'cloudevents-player' created to latest revision 'cloudevents-player-vwybw-1' is available at URL: http://cloudevents-player.default.127.0.0.1.nip.io Wait, my Revision is named something different! Since we didn't assign a revision-name , Knative Serving automatically created one for us, it's ok if your Revision is named something different. Examining the CloudEvents Player \u00b6 We can use the CloudEvents Player to send and receive CloudEvents. If you open the Service URL in your browser, you should be greeted by a form titled \"Create event.\" The user interface for the CloudEvents Player What do these fields mean? Field Description Event ID A unique ID. Click the loop icon to generate a new one. Event Type An event type. Event Source An event source. Specversion Demarcates which CloudEvents spec you're using (should always be 1.0). Message The data section of the CloudEvent, a payload which is carrying the data you care to be delivered. For more information on the CloudEvents Specification, check out the CloudEvents Spec . Fill out the form with whatever you data you would like to and hit the \"SEND EVENT\" button. Make sure your Event Source does not contain any spaces! Tip: Clicking the will show you the CloudEvent as the Broker sees it. The icon in the \"Status\" column implies that the event has been sent to our Broker , but where has the event gone? Well, right now, nowhere! A Broker is simply a receptacle for events. In order for your events to be sent somewhere, you must create a Trigger which listens for your events and places them somewhere. Let's do that next!","title":"Introducing the CloudEvents Player"},{"location":"getting-started/first-source/#creating-your-first-source","text":"The CloudEvents Player acts as a Source for CloudEvents by intaking the URL of our Broker as an environment variable ( BROKER_URL ) and sending CloudEvents via the UI. Create the CloudEvents Player Service: kn kn service create cloudevents-player \\ --image ruromero/cloudevents-player:latest \\ --env BROKER_URL = http://broker-ingress.knative-eventing.svc.cluster.local/default/default YAML //TODO Expected Output Service 'cloudevents-player' created to latest revision 'cloudevents-player-vwybw-1' is available at URL: http://cloudevents-player.default.127.0.0.1.nip.io Wait, my Revision is named something different! Since we didn't assign a revision-name , Knative Serving automatically created one for us, it's ok if your Revision is named something different.","title":"Creating your first Source"},{"location":"getting-started/first-source/#examining-the-cloudevents-player","text":"We can use the CloudEvents Player to send and receive CloudEvents. If you open the Service URL in your browser, you should be greeted by a form titled \"Create event.\" The user interface for the CloudEvents Player What do these fields mean? Field Description Event ID A unique ID. Click the loop icon to generate a new one. Event Type An event type. Event Source An event source. Specversion Demarcates which CloudEvents spec you're using (should always be 1.0). Message The data section of the CloudEvent, a payload which is carrying the data you care to be delivered. For more information on the CloudEvents Specification, check out the CloudEvents Spec . Fill out the form with whatever you data you would like to and hit the \"SEND EVENT\" button. Make sure your Event Source does not contain any spaces! Tip: Clicking the will show you the CloudEvent as the Broker sees it. The icon in the \"Status\" column implies that the event has been sent to our Broker , but where has the event gone? Well, right now, nowhere! A Broker is simply a receptacle for events. In order for your events to be sent somewhere, you must create a Trigger which listens for your events and places them somewhere. Let's do that next!","title":"Examining the CloudEvents Player"},{"location":"getting-started/first-traffic-split/","text":"Basics of Traffic Splitting \u00b6 The last super power of Knative Serving we'll go over in this tutorial is traffic splitting. What are some common traffic splitting use-cases? Splitting traffic is useful for a number of very common modern infrastructure needs, such as blue/green deployments and canary deployments . Bringing these industry standards to bear on Kubernetes is as simple as a single CLI command on Knative or YAML tweak, let's see how! Creating a new Revision \u00b6 You may have noticed that when you created your Knative Service you assigned it a revision-name , \"world\". When your Service was created, Knative returned both a URL and a 'latest revision' name for your Knative Service. But what happens if you make a change to your Service? What exactly is a Revision ?\" You can think of a Revision as a stateless, autoscaling snapshot-in-time of application code and configuration. A new Revision will get created each and every time you make changes to your Knative Service, whether you assign it a name or not. When splitting traffic, Knative splits traffic between different Revisions of your Knative Service. Instead of TARGET =\"World,\" let's update the environment variable TARGET on our Knative Service hello to greet \"Knative\". Lets name this new revision hello-knative kn kn service update hello \\ --env TARGET = Knative \\ --revision-name = knative YAML apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: metadata: name: knative spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"Knative\" Once you've edited your existing YAML file: kubectl apply -f hello.yaml As before, Knative prints. out some helpful information to the CLI. Expected output: Service hello created to latest revision 'hello-knative' is available at URL: http://hello.default.127.0.0.1.nip.io Note, since we are updating an existing Knative Service hello , the URL doesn't change, but our new Revision should have the new name \"hello-knative\" Let's access our Knative Service again on the browser http://hello.default.127.0.0.1.nip.io to see the change, or use curl in your terminal: curl http://hello.default.127.0.0.1.nip.io Expected output: Hello Knative! Splitting Traffic \u00b6 You may at this point be wondering, \"where did 'Hello World!' go?\" Revisions are a stateless snapshot-in-time of application code and configuration so your \"hello-world\" Revision is still available to you. We can easily see a list of our existing revisions with the kn CLI: kn kn revisions list kubectl Though the following example doesn't cover it, you can peak under the hood to Kubernetes to see the revisions as Kubernetes sees them. kubectl get revisions Expected output: NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-knative hello 100 % 2 30s 3 OK / 4 True hello-world hello 1 5m 3 OK / 4 True The column most relevant for our purposes is \"TRAFFIC\". It looks like 100% of traffic is going to our latest Revision (\"hello-knative\") and 0% of traffic is going to the Revision we configured earlier (\"hello-world\") By default, when Knative creates a brand new Revision it directs 100% of traffic to the latest Revision of your service. We can change this default behavior by specifying how much traffic we want each of our Revisions to receive. Lets split traffic in half, using a percentage of 50%. Info @latest will always point to our \"latest\" Revision which, at the moment, is hello-knative . kn kn service update hello \\ --traffic hello-world = 50 \\ --traffic @latest = 50 YAML apiVersion: serving.knative.dev/v1 kind: Route metadata: name: route-hello spec: traffic: - revisionName: @latest - percent: 50 - revisionName: hello-world percent: 50 Once you've edited your existing YAML file: kubectl apply -f hello.yaml Verify traffic split configure correctly by listing the revisions again. kn kn revisions list kubectl Though the following example doesn't cover it, you can peak under the hood to Kubernetes to see the revisions as Kubernetes sees them. kubectl get revisions Expected output: NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-knative hello 50 % 2 10m 3 OK / 4 True hello-world hello 50 % 1 36m 3 OK / 4 True Access the Knative service on the browser again http://hello.default.127.0.0.1.nip.io , and refresh multiple times to see the different output being served by each revision. You can use curl to access multiple times. curl http://hello.default.127.0.0.1.nip.io On the terminal you will see the output from the both revisions. Expected output: curl http://hello.default.127.0.0.1.nip.io Hello Knative! curl http://hello.default.127.0.0.1.nip.io Hello World! Congratulations, you've successfully split traffic between 2 different Revisions . Up next, Knative Eventing!","title":"Traffic Splitting"},{"location":"getting-started/first-traffic-split/#basics-of-traffic-splitting","text":"The last super power of Knative Serving we'll go over in this tutorial is traffic splitting. What are some common traffic splitting use-cases? Splitting traffic is useful for a number of very common modern infrastructure needs, such as blue/green deployments and canary deployments . Bringing these industry standards to bear on Kubernetes is as simple as a single CLI command on Knative or YAML tweak, let's see how!","title":"Basics of Traffic Splitting"},{"location":"getting-started/first-traffic-split/#creating-a-new-revision","text":"You may have noticed that when you created your Knative Service you assigned it a revision-name , \"world\". When your Service was created, Knative returned both a URL and a 'latest revision' name for your Knative Service. But what happens if you make a change to your Service? What exactly is a Revision ?\" You can think of a Revision as a stateless, autoscaling snapshot-in-time of application code and configuration. A new Revision will get created each and every time you make changes to your Knative Service, whether you assign it a name or not. When splitting traffic, Knative splits traffic between different Revisions of your Knative Service. Instead of TARGET =\"World,\" let's update the environment variable TARGET on our Knative Service hello to greet \"Knative\". Lets name this new revision hello-knative kn kn service update hello \\ --env TARGET = Knative \\ --revision-name = knative YAML apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: metadata: name: knative spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"Knative\" Once you've edited your existing YAML file: kubectl apply -f hello.yaml As before, Knative prints. out some helpful information to the CLI. Expected output: Service hello created to latest revision 'hello-knative' is available at URL: http://hello.default.127.0.0.1.nip.io Note, since we are updating an existing Knative Service hello , the URL doesn't change, but our new Revision should have the new name \"hello-knative\" Let's access our Knative Service again on the browser http://hello.default.127.0.0.1.nip.io to see the change, or use curl in your terminal: curl http://hello.default.127.0.0.1.nip.io Expected output: Hello Knative!","title":"Creating a new Revision"},{"location":"getting-started/first-traffic-split/#splitting-traffic","text":"You may at this point be wondering, \"where did 'Hello World!' go?\" Revisions are a stateless snapshot-in-time of application code and configuration so your \"hello-world\" Revision is still available to you. We can easily see a list of our existing revisions with the kn CLI: kn kn revisions list kubectl Though the following example doesn't cover it, you can peak under the hood to Kubernetes to see the revisions as Kubernetes sees them. kubectl get revisions Expected output: NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-knative hello 100 % 2 30s 3 OK / 4 True hello-world hello 1 5m 3 OK / 4 True The column most relevant for our purposes is \"TRAFFIC\". It looks like 100% of traffic is going to our latest Revision (\"hello-knative\") and 0% of traffic is going to the Revision we configured earlier (\"hello-world\") By default, when Knative creates a brand new Revision it directs 100% of traffic to the latest Revision of your service. We can change this default behavior by specifying how much traffic we want each of our Revisions to receive. Lets split traffic in half, using a percentage of 50%. Info @latest will always point to our \"latest\" Revision which, at the moment, is hello-knative . kn kn service update hello \\ --traffic hello-world = 50 \\ --traffic @latest = 50 YAML apiVersion: serving.knative.dev/v1 kind: Route metadata: name: route-hello spec: traffic: - revisionName: @latest - percent: 50 - revisionName: hello-world percent: 50 Once you've edited your existing YAML file: kubectl apply -f hello.yaml Verify traffic split configure correctly by listing the revisions again. kn kn revisions list kubectl Though the following example doesn't cover it, you can peak under the hood to Kubernetes to see the revisions as Kubernetes sees them. kubectl get revisions Expected output: NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-knative hello 50 % 2 10m 3 OK / 4 True hello-world hello 50 % 1 36m 3 OK / 4 True Access the Knative service on the browser again http://hello.default.127.0.0.1.nip.io , and refresh multiple times to see the different output being served by each revision. You can use curl to access multiple times. curl http://hello.default.127.0.0.1.nip.io On the terminal you will see the output from the both revisions. Expected output: curl http://hello.default.127.0.0.1.nip.io Hello Knative! curl http://hello.default.127.0.0.1.nip.io Hello World! Congratulations, you've successfully split traffic between 2 different Revisions . Up next, Knative Eventing!","title":"Splitting Traffic"},{"location":"getting-started/first-trigger/","text":"Creating your first Trigger \u00b6 kn kn trigger create cloudevents-player --sink cloudevents-player YAML //TODO Expected Output Trigger 'cloudevents-player' successfully created in namespace 'default' . What CloudEvents is my Trigger listening for? Since we didn't specify a --filter in our kn command, our Trigger is listening for any CloudEvents coming into the Broker . Now, when we go back to the CloudEvents Player and send an Event, we see that CloudEvents are both sent and received by the CloudEvents Player: You may need to refresh the page to see your changes What if I want to filter on CloudEvent attributes? First, delete your existing Trigger: kn trigger delete cloudevents-player Now let's add a Trigger that listens for a certain CloudEvent Type kn trigger create cloudevents-player --sink cloudevents-player --filter type = some-type If you send a CloudEvent with type \"some-type,\" it will be reflected in the CloudEvents Player UI. Any other types will be ignored by the Trigger . You can filter on any aspect of the CloudEvent you would like to. Some people call this \"Event-Driven Architecture\" which can be used to create your own \"Functions as a Service\" on Kubernetes","title":"Creating your first Trigger"},{"location":"getting-started/first-trigger/#creating-your-first-trigger","text":"kn kn trigger create cloudevents-player --sink cloudevents-player YAML //TODO Expected Output Trigger 'cloudevents-player' successfully created in namespace 'default' . What CloudEvents is my Trigger listening for? Since we didn't specify a --filter in our kn command, our Trigger is listening for any CloudEvents coming into the Broker . Now, when we go back to the CloudEvents Player and send an Event, we see that CloudEvents are both sent and received by the CloudEvents Player: You may need to refresh the page to see your changes What if I want to filter on CloudEvent attributes? First, delete your existing Trigger: kn trigger delete cloudevents-player Now let's add a Trigger that listens for a certain CloudEvent Type kn trigger create cloudevents-player --sink cloudevents-player --filter type = some-type If you send a CloudEvent with type \"some-type,\" it will be reflected in the CloudEvents Player UI. Any other types will be ignored by the Trigger . You can filter on any aspect of the CloudEvent you would like to. Some people call this \"Event-Driven Architecture\" which can be used to create your own \"Functions as a Service\" on Kubernetes","title":"Creating your first Trigger"},{"location":"getting-started/getting-started-eventing/","text":"Introducing the Knative Eventing \u00b6 Background \u00b6 With Knative Serving, we have a powerful tool which can take our containerized code and deploy it with relative ease. With Knative Eventing, you gain a few new super powers that allow you to build Event-Driven Applications. What are Event Driven Applications? Event-driven applications are designed to detect events as they occur, and then deal with them using some event-handling procedure. Producing \"events\" to detect and consuming events with an \"event-handling procedure\" is precisely what Knative Eventing enables. Knative Eventing acts as the \"glue\" between the disparate parts of your architecture and allows you to easily communicate between those parts in a fault-tolerant way. Some examples include: Creating and responding to Kubernetes API events Creating an image processing pipeline Facilitating AI workloads at the edge in large-scale, drone-powered sustainable agriculture projects . As you can see by the examples above, Knative Eventing implementations can range from the dead simple to extremely complex , the concepts you'll learn will be a great starting point to accomplish either.","title":"Introducing Knative Eventing"},{"location":"getting-started/getting-started-eventing/#introducing-the-knative-eventing","text":"","title":"Introducing the Knative Eventing"},{"location":"getting-started/getting-started-eventing/#background","text":"With Knative Serving, we have a powerful tool which can take our containerized code and deploy it with relative ease. With Knative Eventing, you gain a few new super powers that allow you to build Event-Driven Applications. What are Event Driven Applications? Event-driven applications are designed to detect events as they occur, and then deal with them using some event-handling procedure. Producing \"events\" to detect and consuming events with an \"event-handling procedure\" is precisely what Knative Eventing enables. Knative Eventing acts as the \"glue\" between the disparate parts of your architecture and allows you to easily communicate between those parts in a fault-tolerant way. Some examples include: Creating and responding to Kubernetes API events Creating an image processing pipeline Facilitating AI workloads at the edge in large-scale, drone-powered sustainable agriculture projects . As you can see by the examples above, Knative Eventing implementations can range from the dead simple to extremely complex , the concepts you'll learn will be a great starting point to accomplish either.","title":"Background"},{"location":"getting-started/getting-started/","text":"Before you begin \u00b6 Prerequisites \u00b6 Install kind . \u00b6 kind (Kubernetes in Docker) is a tool for running local Kubernetes clusters using Docker container \u201cnodes\u201d. kind was primarily designed for testing Kubernetes itself, but may be used for local development. See Kind website for installation options. Install kubectl \u00b6 The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. See the Kubernetes docs for installation options. Install kn \u00b6 The Knative CLI kn provides a quick and easy interface for creating Knative resources such as Knative Services and event sources, without the need to create or modify YAML files directly. kn also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting. Installing the kn CLI Using Homebrew For macOS, you can install kn by using Homebrew . brew install knative/client/kn Using a binary You can install kn by downloading the executable binary for your system and placing it in the system path. A link to the latest stable binary release is available on the kn release page . Installing kn using Go Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Running kn using container images WARNING: Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable. Links to images are available here: Latest release Nightly container image You can run kn from a container image. For example: docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list NOTE: Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn . For more complex installations, such as nightly releases, see Install kn Installing Knative (sandbox) \u00b6 The fastest way to get started with Knative locally is to use a Knative on Kind (konk) Install Knative and Kubernetes on a local Docker Daemon using Konk curl -sL install.konk.dev | bash What does the KonK script actually do? Knative on Kind (KonK) is a shell script which: Checks to see that you have Kind installed and creates a Cluster called \"knative\" via 01-kind.sh Installs Knative Serving with Kourier as the networking layer and nip.io as the DNS + some port-forwarding magic on the \"knative\" Cluster via 02-serving.sh Installs Knative Eventing with an In-Memory Channels and In-Memory Broker on the \"knative\" Cluster via 04-eventing.sh","title":"Before you begin"},{"location":"getting-started/getting-started/#before-you-begin","text":"","title":"Before you begin"},{"location":"getting-started/getting-started/#prerequisites","text":"","title":"Prerequisites"},{"location":"getting-started/getting-started/#install-kind","text":"kind (Kubernetes in Docker) is a tool for running local Kubernetes clusters using Docker container \u201cnodes\u201d. kind was primarily designed for testing Kubernetes itself, but may be used for local development. See Kind website for installation options.","title":"Install kind."},{"location":"getting-started/getting-started/#install-kubectl","text":"The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs. See the Kubernetes docs for installation options.","title":"Install kubectl"},{"location":"getting-started/getting-started/#install-kn","text":"The Knative CLI kn provides a quick and easy interface for creating Knative resources such as Knative Services and event sources, without the need to create or modify YAML files directly. kn also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting. Installing the kn CLI Using Homebrew For macOS, you can install kn by using Homebrew . brew install knative/client/kn Using a binary You can install kn by downloading the executable binary for your system and placing it in the system path. A link to the latest stable binary release is available on the kn release page . Installing kn using Go Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Running kn using container images WARNING: Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable. Links to images are available here: Latest release Nightly container image You can run kn from a container image. For example: docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list NOTE: Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn . For more complex installations, such as nightly releases, see Install kn","title":"Install kn"},{"location":"getting-started/getting-started/#installing-knative-sandbox","text":"The fastest way to get started with Knative locally is to use a Knative on Kind (konk) Install Knative and Kubernetes on a local Docker Daemon using Konk curl -sL install.konk.dev | bash What does the KonK script actually do? Knative on Kind (KonK) is a shell script which: Checks to see that you have Kind installed and creates a Cluster called \"knative\" via 01-kind.sh Installs Knative Serving with Kourier as the networking layer and nip.io as the DNS + some port-forwarding magic on the \"knative\" Cluster via 02-serving.sh Installs Knative Eventing with an In-Memory Channels and In-Memory Broker on the \"knative\" Cluster via 04-eventing.sh","title":"Installing Knative (sandbox)"},{"location":"getting-started/next-steps/","text":"Next Steps \u00b6 //TODO","title":"What's Next?"},{"location":"getting-started/next-steps/#next-steps","text":"//TODO","title":"Next Steps"},{"location":"install/","text":"Installing Knative \u00b6 Tip If you're looking for an easy way to install a local distribution of Knative for prototyping, check out our Quick Install w/ KonK You can install the Serving component, Eventing component, or both on your cluster by using one of the following deployment options: Using a YAML-based installation Using the Knative Operator . Following the documentation for vendor managed Knative offerings . You can also upgrade an existing Knative installation . NOTE: Knative installation instructions assume you are running Mac or Linux with a bash shell. Next steps \u00b6 Install the Knative CLI to use kn commands.","title":"Installing Knative"},{"location":"install/#installing-knative","text":"Tip If you're looking for an easy way to install a local distribution of Knative for prototyping, check out our Quick Install w/ KonK You can install the Serving component, Eventing component, or both on your cluster by using one of the following deployment options: Using a YAML-based installation Using the Knative Operator . Following the documentation for vendor managed Knative offerings . You can also upgrade an existing Knative installation . NOTE: Knative installation instructions assume you are running Mac or Linux with a bash shell.","title":"Installing Knative"},{"location":"install/#next-steps","text":"Install the Knative CLI to use kn commands.","title":"Next steps"},{"location":"install/check-install-version/","text":"Checking the version of your Knative components \u00b6 To obtain the version of the Knative component that you have running on your cluster, you query for the [component].knative.dev/release label with the following commands: Knative Serving kubectl get namespace knative-serving -o 'go-template={{index .metadata.labels \"serving.knative.dev/release\"}}' Knative Eventing kubectl get namespace knative-eventing -o 'go-template={{index .metadata.labels \"eventing.knative.dev/release\"}}'","title":"Checking the version of your Knative components"},{"location":"install/check-install-version/#checking-the-version-of-your-knative-components","text":"To obtain the version of the Knative component that you have running on your cluster, you query for the [component].knative.dev/release label with the following commands: Knative Serving kubectl get namespace knative-serving -o 'go-template={{index .metadata.labels \"serving.knative.dev/release\"}}' Knative Eventing kubectl get namespace knative-eventing -o 'go-template={{index .metadata.labels \"eventing.knative.dev/release\"}}'","title":"Checking the version of your Knative components"},{"location":"install/install-eventing-with-yaml/","text":"Installing Knative Eventing using YAML files \u00b6 This topic describes how to install Knative Eventing by applying YAML files using the kubectl CLI. Prerequisites \u00b6 Before installation, you must meet the prerequisites. See Knative Prerequisites . Install the Eventing component \u00b6 To install the Eventing component: Install the required custom resource definitions (CRDs): kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Install the core components of Eventing: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Info For information about the YAML files in the Knative Serving and Eventing releases, see Installation files . Verify the installation \u00b6 Monitor the Knative components until all of the components show a STATUS of Running : kubectl get pods --namespace knative-eventing Optional: Install a default channel (messaging) layer \u00b6 The tabs below expand to show instructions for installing a default channel layer. Follow the procedure for the channel of your choice: Apache Kafka Channel First, Install Apache Kafka for Kubernetes Then install the Apache Kafka channel: curl -L \"https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka/latest/channel-consolidated.yaml\" \\ | sed 's/REPLACE_WITH_CLUSTER_URL/my-cluster-kafka-bootstrap.kafka:9092/' \\ | kubectl apply -f - Tip To learn more about the Apache Kafka channel, try our sample Google Cloud Pub/Sub Channel Install the Google Cloud Pub/Sub channel: # This installs both the Channel and the GCP Sources. kubectl apply -f https://storage.googleapis.com/google-nightly/knative-gcp/latest/cloud-run-events.yaml Tip To learn more about the Google Cloud Pub/Sub channel, try our sample In-Memory (standalone) The following command installs an implementation of channel that runs in-memory. This implementation is nice because it is simple and standalone, but it is kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml NATS Channel First, Install NATS Streaming for Kubernetes Then install the NATS Streaming channel: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-natss/latest/300-natss-channel.yaml Optional: Install a broker layer: \u00b6 The tabs below expand to show instructions for installing the broker layer. Follow the procedure for the broker of your choice: Apache Kafka Broker The following commands install the Apache Kafka broker, and run event routing in a system namespace, knative-eventing , by default. Install the Kafka controller by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka broker data plane by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml For more information, see the Kafka broker documentation. MT-Channel-based The following command installs an implementation of broker that utilizes channels and runs event routing components in a System Namespace, providing a smaller and simpler installation. kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml To customize which broker channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | # This is the cluster-wide default broker channel. clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: imc-channel namespace: knative-eventing # This allows you to specify different defaults per-namespace, # in this case the \"some-namespace\" namespace will use the Kafka # channel ConfigMap by default (only for example, you will need # to install kafka also to make use of this). namespaceDefaults: some-namespace: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing The referenced imc-channel and kafka-channel example ConfigMaps would look like: apiVersion : v1 kind : ConfigMap metadata : name : imc-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1alpha1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Warning In order to use the KafkaChannel make sure it is installed on the cluster as discussed above. Next steps \u00b6 After installing Knative Eventing: To easily interact with Knative Eventing components, install the kn CLI To add optional enhancements to your installation, see Installing optional extensions Installing Knative Serving using YAML files","title":"Install Eventing with YAML"},{"location":"install/install-eventing-with-yaml/#installing-knative-eventing-using-yaml-files","text":"This topic describes how to install Knative Eventing by applying YAML files using the kubectl CLI.","title":"Installing Knative Eventing using YAML files"},{"location":"install/install-eventing-with-yaml/#prerequisites","text":"Before installation, you must meet the prerequisites. See Knative Prerequisites .","title":"Prerequisites"},{"location":"install/install-eventing-with-yaml/#install-the-eventing-component","text":"To install the Eventing component: Install the required custom resource definitions (CRDs): kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Install the core components of Eventing: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Info For information about the YAML files in the Knative Serving and Eventing releases, see Installation files .","title":"Install the Eventing component"},{"location":"install/install-eventing-with-yaml/#verify-the-installation","text":"Monitor the Knative components until all of the components show a STATUS of Running : kubectl get pods --namespace knative-eventing","title":"Verify the installation"},{"location":"install/install-eventing-with-yaml/#optional-install-a-default-channel-messaging-layer","text":"The tabs below expand to show instructions for installing a default channel layer. Follow the procedure for the channel of your choice: Apache Kafka Channel First, Install Apache Kafka for Kubernetes Then install the Apache Kafka channel: curl -L \"https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka/latest/channel-consolidated.yaml\" \\ | sed 's/REPLACE_WITH_CLUSTER_URL/my-cluster-kafka-bootstrap.kafka:9092/' \\ | kubectl apply -f - Tip To learn more about the Apache Kafka channel, try our sample Google Cloud Pub/Sub Channel Install the Google Cloud Pub/Sub channel: # This installs both the Channel and the GCP Sources. kubectl apply -f https://storage.googleapis.com/google-nightly/knative-gcp/latest/cloud-run-events.yaml Tip To learn more about the Google Cloud Pub/Sub channel, try our sample In-Memory (standalone) The following command installs an implementation of channel that runs in-memory. This implementation is nice because it is simple and standalone, but it is kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml NATS Channel First, Install NATS Streaming for Kubernetes Then install the NATS Streaming channel: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-natss/latest/300-natss-channel.yaml","title":"Optional: Install a default channel (messaging) layer"},{"location":"install/install-eventing-with-yaml/#optional-install-a-broker-layer","text":"The tabs below expand to show instructions for installing the broker layer. Follow the procedure for the broker of your choice: Apache Kafka Broker The following commands install the Apache Kafka broker, and run event routing in a system namespace, knative-eventing , by default. Install the Kafka controller by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka broker data plane by entering the following command: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml For more information, see the Kafka broker documentation. MT-Channel-based The following command installs an implementation of broker that utilizes channels and runs event routing components in a System Namespace, providing a smaller and simpler installation. kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml To customize which broker channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | # This is the cluster-wide default broker channel. clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: imc-channel namespace: knative-eventing # This allows you to specify different defaults per-namespace, # in this case the \"some-namespace\" namespace will use the Kafka # channel ConfigMap by default (only for example, you will need # to install kafka also to make use of this). namespaceDefaults: some-namespace: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing The referenced imc-channel and kafka-channel example ConfigMaps would look like: apiVersion : v1 kind : ConfigMap metadata : name : imc-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1alpha1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Warning In order to use the KafkaChannel make sure it is installed on the cluster as discussed above.","title":"Optional: Install a broker layer:"},{"location":"install/install-eventing-with-yaml/#next-steps","text":"After installing Knative Eventing: To easily interact with Knative Eventing components, install the kn CLI To add optional enhancements to your installation, see Installing optional extensions Installing Knative Serving using YAML files","title":"Next steps"},{"location":"install/install-extensions/","text":"Installing optional extensions \u00b6 To add extra features to your Knative Serving or Eventing installation, you can install extensions by applying YAML files using the kubectl CLI. For information about the YAML files in the Knative Serving and Eventing releases, see Installation files . Prerequisites \u00b6 Before you install any optional extensions, you must install Knative Serving or Eventing. See Installing Serving using YAML files and Installing Eventing using YAML files . Install optional Serving extensions \u00b6 The tabs below expand to show instructions for installing each Serving extension. HPA autoscaling Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will install the components needed to support HPA-class autoscaling: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml TLS with cert-manager Knative supports automatically provisioning TLS certificates via cert-manager . The following commands will install the components needed to support the provisioning of TLS certificates via cert-manager. First, install cert-manager version 0.12.0 or higher Next, install the component that integrates Knative with cert-manager: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Now configure Knative to automatically configure TLS certificates . TLS via HTTP01 Knative supports automatically provisioning TLS certificates using Let's Encrypt HTTP01 challenges. The following commands will install the components needed to support that. First, install the net-http01 controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml Next, configure the certificate.class to use this certificate type. kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"certificate.class\":\"net-http01.certificate.networking.knative.dev\"}}' Lastly, enable auto-TLS. kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"autoTLS\":\"Enabled\"}}' TLS wildcard support If you are using a Certificate implementation that supports provisioning wildcard certificates (e.g. cert-manager with a DNS01 issuer), then the most efficient way to provision certificates is with the namespace wildcard certificate controller. The following command will install the components needed to provision wildcard certificates in each namespace: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-nscert.yaml Warning Note this will not work with HTTP01 either via cert-manager or the net-http01 options. DomainMapping CRD The DomainMapping CRD allows a user to map a Domain Name that they own to a specific Knative Service. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-domainmapping-crds.yaml kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-domainmapping.yaml Install optional Eventing extensions \u00b6 The tabs below expand to show instructions for installing each Eventing extension. Apache Kafka Sink Install the Kafka controller: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Sink data plane: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml For more information, see the Kafka Sink documentation. Sugar Controller The following command installs the Eventing Sugar Controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml The Knative Eventing Sugar Controller will react to special labels and annotations and produce Eventing resources. For example: When a Namespace is labeled with eventing.knative.dev/injection=enabled , the controller will create a default broker in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller will create a Broker named by that Trigger in the Trigger's Namespace. The following command enables the default Broker on a namespace (here default ): kubectl label namespace default eventing.knative.dev/injection = enabled Github Source The following command installs the single-tenant Github source: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-github/latest/github.yaml The single-tenant GitHub source creates one Knative service per GitHub source. The following command installs the multi-tenant GitHub source: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-github/latest/mt-github.yaml The multi-tenant GitHub source creates only one Knative service handling all GitHub sources in the cluster. This source does not support logging or tracing configuration yet. To learn more about the Github source, try our sample Apache Camel-K Source The following command installs the Apache Camel-K Source: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-camel/latest/camel.yaml To learn more about the Apache Camel-K source, try our sample Apache Kafka Source The following command installs the Apache Kafka Source: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka/latest/source.yaml To learn more about the Apache Kafka source, try our sample GCP Sources The following command installs the GCP Sources: # This installs both the Sources and the Channel. kubectl apply -f https://storage.googleapis.com/google-nightly/knative-gcp/latest/cloud-run-events.yaml To learn more about the Cloud Pub/Sub source, try our sample . To learn more about the Cloud Storage source, try our sample . To learn more about the Cloud Scheduler source, try our sample . To learn more about the Cloud Audit Logs source, try our sample . Apache CouchDB Source The following command installs the Apache CouchDB Source: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-couchdb/latest/couchdb.yaml To learn more about the Apache CouchDB source, read the documentation . VMware Sources and Bindings The following command installs the VMware Sources and Bindings: kubectl apply -f https://storage.googleapis.com/vmware-tanzu-nightly/sources-for-knative/latest/release.yaml To learn more about the VMware sources and bindings, try our samples . Next steps \u00b6 To easily interact with Knative Services and Eventing components, install the kn CLI","title":"Install optional extensions"},{"location":"install/install-extensions/#installing-optional-extensions","text":"To add extra features to your Knative Serving or Eventing installation, you can install extensions by applying YAML files using the kubectl CLI. For information about the YAML files in the Knative Serving and Eventing releases, see Installation files .","title":"Installing optional extensions"},{"location":"install/install-extensions/#prerequisites","text":"Before you install any optional extensions, you must install Knative Serving or Eventing. See Installing Serving using YAML files and Installing Eventing using YAML files .","title":"Prerequisites"},{"location":"install/install-extensions/#install-optional-serving-extensions","text":"The tabs below expand to show instructions for installing each Serving extension. HPA autoscaling Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will install the components needed to support HPA-class autoscaling: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml TLS with cert-manager Knative supports automatically provisioning TLS certificates via cert-manager . The following commands will install the components needed to support the provisioning of TLS certificates via cert-manager. First, install cert-manager version 0.12.0 or higher Next, install the component that integrates Knative with cert-manager: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Now configure Knative to automatically configure TLS certificates . TLS via HTTP01 Knative supports automatically provisioning TLS certificates using Let's Encrypt HTTP01 challenges. The following commands will install the components needed to support that. First, install the net-http01 controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml Next, configure the certificate.class to use this certificate type. kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"certificate.class\":\"net-http01.certificate.networking.knative.dev\"}}' Lastly, enable auto-TLS. kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"autoTLS\":\"Enabled\"}}' TLS wildcard support If you are using a Certificate implementation that supports provisioning wildcard certificates (e.g. cert-manager with a DNS01 issuer), then the most efficient way to provision certificates is with the namespace wildcard certificate controller. The following command will install the components needed to provision wildcard certificates in each namespace: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-nscert.yaml Warning Note this will not work with HTTP01 either via cert-manager or the net-http01 options. DomainMapping CRD The DomainMapping CRD allows a user to map a Domain Name that they own to a specific Knative Service. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-domainmapping-crds.yaml kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-domainmapping.yaml","title":"Install optional Serving extensions"},{"location":"install/install-extensions/#install-optional-eventing-extensions","text":"The tabs below expand to show instructions for installing each Eventing extension. Apache Kafka Sink Install the Kafka controller: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Sink data plane: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml For more information, see the Kafka Sink documentation. Sugar Controller The following command installs the Eventing Sugar Controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml The Knative Eventing Sugar Controller will react to special labels and annotations and produce Eventing resources. For example: When a Namespace is labeled with eventing.knative.dev/injection=enabled , the controller will create a default broker in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller will create a Broker named by that Trigger in the Trigger's Namespace. The following command enables the default Broker on a namespace (here default ): kubectl label namespace default eventing.knative.dev/injection = enabled Github Source The following command installs the single-tenant Github source: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-github/latest/github.yaml The single-tenant GitHub source creates one Knative service per GitHub source. The following command installs the multi-tenant GitHub source: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-github/latest/mt-github.yaml The multi-tenant GitHub source creates only one Knative service handling all GitHub sources in the cluster. This source does not support logging or tracing configuration yet. To learn more about the Github source, try our sample Apache Camel-K Source The following command installs the Apache Camel-K Source: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-camel/latest/camel.yaml To learn more about the Apache Camel-K source, try our sample Apache Kafka Source The following command installs the Apache Kafka Source: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-kafka/latest/source.yaml To learn more about the Apache Kafka source, try our sample GCP Sources The following command installs the GCP Sources: # This installs both the Sources and the Channel. kubectl apply -f https://storage.googleapis.com/google-nightly/knative-gcp/latest/cloud-run-events.yaml To learn more about the Cloud Pub/Sub source, try our sample . To learn more about the Cloud Storage source, try our sample . To learn more about the Cloud Scheduler source, try our sample . To learn more about the Cloud Audit Logs source, try our sample . Apache CouchDB Source The following command installs the Apache CouchDB Source: kubectl apply -f https://storage.googleapis.com/knative-sandbox-nightly/eventing-couchdb/latest/couchdb.yaml To learn more about the Apache CouchDB source, read the documentation . VMware Sources and Bindings The following command installs the VMware Sources and Bindings: kubectl apply -f https://storage.googleapis.com/vmware-tanzu-nightly/sources-for-knative/latest/release.yaml To learn more about the VMware sources and bindings, try our samples .","title":"Install optional Eventing extensions"},{"location":"install/install-extensions/#next-steps","text":"To easily interact with Knative Services and Eventing components, install the kn CLI","title":"Next steps"},{"location":"install/install-serving-with-yaml/","text":"Installing Knative Serving using YAML files \u00b6 This topic describes how to install Knative Serving by applying YAML files using the kubectl CLI. Prerequisites \u00b6 Before installation, you must meet the prerequisites. See Knative Prerequisites . Install the Serving component \u00b6 To install the serving component: Install the required custom resources: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Install the core components of Knative Serving: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Info For information about the YAML files in the Knative Serving and Eventing releases, see Installation files . Install a networking layer \u00b6 The tabs below expand to show instructions for installing a networking layer. Follow the procedure for the networking layer of your choice: Kourier (Choose this if you are not sure) The following commands install Kourier and enable its Knative integration. Install the Knative Kourier controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml To configure Knative Serving to use Kourier by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"kourier.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace kourier-system get service kourier Tip Save this to use in the Configure DNS section. Ambassador The following commands install Ambassador and enable its Knative integration. Create a namespace to install Ambassador in: kubectl create namespace ambassador Install Ambassador: kubectl apply --namespace ambassador \\ -f https://getambassador.io/yaml/ambassador/ambassador-crds.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-service.yaml Give Ambassador the required permissions: kubectl patch clusterrolebinding ambassador -p '{\"subjects\":[{\"kind\": \"ServiceAccount\", \"name\": \"ambassador\", \"namespace\": \"ambassador\"}]}' Enable Knative support in Ambassador: kubectl set env --namespace ambassador deployments/ambassador AMBASSADOR_KNATIVE_SUPPORT = true To configure Knative Serving to use Ambassador by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"ambassador.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace ambassador get service ambassador Tip Save this to use in the Configure DNS section. Contour The following commands install Contour and enable its Knative integration. Install a properly configured Contour: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml Install the Knative Contour controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml To configure Knative Serving to use Contour by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"contour.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace contour-external get service envoy Tip Save this to use in the Configure DNS section. Gloo For a detailed guide on Gloo integration, see Installing Gloo for Knative in the Gloo documentation. The following commands install Gloo and enable its Knative integration. Make sure glooctl is installed (version 1.3.x and higher recommended): glooctl version If it is not installed, you can install the latest version using: curl -sL https://run.solo.io/gloo/install | sh export PATH = $HOME /.gloo/bin: $PATH Or following the Gloo CLI install instructions . Install Gloo and the Knative integration: glooctl install knative --install-knative = false Fetch the External IP or CNAME: glooctl proxy url --name knative-external-proxy Tip Save this to use in the Configure DNS section. Istio The following commands install Istio and enable its Knative integration. Install a properly configured Istio ( Advanced installation ) kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Install the Knative Istio controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Fetch the External IP or CNAME: kubectl --namespace istio-system get service istio-ingressgateway Tip Save this to use in the Configure DNS section. Kong The following commands install Kong and enable its Knative integration. Install Kong Ingress Controller: kubectl apply -f https://raw.githubusercontent.com/Kong/kubernetes-ingress-controller/0.9.x/deploy/single/all-in-one-dbless.yaml To configure Knative Serving to use Kong by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"kong\"}}' Fetch the External IP or CNAME: kubectl --namespace kong get service kong-proxy Tip Save this to use in the Configure DNS section. Verify the installation \u00b6 Monitor the Knative components until all of the components show a STATUS of Running or Completed : kubectl get pods --namespace knative-serving Configure DNS \u00b6 You can configure DNS to prevent the need to run curl commands with a host header. The tabs below expand to show instructions for configuring DNS. Follow the procedure for the DNS of your choice: Magic DNS (xip.io) We ship a simple Kubernetes Job called \"default domain\" that will (see caveats) configure Knative Serving to use xip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml CAVEAT This will only work if the cluster LoadBalancer service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like Minikube. For these, see \"Real DNS\" or \"Temporary DNS\". Real DNS To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35 .233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, direct Knative to use that domain: # Replace knative.example.com with your domain suffix kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"knative.example.com\":\"\"}}' Temporary DNS Info If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (xip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc Verify the output NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer in section 3 above, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the above helloworld-go application, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 Verify the output In the case of the provided helloworld-go sample application, the output should, using the default configuration, be: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Next steps \u00b6 After installing Knative Serving: Installing Knative Eventing using YAML files To add optional enhancements to your installation, see Installing optional extensions . To easily interact with Knative Services, install the kn CLI","title":"Install Serving with YAML"},{"location":"install/install-serving-with-yaml/#installing-knative-serving-using-yaml-files","text":"This topic describes how to install Knative Serving by applying YAML files using the kubectl CLI.","title":"Installing Knative Serving using YAML files"},{"location":"install/install-serving-with-yaml/#prerequisites","text":"Before installation, you must meet the prerequisites. See Knative Prerequisites .","title":"Prerequisites"},{"location":"install/install-serving-with-yaml/#install-the-serving-component","text":"To install the serving component: Install the required custom resources: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Install the core components of Knative Serving: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Info For information about the YAML files in the Knative Serving and Eventing releases, see Installation files .","title":"Install the Serving component"},{"location":"install/install-serving-with-yaml/#install-a-networking-layer","text":"The tabs below expand to show instructions for installing a networking layer. Follow the procedure for the networking layer of your choice: Kourier (Choose this if you are not sure) The following commands install Kourier and enable its Knative integration. Install the Knative Kourier controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml To configure Knative Serving to use Kourier by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"kourier.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace kourier-system get service kourier Tip Save this to use in the Configure DNS section. Ambassador The following commands install Ambassador and enable its Knative integration. Create a namespace to install Ambassador in: kubectl create namespace ambassador Install Ambassador: kubectl apply --namespace ambassador \\ -f https://getambassador.io/yaml/ambassador/ambassador-crds.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-service.yaml Give Ambassador the required permissions: kubectl patch clusterrolebinding ambassador -p '{\"subjects\":[{\"kind\": \"ServiceAccount\", \"name\": \"ambassador\", \"namespace\": \"ambassador\"}]}' Enable Knative support in Ambassador: kubectl set env --namespace ambassador deployments/ambassador AMBASSADOR_KNATIVE_SUPPORT = true To configure Knative Serving to use Ambassador by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"ambassador.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace ambassador get service ambassador Tip Save this to use in the Configure DNS section. Contour The following commands install Contour and enable its Knative integration. Install a properly configured Contour: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml Install the Knative Contour controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml To configure Knative Serving to use Contour by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"contour.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace contour-external get service envoy Tip Save this to use in the Configure DNS section. Gloo For a detailed guide on Gloo integration, see Installing Gloo for Knative in the Gloo documentation. The following commands install Gloo and enable its Knative integration. Make sure glooctl is installed (version 1.3.x and higher recommended): glooctl version If it is not installed, you can install the latest version using: curl -sL https://run.solo.io/gloo/install | sh export PATH = $HOME /.gloo/bin: $PATH Or following the Gloo CLI install instructions . Install Gloo and the Knative integration: glooctl install knative --install-knative = false Fetch the External IP or CNAME: glooctl proxy url --name knative-external-proxy Tip Save this to use in the Configure DNS section. Istio The following commands install Istio and enable its Knative integration. Install a properly configured Istio ( Advanced installation ) kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Install the Knative Istio controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Fetch the External IP or CNAME: kubectl --namespace istio-system get service istio-ingressgateway Tip Save this to use in the Configure DNS section. Kong The following commands install Kong and enable its Knative integration. Install Kong Ingress Controller: kubectl apply -f https://raw.githubusercontent.com/Kong/kubernetes-ingress-controller/0.9.x/deploy/single/all-in-one-dbless.yaml To configure Knative Serving to use Kong by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"kong\"}}' Fetch the External IP or CNAME: kubectl --namespace kong get service kong-proxy Tip Save this to use in the Configure DNS section.","title":"Install a networking layer"},{"location":"install/install-serving-with-yaml/#verify-the-installation","text":"Monitor the Knative components until all of the components show a STATUS of Running or Completed : kubectl get pods --namespace knative-serving","title":"Verify the installation"},{"location":"install/install-serving-with-yaml/#configure-dns","text":"You can configure DNS to prevent the need to run curl commands with a host header. The tabs below expand to show instructions for configuring DNS. Follow the procedure for the DNS of your choice: Magic DNS (xip.io) We ship a simple Kubernetes Job called \"default domain\" that will (see caveats) configure Knative Serving to use xip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml CAVEAT This will only work if the cluster LoadBalancer service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like Minikube. For these, see \"Real DNS\" or \"Temporary DNS\". Real DNS To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35 .233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, direct Knative to use that domain: # Replace knative.example.com with your domain suffix kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"knative.example.com\":\"\"}}' Temporary DNS Info If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (xip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc Verify the output NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer in section 3 above, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the above helloworld-go application, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 Verify the output In the case of the provided helloworld-go sample application, the output should, using the default configuration, be: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution.","title":"Configure DNS"},{"location":"install/install-serving-with-yaml/#next-steps","text":"After installing Knative Serving: Installing Knative Eventing using YAML files To add optional enhancements to your installation, see Installing optional extensions . To easily interact with Knative Services, install the kn CLI","title":"Next steps"},{"location":"install/installation-files/","text":"Installation files \u00b6 This guide provides reference information about the YAML files in the Knative Serving and Eventing releases. The YAML files in the releases include: The custom resource definitions (CRDs) and core components required to install Knative. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Serving using YAML files and Installing Eventing using YAML files . Knative Serving installation files \u00b6 The table below describes the installation files in the Knative Serving release: File name Description Dependencies serving-core.yaml Required: Knative Serving core components. serving-crds.yaml serving-crds.yaml Required: Knative Serving core CRDs. none serving-default-domain.yaml Configures Knative Serving to use http://xip.io as the default DNS suffix. serving-core.yaml serving-domainmapping-crds.yaml CRDs used by the Domain Mapping feature. none serving-domainmapping.yaml Components used by the Domain Mapping feature. serving-domainmapping-crds.yaml serving-hpa.yaml Components to autoscale Knative revisions through the Kubernetes Horizontal Pod Autoscaler. serving-core.yaml serving-nscert.yaml Components to provision TLS wildcard certificates. serving-core.yaml serving-post-install-jobs.yaml Additional jobs after installing serving-core.yaml . Currently it is the same as serving-storage-version-migration.yaml . serving-core.yaml serving-storage-version-migration.yaml Migrates the storage version of Knative resources, including Service, Route, Revision, and Configuration, from v1alpha1 and v1beta1 to v1 . Required by upgrade from version 0.18 to 0.19. serving-core.yaml Knative Eventing installation files \u00b6 The table below describes the installation files in the Knative Eventing release: File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing-sugar-controller.yaml Reconciler that watches for labels and annotations on certain resources to inject eventing components. eventing-core.yaml eventing.yaml Combines eventing-core.yaml , mt-channel-broker.yaml , and in-memory-channel.yaml . none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml","title":"Installation Files"},{"location":"install/installation-files/#installation-files","text":"This guide provides reference information about the YAML files in the Knative Serving and Eventing releases. The YAML files in the releases include: The custom resource definitions (CRDs) and core components required to install Knative. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Serving using YAML files and Installing Eventing using YAML files .","title":"Installation files"},{"location":"install/installation-files/#knative-serving-installation-files","text":"The table below describes the installation files in the Knative Serving release: File name Description Dependencies serving-core.yaml Required: Knative Serving core components. serving-crds.yaml serving-crds.yaml Required: Knative Serving core CRDs. none serving-default-domain.yaml Configures Knative Serving to use http://xip.io as the default DNS suffix. serving-core.yaml serving-domainmapping-crds.yaml CRDs used by the Domain Mapping feature. none serving-domainmapping.yaml Components used by the Domain Mapping feature. serving-domainmapping-crds.yaml serving-hpa.yaml Components to autoscale Knative revisions through the Kubernetes Horizontal Pod Autoscaler. serving-core.yaml serving-nscert.yaml Components to provision TLS wildcard certificates. serving-core.yaml serving-post-install-jobs.yaml Additional jobs after installing serving-core.yaml . Currently it is the same as serving-storage-version-migration.yaml . serving-core.yaml serving-storage-version-migration.yaml Migrates the storage version of Knative resources, including Service, Route, Revision, and Configuration, from v1alpha1 and v1beta1 to v1 . Required by upgrade from version 0.18 to 0.19. serving-core.yaml","title":"Knative Serving installation files"},{"location":"install/installation-files/#knative-eventing-installation-files","text":"The table below describes the installation files in the Knative Eventing release: File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing-sugar-controller.yaml Reconciler that watches for labels and annotations on certain resources to inject eventing components. eventing-core.yaml eventing.yaml Combines eventing-core.yaml , mt-channel-broker.yaml , and in-memory-channel.yaml . none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml","title":"Knative Eventing installation files"},{"location":"install/installing-istio/","text":"Installing Istio for Knative \u00b6 This guide walks you through manually installing and customizing Istio for use with Knative. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need to customize your installation. Before you begin \u00b6 You need: A Kubernetes cluster created. istioctl (v1.7 or later) installed. Supported Istio versions \u00b6 The current known-to-be-stable version of Istio tested in conjunction with Knative is v1.8.2 . Versions in the 1.7 line are generally fine too. 1.8.0 and 1.8.1 have bugs that don't work with Knative. Installing Istio \u00b6 When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Installing Istio without sidecar injection instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs. You can easily customize your Istio installation with istioctl . The below sections cover a few useful Istio configurations and their benefits. Choosing an Istio installation \u00b6 You can install Istio with or without a service mesh: Installing Istio without sidecar injection (Recommended default installation) Installing Istio with sidecar injection If you want to get up and running with Knative quickly, we recommend installing Istio without automatic sidecar injection. This install is also recommended for users who don't need the Istio service mesh, or who want to enable the service mesh by manually injecting the Istio sidecars . Installing Istio without sidecar injection \u00b6 Enter the following command to install Istio: cat << EOF > ./istio-minimal-operator.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: values: global: proxy: autoInject: disabled useMCP: false # The third-party-jwt is not enabled on all k8s. # See: https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens jwtPolicy: first-party-jwt addonComponents: pilot: enabled: true components: ingressGateways: - name: istio-ingressgateway enabled: true EOF istioctl install -f istio-minimal-operator.yaml Installing Istio with sidecar injection \u00b6 If you want to enable the Istio service mesh, you must enable automatic sidecar injection . The Istio service mesh provides a few benefits: Allows you to turn on mutual TLS , which secures service-to-service traffic within the cluster. Allows you to use the Istio authorization policy , controlling the access to each Knative service based on Istio service roles. To automatic sidecar injection, set autoInject: enabled in addition to above operator configuration. global: proxy: autoInject: enabled Using Istio mTLS feature \u00b6 Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment. Enable sidecar container on knative-serving system namespace. kubectl label namespace knative-serving istio-injection = enabled Set PeerAuthentication to PERMISSIVE on knative-serving system namespace. cat <<EOF | kubectl apply -f - apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"knative-serving\" spec: mtls: mode: PERMISSIVE EOF After you install the cluster local gateway, your service and deployment for the local gateway is named knative-local-gateway . Updating the config-istio configmap to use a non-default local gateway \u00b6 If you create a custom service and deployment for local gateway with a name other than knative-local-gateway , you need to update gateway configmap config-istio under the knative-serving namespace. Edit the config-istio configmap: kubectl edit configmap config-istio -n knative-serving Replace the local-gateway.knative-serving.knative-local-gateway field with the custom service. As an example, if you name both the service and deployment custom-local-gateway under the namespace istio-system , it should be updated to: custom-local-gateway.istio-system.svc.cluster.local As an example, if both the custom service and deployment are labeled with custom: custom-local-gateway , not the default istio: knative-local-gateway , you must update gateway instance knative-local-gateway in the knative-serving namespace: kubectl edit gateway knative-local-gateway -n knative-serving Replace the label selector with the label of your service: istio: knative-local-gateway For the service above, it should be updated to: custom: custom-local-gateway If there is a change in service ports (compared to that of knative-local-gateway ), update the port info in the gateway accordingly. Verifying your Istio install \u00b6 View the status of your Istio installation to make sure the install was successful. It might take a few seconds, so rerun the following command until all of the pods show a STATUS of Running or Completed : kubectl get pods --namespace istio-system Tip: You can append the --watch flag to the kubectl get commands to view the pod status in realtime. You use CTRL + C to exit watch mode. Configuring DNS \u00b6 Knative dispatches to different services based on their hostname, so it is recommended to have DNS properly configured. To do this, begin by looking up the external IP address that Istio received: $ kubectl get svc -nistio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.0.2.24 34.83.80.117 15020:32206/TCP,80:30742/TCP,443:30996/TCP 2m14s istio-pilot ClusterIP 10.0.3.27 <none> 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m14s This external IP can be used with your DNS provider with a wildcard A record. However, for a basic non-production set up, this external IP address can be used with xip.io in the config-domain ConfigMap in knative-serving . You can edit this by using the following command: kubectl edit cm config-domain --namespace knative-serving Given the external IP above, change the content to: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: # xip.io is a \"magic\" DNS provider, which resolves all DNS lookups for: # *.{ip}.xip.io to {ip}. 34.83.80.117.xip.io: \"\" Istio resources \u00b6 For the official Istio installation guide, see the Istio Kubernetes Getting Started Guide . For the full list of available configs when installing Istio with istioctl , see the Istio Installation Options reference . Clean up Istio \u00b6 See the Uninstall Istio . What's next \u00b6 Install Knative . Try the Getting Started with App Deployment guide for Knative serving.","title":"Installing Istio for Knative"},{"location":"install/installing-istio/#installing-istio-for-knative","text":"This guide walks you through manually installing and customizing Istio for use with Knative. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need to customize your installation.","title":"Installing Istio for Knative"},{"location":"install/installing-istio/#before-you-begin","text":"You need: A Kubernetes cluster created. istioctl (v1.7 or later) installed.","title":"Before you begin"},{"location":"install/installing-istio/#supported-istio-versions","text":"The current known-to-be-stable version of Istio tested in conjunction with Knative is v1.8.2 . Versions in the 1.7 line are generally fine too. 1.8.0 and 1.8.1 have bugs that don't work with Knative.","title":"Supported Istio versions"},{"location":"install/installing-istio/#installing-istio","text":"When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Installing Istio without sidecar injection instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs. You can easily customize your Istio installation with istioctl . The below sections cover a few useful Istio configurations and their benefits.","title":"Installing Istio"},{"location":"install/installing-istio/#choosing-an-istio-installation","text":"You can install Istio with or without a service mesh: Installing Istio without sidecar injection (Recommended default installation) Installing Istio with sidecar injection If you want to get up and running with Knative quickly, we recommend installing Istio without automatic sidecar injection. This install is also recommended for users who don't need the Istio service mesh, or who want to enable the service mesh by manually injecting the Istio sidecars .","title":"Choosing an Istio installation"},{"location":"install/installing-istio/#installing-istio-without-sidecar-injection","text":"Enter the following command to install Istio: cat << EOF > ./istio-minimal-operator.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: values: global: proxy: autoInject: disabled useMCP: false # The third-party-jwt is not enabled on all k8s. # See: https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens jwtPolicy: first-party-jwt addonComponents: pilot: enabled: true components: ingressGateways: - name: istio-ingressgateway enabled: true EOF istioctl install -f istio-minimal-operator.yaml","title":"Installing Istio without sidecar injection"},{"location":"install/installing-istio/#installing-istio-with-sidecar-injection","text":"If you want to enable the Istio service mesh, you must enable automatic sidecar injection . The Istio service mesh provides a few benefits: Allows you to turn on mutual TLS , which secures service-to-service traffic within the cluster. Allows you to use the Istio authorization policy , controlling the access to each Knative service based on Istio service roles. To automatic sidecar injection, set autoInject: enabled in addition to above operator configuration. global: proxy: autoInject: enabled","title":"Installing Istio with sidecar injection"},{"location":"install/installing-istio/#using-istio-mtls-feature","text":"Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment. Enable sidecar container on knative-serving system namespace. kubectl label namespace knative-serving istio-injection = enabled Set PeerAuthentication to PERMISSIVE on knative-serving system namespace. cat <<EOF | kubectl apply -f - apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"knative-serving\" spec: mtls: mode: PERMISSIVE EOF After you install the cluster local gateway, your service and deployment for the local gateway is named knative-local-gateway .","title":"Using Istio mTLS feature"},{"location":"install/installing-istio/#updating-the-config-istio-configmap-to-use-a-non-default-local-gateway","text":"If you create a custom service and deployment for local gateway with a name other than knative-local-gateway , you need to update gateway configmap config-istio under the knative-serving namespace. Edit the config-istio configmap: kubectl edit configmap config-istio -n knative-serving Replace the local-gateway.knative-serving.knative-local-gateway field with the custom service. As an example, if you name both the service and deployment custom-local-gateway under the namespace istio-system , it should be updated to: custom-local-gateway.istio-system.svc.cluster.local As an example, if both the custom service and deployment are labeled with custom: custom-local-gateway , not the default istio: knative-local-gateway , you must update gateway instance knative-local-gateway in the knative-serving namespace: kubectl edit gateway knative-local-gateway -n knative-serving Replace the label selector with the label of your service: istio: knative-local-gateway For the service above, it should be updated to: custom: custom-local-gateway If there is a change in service ports (compared to that of knative-local-gateway ), update the port info in the gateway accordingly.","title":"Updating the config-istio configmap to use a non-default local gateway"},{"location":"install/installing-istio/#verifying-your-istio-install","text":"View the status of your Istio installation to make sure the install was successful. It might take a few seconds, so rerun the following command until all of the pods show a STATUS of Running or Completed : kubectl get pods --namespace istio-system Tip: You can append the --watch flag to the kubectl get commands to view the pod status in realtime. You use CTRL + C to exit watch mode.","title":"Verifying your Istio install"},{"location":"install/installing-istio/#configuring-dns","text":"Knative dispatches to different services based on their hostname, so it is recommended to have DNS properly configured. To do this, begin by looking up the external IP address that Istio received: $ kubectl get svc -nistio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.0.2.24 34.83.80.117 15020:32206/TCP,80:30742/TCP,443:30996/TCP 2m14s istio-pilot ClusterIP 10.0.3.27 <none> 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m14s This external IP can be used with your DNS provider with a wildcard A record. However, for a basic non-production set up, this external IP address can be used with xip.io in the config-domain ConfigMap in knative-serving . You can edit this by using the following command: kubectl edit cm config-domain --namespace knative-serving Given the external IP above, change the content to: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: # xip.io is a \"magic\" DNS provider, which resolves all DNS lookups for: # *.{ip}.xip.io to {ip}. 34.83.80.117.xip.io: \"\"","title":"Configuring DNS"},{"location":"install/installing-istio/#istio-resources","text":"For the official Istio installation guide, see the Istio Kubernetes Getting Started Guide . For the full list of available configs when installing Istio with istioctl , see the Istio Installation Options reference .","title":"Istio resources"},{"location":"install/installing-istio/#clean-up-istio","text":"See the Uninstall Istio .","title":"Clean up Istio"},{"location":"install/installing-istio/#whats-next","text":"Install Knative . Try the Getting Started with App Deployment guide for Knative serving.","title":"What's next"},{"location":"install/knative-offerings/","text":"Knative Offerings \u00b6 Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products. Please check with each of these vendors for what is or is not supported. Here is a list of commercial Knative products (alphabetically): Gardener : Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime. Google Cloud Run for Anthos : Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications. Google Cloud Run : A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds. IBM Cloud Code Engine : A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions. Red Hat Openshift Serverless : enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications. TriggerMesh Cloud : A fully-managed Knative and Tekton cloud-native integration platform. With support for AWS, Azure and Google event sources and brokers.","title":"Using a Knative-based Offering"},{"location":"install/knative-offerings/#knative-offerings","text":"Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products. Please check with each of these vendors for what is or is not supported. Here is a list of commercial Knative products (alphabetically): Gardener : Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime. Google Cloud Run for Anthos : Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications. Google Cloud Run : A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds. IBM Cloud Code Engine : A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions. Red Hat Openshift Serverless : enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications. TriggerMesh Cloud : A fully-managed Knative and Tekton cloud-native integration platform. With support for AWS, Azure and Google event sources and brokers.","title":"Knative Offerings"},{"location":"install/knative-with-operators/","text":"Knative Operator installation \u00b6 Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster. NOTE: The Knative Operator is still in Alpha phase. It has not been tested in a production environment, and should be used for development or test purposes only. Prerequisites \u00b6 You have a cluster that uses Kubernetes v1.18 or newer. You have installed the kubectl CLI. If you have only one node in your cluster, you will need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you will need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. Your Kubernetes cluster must have access to the internet, since Kubernetes needs to be able to fetch images, such as gcr.io/knative-releases/knative.dev/operator/cmd/operator:<version> . You have installed Istio . Installing the latest release \u00b6 You can find information about the different released versions of the Knative Operator on the Releases page . Install the latest stable Operator release: kubectl apply -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml Verify your installation \u00b6 Verify your installation: kubectl get deployment knative-operator If the operator is installed correctly, the deployment shows a Ready status: NAME READY UP-TO-DATE AVAILABLE AGE knative-operator 1/1 1 1 19h Track the log \u00b6 Track the log of the operator: kubectl logs -f deploy/knative-operator Installing the Knative Serving component \u00b6 Create and apply the Knative Serving CR: \u00b6 Install Current Serving Install Current Serving (default) You can install the latest available Knative Serving in the operator by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving If you do not specify a version by using spec_version, the operator defaults to the latest available version. Install Future Knative Serving You do not need to upgrade the operator to a newer version to install new releases of Knative Serving. If Knative Serving launches a new version, e.g. $spec_version , you can install it by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version manifests: - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-core.yaml - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-hpa.yaml - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-post-install-jobs.yaml - URL: https://github.com/knative/net-istio/releases/download/v${VERSION}/net-istio.yaml The field $spec_version is used to set the version of Knative Serving. Replace $spec_version with the correct version number. The tag ${VERSION} is automatically replaced with the version number from spec_version by the operator. The field spec.manifests is used to specify one or multiple URL links of Knative Serving component. Do not forget to add the valid URL of the Knative network ingress plugin. Knative Serving component is still tightly-coupled with a network ingress plugin in the operator. As in the above example, you can use net-istio . The ordering of the URLs is critical. Put the manifest you want to apply first on the top. Install Customized Knative Serving The operator provides you the flexibility to install customized Knative Serving based your own requirements. As long as the manifests of customized Knative Serving are accessible to the operator, they can be installed. There are two modes available for you to install the customized manifests: overwrite mode and append mode. With the overwrite mode, you need to define all the manifests for Knative Serving to install, because the operator will no long install any available default manifests. With the append mode, you only need to define your customized manifests, and the customized manifests are installed, after default manifests are applied. You can use the overwrite mode to customize all the Knative Serving manifests. For example, the version of the customized Knative Serving is $spec_version , and it is available at https://my-serving/serving.yaml . You choose net-istio as the ingress plugin, which is available at https://my-net-istio/net-istio.yaml . You can create the content of Serving CR as below to install your Knative Serving and the istio ingress: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version manifests: - URL: https://my-serving/serving.yaml - URL: https://my-net-istio/net-istio.yaml You can make the customized Knative Serving available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Serving, by leveraging both spec_version and spec.manifests . Do not skip either field. You can use the append mode to add your customized manifests into the default manifests. For example, you only customize a few resources, and make them available at https://my-serving/serving-custom.yaml . You still need to install the default Knative Serving. In this case, you can create the content of Serving CR as below: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version additionalManifests: - URL: https://my-serving/serving-custom.yaml Knative operator will install the default manifests of Knative Serving at the version $spec_version , and then install your customized manifests based on them. Verify the Knative Serving deployment: \u00b6 ``` kubectl get deployment -n knative-serving ``` If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show `READY` status. Here is a sample output: ``` NAME READY UP-TO-DATE AVAILABLE AGE activator 1/1 1 1 18s autoscaler 1/1 1 1 18s autoscaler-hpa 1/1 1 1 14s controller 1/1 1 1 18s istio-webhook 1/1 1 1 12s networking-istio 1/1 1 1 12s webhook 1/1 1 1 17s ``` 3. Check the status of Knative Serving Custom Resource: ``` kubectl get KnativeServing knative-serving -n knative-serving ``` If Knative Serving is successfully installed, you should see: ``` NAME VERSION READY REASON knative-serving <version number> True ``` Installing with Different Networking Layers \u00b6 Installing the Knative Serving component with different network layers Knative Operator can configure Knative Serving component with different network layer options. Istio is the default network layer, if the ingress is not specified in the Knative Serving CR. Click on each tab below to see how you can configure Knative Serving with different ingresses: Ambassador The following commands install Ambassador and enable its Knative integration. Create a namespace to install Ambassador in: kubectl create namespace ambassador Install Ambassador: kubectl apply --namespace ambassador \\ --filename https://getambassador.io/yaml/ambassador/ambassador-crds.yaml \\ --filename https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml \\ --filename https://getambassador.io/yaml/ambassador/ambassador-service.yaml Give Ambassador the required permissions: kubectl patch clusterrolebinding ambassador -p '{\"subjects\":[{\"kind\": \"ServiceAccount\", \"name\": \"ambassador\", \"namespace\": \"ambassador\"}]}' Enable Knative support in Ambassador: kubectl set env --namespace ambassador deployments/ambassador AMBASSADOR_KNATIVE_SUPPORT = true To configure Knative Serving to use Ambassador, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: network: ingress.class: \"ambassador.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace ambassador get service ambassador Save this for configuring DNS below. Contour The following commands install Contour and enable its Knative integration. Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: contour: enabled: true config: network: ingress.class: \"contour.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace contour-external get service envoy Save this for configuring DNS below. Gloo For a detailed guide on Gloo integration, see Installing Gloo for Knative in the Gloo documentation. The following commands install Gloo and enable its Knative integration. Make sure glooctl is installed (version 1.3.x and higher recommended): glooctl version If it is not installed, you can install the latest version using: curl -sL https://run.solo.io/gloo/install | sh export PATH = $HOME /.gloo/bin: $PATH Or following the Gloo CLI install instructions . Install Gloo and the Knative integration: glooctl install knative --install-knative = false To configure Knative Serving to use Gloo, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving EOF There is no need to configure the ingress class to use the gloo. Fetch the External IP or CNAME: glooctl proxy url --name knative-external-proxy Save this for configuring DNS below. Kong The following commands install Kong and enable its Knative integration. Install Kong Ingress Controller: kubectl apply --filename https://raw.githubusercontent.com/Kong/kubernetes-ingress-controller/0.9.x/deploy/single/all-in-one-dbless.yaml To configure Knative Serving to use Kong, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: network: ingress.class: \"kong\" EOF Fetch the External IP or CNAME: kubectl --namespace kong get service kong-proxy Save this for configuring DNS below. Kourier The following commands install Kourier and enable its Knative integration. To configure Knative Serving to use Kourier, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: kourier: enabled: true config: network: ingress.class: \"kourier.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace knative-serving get service kourier Save this for configuring DNS below. Configure DNS \u00b6 Magic DNS (xip.io) We ship a simple Kubernetes Job called \"default domain\" that will (see caveats) configure Knative Serving to use xip.io as the default DNS suffix. kubectl apply --filename https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Caveat : This will only work if the cluster LoadBalancer service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like Minikube. For these, see \"Real DNS\" or \"Temporary DNS\". Real DNS To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, add the following section into your existing Serving CR, and apply it: # Replace knative.example.com with your domain suffix apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: \"knative.example.com\" : \"\" ... Temporary DNS If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (xip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer in section 3 above, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the above helloworld-go application, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output should be: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Monitor the Knative components until all of the components show a STATUS of Running or Completed : kubectl get pods --namespace knative-serving Installing the Knative Eventing component \u00b6 Create and apply the Knative Eventing CR: You can install the latest available Knative Eventing in the operator by applying a YAML file containing the following: Install Current Evenintg (default) apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing If you do not specify a version by using spec.version, the operator defaults to the latest available version. Install Future Knative Eventing You do not need to upgrade the operator to a newer version to install new releases of Knative Eventing. If Knative Eventing launches a new version, e.g. $spec_version , you can install it by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version manifests: - URL: https://github.com/knative/eventing/releases/download/v${VERSION}/eventing.yaml - URL: https://github.com/knative/eventing/releases/download/v${VERSION}/eventing-post-install-jobs.yaml The field spec.version is used to set the version of Knative Eventing. Replace $spec_version with the correct version number. The tag ${VERSION} is automatically replaced with the version number from spec.version by the operator. The field spec.manifests is used to specify one or multiple URL links of Knative Eventing component. Do not forget to add the valid URL of the Knative network ingress plugin. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. Install Customized Knative Evening The operator provides you the flexibility to install customized Knative Eventing based your own requirements. As long as the manifests of customized Knative Eventing are accessible to the operator, they can be installed. There are two modes available for you to install the customized manifests: overwrite mode and append mode. With the overwrite mode, you need to define all the manifests for Knative Eventing to install, because the operator will no long install any available default manifests. With the append mode, you only need to define your customized manifests, and the customized manifests are installed, after default manifests are applied. You can use the overwrite mode to customize all the Knative Eventing manifests. For example, the version of the customized Knative Eventing is $spec_version , and it is available at https://my-eventing/eventing.yaml . You can create the content of Eventing CR as below to install your Knative Eventing: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version manifests: - URL: https://my-eventing/eventing.yaml You can make the customized Knative Eventing available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both spec.version and spec.manifests . Do not skip either field. You can use the append mode to add your customized manifests into the default manifests. For example, you only customize a few resources, and make them available at https://my-eventing/eventing-custom.yaml . You still need to install the default Knative eventing. In this case, you can create the content of Eventing CR as below: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version additionalManifests: - URL: https://my-eventing/eventing-custom.yaml Knative operator will install the default manifests of Knative Eventing at the version $spec_version , and then install your customized manifests based on them. Verify the Knative Eventing deployment: \u00b6 kubectl get deployment -n knative-eventing If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE broker-controller 1/1 1 1 63s broker-filter 1/1 1 1 62s broker-ingress 1/1 1 1 62s eventing-controller 1/1 1 1 67s eventing-webhook 1/1 1 1 67s imc-controller 1/1 1 1 59s imc-dispatcher 1/1 1 1 59s mt-broker-controller 1/1 1 1 62s Check the status of Knative Eventing Custom Resource: \u00b6 kubectl get KnativeEventing knative-eventing -n knative-eventing If Knative Eventing is successfully installed, you should see: NAME VERSION READY REASON knative-eventing <version number> True Uninstall Knative \u00b6 Removing the Knative Serving component \u00b6 Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving Removing Knative Eventing component \u00b6 Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work. Removing the Knative Operator: \u00b6 If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/ What's next \u00b6 Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"Installing with the Operator"},{"location":"install/knative-with-operators/#knative-operator-installation","text":"Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster. NOTE: The Knative Operator is still in Alpha phase. It has not been tested in a production environment, and should be used for development or test purposes only.","title":"Knative Operator installation"},{"location":"install/knative-with-operators/#prerequisites","text":"You have a cluster that uses Kubernetes v1.18 or newer. You have installed the kubectl CLI. If you have only one node in your cluster, you will need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you will need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. Your Kubernetes cluster must have access to the internet, since Kubernetes needs to be able to fetch images, such as gcr.io/knative-releases/knative.dev/operator/cmd/operator:<version> . You have installed Istio .","title":"Prerequisites"},{"location":"install/knative-with-operators/#installing-the-latest-release","text":"You can find information about the different released versions of the Knative Operator on the Releases page . Install the latest stable Operator release: kubectl apply -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml","title":"Installing the latest release"},{"location":"install/knative-with-operators/#verify-your-installation","text":"Verify your installation: kubectl get deployment knative-operator If the operator is installed correctly, the deployment shows a Ready status: NAME READY UP-TO-DATE AVAILABLE AGE knative-operator 1/1 1 1 19h","title":"Verify your installation"},{"location":"install/knative-with-operators/#track-the-log","text":"Track the log of the operator: kubectl logs -f deploy/knative-operator","title":"Track the log"},{"location":"install/knative-with-operators/#installing-the-knative-serving-component","text":"","title":"Installing the Knative Serving component"},{"location":"install/knative-with-operators/#create-and-apply-the-knative-serving-cr","text":"Install Current Serving Install Current Serving (default) You can install the latest available Knative Serving in the operator by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving If you do not specify a version by using spec_version, the operator defaults to the latest available version. Install Future Knative Serving You do not need to upgrade the operator to a newer version to install new releases of Knative Serving. If Knative Serving launches a new version, e.g. $spec_version , you can install it by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version manifests: - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-core.yaml - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-hpa.yaml - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-post-install-jobs.yaml - URL: https://github.com/knative/net-istio/releases/download/v${VERSION}/net-istio.yaml The field $spec_version is used to set the version of Knative Serving. Replace $spec_version with the correct version number. The tag ${VERSION} is automatically replaced with the version number from spec_version by the operator. The field spec.manifests is used to specify one or multiple URL links of Knative Serving component. Do not forget to add the valid URL of the Knative network ingress plugin. Knative Serving component is still tightly-coupled with a network ingress plugin in the operator. As in the above example, you can use net-istio . The ordering of the URLs is critical. Put the manifest you want to apply first on the top. Install Customized Knative Serving The operator provides you the flexibility to install customized Knative Serving based your own requirements. As long as the manifests of customized Knative Serving are accessible to the operator, they can be installed. There are two modes available for you to install the customized manifests: overwrite mode and append mode. With the overwrite mode, you need to define all the manifests for Knative Serving to install, because the operator will no long install any available default manifests. With the append mode, you only need to define your customized manifests, and the customized manifests are installed, after default manifests are applied. You can use the overwrite mode to customize all the Knative Serving manifests. For example, the version of the customized Knative Serving is $spec_version , and it is available at https://my-serving/serving.yaml . You choose net-istio as the ingress plugin, which is available at https://my-net-istio/net-istio.yaml . You can create the content of Serving CR as below to install your Knative Serving and the istio ingress: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version manifests: - URL: https://my-serving/serving.yaml - URL: https://my-net-istio/net-istio.yaml You can make the customized Knative Serving available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Serving, by leveraging both spec_version and spec.manifests . Do not skip either field. You can use the append mode to add your customized manifests into the default manifests. For example, you only customize a few resources, and make them available at https://my-serving/serving-custom.yaml . You still need to install the default Knative Serving. In this case, you can create the content of Serving CR as below: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version additionalManifests: - URL: https://my-serving/serving-custom.yaml Knative operator will install the default manifests of Knative Serving at the version $spec_version , and then install your customized manifests based on them.","title":"Create and apply the Knative Serving CR:"},{"location":"install/knative-with-operators/#verify-the-knative-serving-deployment","text":"``` kubectl get deployment -n knative-serving ``` If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show `READY` status. Here is a sample output: ``` NAME READY UP-TO-DATE AVAILABLE AGE activator 1/1 1 1 18s autoscaler 1/1 1 1 18s autoscaler-hpa 1/1 1 1 14s controller 1/1 1 1 18s istio-webhook 1/1 1 1 12s networking-istio 1/1 1 1 12s webhook 1/1 1 1 17s ``` 3. Check the status of Knative Serving Custom Resource: ``` kubectl get KnativeServing knative-serving -n knative-serving ``` If Knative Serving is successfully installed, you should see: ``` NAME VERSION READY REASON knative-serving <version number> True ```","title":"Verify the Knative Serving deployment:"},{"location":"install/knative-with-operators/#installing-with-different-networking-layers","text":"Installing the Knative Serving component with different network layers Knative Operator can configure Knative Serving component with different network layer options. Istio is the default network layer, if the ingress is not specified in the Knative Serving CR. Click on each tab below to see how you can configure Knative Serving with different ingresses: Ambassador The following commands install Ambassador and enable its Knative integration. Create a namespace to install Ambassador in: kubectl create namespace ambassador Install Ambassador: kubectl apply --namespace ambassador \\ --filename https://getambassador.io/yaml/ambassador/ambassador-crds.yaml \\ --filename https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml \\ --filename https://getambassador.io/yaml/ambassador/ambassador-service.yaml Give Ambassador the required permissions: kubectl patch clusterrolebinding ambassador -p '{\"subjects\":[{\"kind\": \"ServiceAccount\", \"name\": \"ambassador\", \"namespace\": \"ambassador\"}]}' Enable Knative support in Ambassador: kubectl set env --namespace ambassador deployments/ambassador AMBASSADOR_KNATIVE_SUPPORT = true To configure Knative Serving to use Ambassador, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: network: ingress.class: \"ambassador.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace ambassador get service ambassador Save this for configuring DNS below. Contour The following commands install Contour and enable its Knative integration. Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: contour: enabled: true config: network: ingress.class: \"contour.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace contour-external get service envoy Save this for configuring DNS below. Gloo For a detailed guide on Gloo integration, see Installing Gloo for Knative in the Gloo documentation. The following commands install Gloo and enable its Knative integration. Make sure glooctl is installed (version 1.3.x and higher recommended): glooctl version If it is not installed, you can install the latest version using: curl -sL https://run.solo.io/gloo/install | sh export PATH = $HOME /.gloo/bin: $PATH Or following the Gloo CLI install instructions . Install Gloo and the Knative integration: glooctl install knative --install-knative = false To configure Knative Serving to use Gloo, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving EOF There is no need to configure the ingress class to use the gloo. Fetch the External IP or CNAME: glooctl proxy url --name knative-external-proxy Save this for configuring DNS below. Kong The following commands install Kong and enable its Knative integration. Install Kong Ingress Controller: kubectl apply --filename https://raw.githubusercontent.com/Kong/kubernetes-ingress-controller/0.9.x/deploy/single/all-in-one-dbless.yaml To configure Knative Serving to use Kong, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: network: ingress.class: \"kong\" EOF Fetch the External IP or CNAME: kubectl --namespace kong get service kong-proxy Save this for configuring DNS below. Kourier The following commands install Kourier and enable its Knative integration. To configure Knative Serving to use Kourier, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: kourier: enabled: true config: network: ingress.class: \"kourier.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace knative-serving get service kourier Save this for configuring DNS below.","title":"Installing with Different Networking Layers"},{"location":"install/knative-with-operators/#configure-dns","text":"Magic DNS (xip.io) We ship a simple Kubernetes Job called \"default domain\" that will (see caveats) configure Knative Serving to use xip.io as the default DNS suffix. kubectl apply --filename https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Caveat : This will only work if the cluster LoadBalancer service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like Minikube. For these, see \"Real DNS\" or \"Temporary DNS\". Real DNS To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, add the following section into your existing Serving CR, and apply it: # Replace knative.example.com with your domain suffix apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: \"knative.example.com\" : \"\" ... Temporary DNS If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (xip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer in section 3 above, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the above helloworld-go application, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output should be: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Monitor the Knative components until all of the components show a STATUS of Running or Completed : kubectl get pods --namespace knative-serving","title":"Configure DNS"},{"location":"install/knative-with-operators/#installing-the-knative-eventing-component","text":"Create and apply the Knative Eventing CR: You can install the latest available Knative Eventing in the operator by applying a YAML file containing the following: Install Current Evenintg (default) apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing If you do not specify a version by using spec.version, the operator defaults to the latest available version. Install Future Knative Eventing You do not need to upgrade the operator to a newer version to install new releases of Knative Eventing. If Knative Eventing launches a new version, e.g. $spec_version , you can install it by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version manifests: - URL: https://github.com/knative/eventing/releases/download/v${VERSION}/eventing.yaml - URL: https://github.com/knative/eventing/releases/download/v${VERSION}/eventing-post-install-jobs.yaml The field spec.version is used to set the version of Knative Eventing. Replace $spec_version with the correct version number. The tag ${VERSION} is automatically replaced with the version number from spec.version by the operator. The field spec.manifests is used to specify one or multiple URL links of Knative Eventing component. Do not forget to add the valid URL of the Knative network ingress plugin. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. Install Customized Knative Evening The operator provides you the flexibility to install customized Knative Eventing based your own requirements. As long as the manifests of customized Knative Eventing are accessible to the operator, they can be installed. There are two modes available for you to install the customized manifests: overwrite mode and append mode. With the overwrite mode, you need to define all the manifests for Knative Eventing to install, because the operator will no long install any available default manifests. With the append mode, you only need to define your customized manifests, and the customized manifests are installed, after default manifests are applied. You can use the overwrite mode to customize all the Knative Eventing manifests. For example, the version of the customized Knative Eventing is $spec_version , and it is available at https://my-eventing/eventing.yaml . You can create the content of Eventing CR as below to install your Knative Eventing: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version manifests: - URL: https://my-eventing/eventing.yaml You can make the customized Knative Eventing available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both spec.version and spec.manifests . Do not skip either field. You can use the append mode to add your customized manifests into the default manifests. For example, you only customize a few resources, and make them available at https://my-eventing/eventing-custom.yaml . You still need to install the default Knative eventing. In this case, you can create the content of Eventing CR as below: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version additionalManifests: - URL: https://my-eventing/eventing-custom.yaml Knative operator will install the default manifests of Knative Eventing at the version $spec_version , and then install your customized manifests based on them.","title":"Installing the Knative Eventing component"},{"location":"install/knative-with-operators/#verify-the-knative-eventing-deployment","text":"kubectl get deployment -n knative-eventing If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE broker-controller 1/1 1 1 63s broker-filter 1/1 1 1 62s broker-ingress 1/1 1 1 62s eventing-controller 1/1 1 1 67s eventing-webhook 1/1 1 1 67s imc-controller 1/1 1 1 59s imc-dispatcher 1/1 1 1 59s mt-broker-controller 1/1 1 1 62s","title":"Verify the Knative Eventing deployment:"},{"location":"install/knative-with-operators/#check-the-status-of-knative-eventing-custom-resource","text":"kubectl get KnativeEventing knative-eventing -n knative-eventing If Knative Eventing is successfully installed, you should see: NAME VERSION READY REASON knative-eventing <version number> True","title":"Check the status of Knative Eventing Custom Resource:"},{"location":"install/knative-with-operators/#uninstall-knative","text":"","title":"Uninstall Knative"},{"location":"install/knative-with-operators/#removing-the-knative-serving-component","text":"Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving","title":"Removing the Knative Serving component"},{"location":"install/knative-with-operators/#removing-knative-eventing-component","text":"Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.","title":"Removing Knative Eventing component"},{"location":"install/knative-with-operators/#removing-the-knative-operator","text":"If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"Removing the Knative Operator:"},{"location":"install/knative-with-operators/#whats-next","text":"Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"What's next"},{"location":"install/prerequisites/","text":"Prerequisites \u00b6 Tip If you're installing Knative for the first time, a better place to start may be Getting Started . Before installing Knative, you must meet the following prerequisites: System requirements \u00b6 For prototyping purposes , Knative will work on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 2 CPU and 4GB of memory. For production purposes , it is recommended that: - If you have only one node in your cluster, you will need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. - If you have multiple nodes in your cluster, for each node you will need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. NOTE: The system requirements provided are recommendations only. The requirements for your installation may vary, depending on whether you use optional components, such as a networking layer. Prerequisites \u00b6 Before installation, you must meet the following prerequisites: You have a cluster that uses Kubernetes v1.18 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, since Kubernetes needs to be able to fetch images. (To pull from a private registry, see Deploying images from a private container registry ) Next Steps: Install Knative Serving and Eventing \u00b6 You can install the Serving component, Eventing component, or both on your cluster. If you're planning on installing both, we recommend starting with Knative Serving. Installing Knative Serving using YAML files Installing Knative Eventing using YAML files","title":"Prerequisites"},{"location":"install/prerequisites/#prerequisites","text":"Tip If you're installing Knative for the first time, a better place to start may be Getting Started . Before installing Knative, you must meet the following prerequisites:","title":"Prerequisites"},{"location":"install/prerequisites/#system-requirements","text":"For prototyping purposes , Knative will work on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 2 CPU and 4GB of memory. For production purposes , it is recommended that: - If you have only one node in your cluster, you will need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. - If you have multiple nodes in your cluster, for each node you will need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. NOTE: The system requirements provided are recommendations only. The requirements for your installation may vary, depending on whether you use optional components, such as a networking layer.","title":"System requirements"},{"location":"install/prerequisites/#prerequisites_1","text":"Before installation, you must meet the following prerequisites: You have a cluster that uses Kubernetes v1.18 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, since Kubernetes needs to be able to fetch images. (To pull from a private registry, see Deploying images from a private container registry )","title":"Prerequisites"},{"location":"install/prerequisites/#next-steps-install-knative-serving-and-eventing","text":"You can install the Serving component, Eventing component, or both on your cluster. If you're planning on installing both, we recommend starting with Knative Serving. Installing Knative Serving using YAML files Installing Knative Eventing using YAML files","title":"Next Steps: Install Knative Serving and Eventing"},{"location":"install/upgrade-installation-with-operator/","text":"Upgrading your installation with Knative operator \u00b6 The Knative operator supports a straightforward upgrade process. It supports upgrading the Knative component by a single minor version number. For example, if you have v0.17 installed, you must upgrade to v0.18 before attempting to upgrade to v0.19. The attribute spec.version is the only field you need to change in the Serving or Eventing CR to perform an upgrade. You do not need to specify the version in terms of the patch number, because the Knative Operator will match the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify 0.19 to upgrade to the latest v0.19 release. There is no need to know the exact patch number. The Knative Operator implements a minus 3 principle to support the Knative versions, which means the current version of the Operator can support Knative with the version back 3 in terms of the minor number. For example, if the current version of the Operator is 0.19.x, it bundles and supports the installation of Knative with the versions, 0.16.x, 0.17.x, 0.18.x and 0.19.x. Before you begin \u00b6 Knative Operator maximizes the automation for the upgrade process, all you need to know is the current version of your Knative, the target version of your Knative, and the namespaces for your Knative installation. In the following instruction, Knative Serving and the Serving custom resource are installed in the knative-serving namespace, and Knative Eventing and the Eventing custom resource are installed in the knative-eventing namespace. Check the current version of the installed Knative \u00b6 If you want to check the version of the installed Knative Serving, you can apply the following command: kubectl get KnativeServing knative-serving --namespace knative-serving If your current version for Knative Serving is 0.19.x, you will get the result as below: NAME VERSION READY REASON knative-serving 0.19.0 True As Knative only supports the upgrade with one single minor version, the target version is 0.20 for Knative Serving. The status True means the Serving CR and Knative Serving are in good status. If you want to check the version of the installed Knative Eventing, you can apply the following command: kubectl get KnativeEventing knative-eventing --namespace knative-eventing If your current version for Knative Eventing is 0.19.x, you will get the result as below: NAME VERSION READY REASON knative-eventing 0.19.0 True As Knative only supports the upgrade with one single minor version, the target version is 0.20 for Knative Eventing. The status True means the Eventing CR and Knative Eventing are in good status. Performing the upgrade \u00b6 To upgrade, apply the Operator CRs with the same spec, but a different target version for the attribute spec.version . If your existing Serving CR is as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.19\" then apply the following CR to upgrade to 0.20: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.20\" If your existing Eventing CR is as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.19\" then apply the following CR to upgrade to 0.20: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.20\" Verifying the upgrade \u00b6 To confirm that your Knative components have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s networking-istio-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s You can also verify the status of Knative by checking the CRs: kubectl get KnativeServing knative-serving --namespace knative-serving kubectl get KnativeEventing knative-eventing --namespace knative-eventing These commands return something similar to: NAME VERSION READY REASON knative-serving 0.20.0 True NAME VERSION READY REASON knative-eventing 0.20.0 True Rollback \u00b6 If the upgrade fails, you can always have a rollback solution to restore your Knative to the current version. If your current version is 0.19, you can apply the following CR to restore Knative Serving and Eventing. For Knative Serving: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.19\" For Knative Eventing: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.19\"","title":"Upgrading your installation with Knative operator"},{"location":"install/upgrade-installation-with-operator/#upgrading-your-installation-with-knative-operator","text":"The Knative operator supports a straightforward upgrade process. It supports upgrading the Knative component by a single minor version number. For example, if you have v0.17 installed, you must upgrade to v0.18 before attempting to upgrade to v0.19. The attribute spec.version is the only field you need to change in the Serving or Eventing CR to perform an upgrade. You do not need to specify the version in terms of the patch number, because the Knative Operator will match the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify 0.19 to upgrade to the latest v0.19 release. There is no need to know the exact patch number. The Knative Operator implements a minus 3 principle to support the Knative versions, which means the current version of the Operator can support Knative with the version back 3 in terms of the minor number. For example, if the current version of the Operator is 0.19.x, it bundles and supports the installation of Knative with the versions, 0.16.x, 0.17.x, 0.18.x and 0.19.x.","title":"Upgrading your installation with Knative operator"},{"location":"install/upgrade-installation-with-operator/#before-you-begin","text":"Knative Operator maximizes the automation for the upgrade process, all you need to know is the current version of your Knative, the target version of your Knative, and the namespaces for your Knative installation. In the following instruction, Knative Serving and the Serving custom resource are installed in the knative-serving namespace, and Knative Eventing and the Eventing custom resource are installed in the knative-eventing namespace.","title":"Before you begin"},{"location":"install/upgrade-installation-with-operator/#check-the-current-version-of-the-installed-knative","text":"If you want to check the version of the installed Knative Serving, you can apply the following command: kubectl get KnativeServing knative-serving --namespace knative-serving If your current version for Knative Serving is 0.19.x, you will get the result as below: NAME VERSION READY REASON knative-serving 0.19.0 True As Knative only supports the upgrade with one single minor version, the target version is 0.20 for Knative Serving. The status True means the Serving CR and Knative Serving are in good status. If you want to check the version of the installed Knative Eventing, you can apply the following command: kubectl get KnativeEventing knative-eventing --namespace knative-eventing If your current version for Knative Eventing is 0.19.x, you will get the result as below: NAME VERSION READY REASON knative-eventing 0.19.0 True As Knative only supports the upgrade with one single minor version, the target version is 0.20 for Knative Eventing. The status True means the Eventing CR and Knative Eventing are in good status.","title":"Check the current version of the installed Knative"},{"location":"install/upgrade-installation-with-operator/#performing-the-upgrade","text":"To upgrade, apply the Operator CRs with the same spec, but a different target version for the attribute spec.version . If your existing Serving CR is as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.19\" then apply the following CR to upgrade to 0.20: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.20\" If your existing Eventing CR is as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.19\" then apply the following CR to upgrade to 0.20: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.20\"","title":"Performing the upgrade"},{"location":"install/upgrade-installation-with-operator/#verifying-the-upgrade","text":"To confirm that your Knative components have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s networking-istio-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s You can also verify the status of Knative by checking the CRs: kubectl get KnativeServing knative-serving --namespace knative-serving kubectl get KnativeEventing knative-eventing --namespace knative-eventing These commands return something similar to: NAME VERSION READY REASON knative-serving 0.20.0 True NAME VERSION READY REASON knative-eventing 0.20.0 True","title":"Verifying the upgrade"},{"location":"install/upgrade-installation-with-operator/#rollback","text":"If the upgrade fails, you can always have a rollback solution to restore your Knative to the current version. If your current version is 0.19, you can apply the following CR to restore Knative Serving and Eventing. For Knative Serving: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.19\" For Knative Eventing: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.19\"","title":"Rollback"},{"location":"install/upgrade-installation/","text":"Upgrading your installation \u00b6 To upgrade your Knative components and plugins, run the kubectl apply command to install the subsequent release. We support upgrading by a single minor version number. For example, if you have v0.14.0 installed, you must upgrade to v0.15.0 before attempting to upgrade to v0.16.0. To verify the version number you currently have installed, see Checking your installation version . If you installed Knative using the operator , the upgrade process will differ. See the operator upgrade guide to learn how to upgrade an installation managed by the operators. Before you begin \u00b6 Before upgrading, there are a few steps you must take to ensure a successful upgrade process. Identify breaking changes \u00b6 You should be aware of any breaking changes between your current and desired versions of Knative. Breaking changes between Knative versions are documented in the Knative release notes. Before upgrading, review the release notes for the target version to learn about any changes you might need to make to your Knative applications: Serving Eventing Eventing-Contrib Release notes are published with each version on the \"Releases\" page of their respective repositories in GitHub. View current pod status \u00b6 Before upgrading, view the status of the pods for the namespaces you plan on upgrading. This allows you to compare the before and after state of your namespace. For example, if you are upgrading Knative Serving and Eventing, enter the following commands to see the current state of each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing Upgrading plug-ins \u00b6 If you have a plug-in installed, make sure to upgrade it at the same time as you upgrade your Knative components. Run pre-install tools before upgrade \u00b6 In some upgrades there are some steps that must happen before the actual upgrade, and these are identified in the release notes. For example, upgrading from v0.15.0 to v0.16.0 for Eventing you have to run: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-pre-install-jobs.yaml Upgrade existing resources to the latest stored version \u00b6 Our custom resources are stored within Kubernetes at a particular version. As we introduce newer and remove older versions you'll need to migrate our resources to the designated stored version. This ensures removing older versions will succeed when upgrading. For the various subprojects - we have a K8s job to help operators perform this migration. The release notes for each release will explicitly whether a migration is required. ie. kubectl create --filename https://storage.googleapis.com/knative-nightly/serving/latest/serving-post-install-jobs.yaml Performing the upgrade \u00b6 To upgrade, apply the .yaml files for the subsequent minor versions of all your installed Knative components and features, remembering to only upgrade by one minor version at a time. For a cluster running v0.15.2 of the Knative Serving and Eventing components, the following command upgrades the installation to v0.16.0: kubectl apply --filename https://github.com/knative/serving/releases/download/v0.16.0/serving-core.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.16.0/eventing.yaml \\ Run post-install tools after the upgrade \u00b6 In some upgrades there are some steps that must happen after the actual upgrade, and these are identified in the release notes. For example, after upgrading from v0.15.0 to v0.16.0 for Eventing you should run: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-post-install-jobs.yaml Verifying the upgrade \u00b6 To confirm that your components and plugins have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-79f674fb7b-dgvss 2 /2 Running 0 43s autoscaler-96dc49858-b24bm 2 /2 Running 1 43s autoscaler-hpa-d887d4895-njtrb 1 /1 Running 0 43s controller-6bcdd87fd6-zz9fx 1 /1 Running 0 41s networking-istio-7fcd97cbf7-z2xmr 1 /1 Running 0 40s webhook-747b799559-4sj6q 1 /1 Running 0 41s NAME READY STATUS RESTARTS AGE eventing-controller-69ffcc6f7d-5l7th 1 /1 Running 0 83s eventing-webhook-6c56fcd86c-42dr8 1 /1 Running 0 81s imc-controller-6bcf5957b5-6ccp2 1 /1 Running 0 80s imc-dispatcher-f59b7c57-q9xcl 1 /1 Running 0 80s sources-controller-8596684d7b-jxkmd 1 /1 Running 0 83s If the age of all your pods has been reset and all pods are up and running, the upgrade was completed successfully. You might notice a status of Terminating for the old pods as they are cleaned up. If necessary, repeat the upgrade process until you reach your desired minor version number.","title":"Upgrading your installation"},{"location":"install/upgrade-installation/#upgrading-your-installation","text":"To upgrade your Knative components and plugins, run the kubectl apply command to install the subsequent release. We support upgrading by a single minor version number. For example, if you have v0.14.0 installed, you must upgrade to v0.15.0 before attempting to upgrade to v0.16.0. To verify the version number you currently have installed, see Checking your installation version . If you installed Knative using the operator , the upgrade process will differ. See the operator upgrade guide to learn how to upgrade an installation managed by the operators.","title":"Upgrading your installation"},{"location":"install/upgrade-installation/#before-you-begin","text":"Before upgrading, there are a few steps you must take to ensure a successful upgrade process.","title":"Before you begin"},{"location":"install/upgrade-installation/#identify-breaking-changes","text":"You should be aware of any breaking changes between your current and desired versions of Knative. Breaking changes between Knative versions are documented in the Knative release notes. Before upgrading, review the release notes for the target version to learn about any changes you might need to make to your Knative applications: Serving Eventing Eventing-Contrib Release notes are published with each version on the \"Releases\" page of their respective repositories in GitHub.","title":"Identify breaking changes"},{"location":"install/upgrade-installation/#view-current-pod-status","text":"Before upgrading, view the status of the pods for the namespaces you plan on upgrading. This allows you to compare the before and after state of your namespace. For example, if you are upgrading Knative Serving and Eventing, enter the following commands to see the current state of each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing","title":"View current pod status"},{"location":"install/upgrade-installation/#upgrading-plug-ins","text":"If you have a plug-in installed, make sure to upgrade it at the same time as you upgrade your Knative components.","title":"Upgrading plug-ins"},{"location":"install/upgrade-installation/#run-pre-install-tools-before-upgrade","text":"In some upgrades there are some steps that must happen before the actual upgrade, and these are identified in the release notes. For example, upgrading from v0.15.0 to v0.16.0 for Eventing you have to run: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-pre-install-jobs.yaml","title":"Run pre-install tools before upgrade"},{"location":"install/upgrade-installation/#upgrade-existing-resources-to-the-latest-stored-version","text":"Our custom resources are stored within Kubernetes at a particular version. As we introduce newer and remove older versions you'll need to migrate our resources to the designated stored version. This ensures removing older versions will succeed when upgrading. For the various subprojects - we have a K8s job to help operators perform this migration. The release notes for each release will explicitly whether a migration is required. ie. kubectl create --filename https://storage.googleapis.com/knative-nightly/serving/latest/serving-post-install-jobs.yaml","title":"Upgrade existing resources to the latest stored version"},{"location":"install/upgrade-installation/#performing-the-upgrade","text":"To upgrade, apply the .yaml files for the subsequent minor versions of all your installed Knative components and features, remembering to only upgrade by one minor version at a time. For a cluster running v0.15.2 of the Knative Serving and Eventing components, the following command upgrades the installation to v0.16.0: kubectl apply --filename https://github.com/knative/serving/releases/download/v0.16.0/serving-core.yaml \\ --filename https://github.com/knative/eventing/releases/download/v0.16.0/eventing.yaml \\","title":"Performing the upgrade"},{"location":"install/upgrade-installation/#run-post-install-tools-after-the-upgrade","text":"In some upgrades there are some steps that must happen after the actual upgrade, and these are identified in the release notes. For example, after upgrading from v0.15.0 to v0.16.0 for Eventing you should run: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-post-install-jobs.yaml","title":"Run post-install tools after the upgrade"},{"location":"install/upgrade-installation/#verifying-the-upgrade","text":"To confirm that your components and plugins have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-79f674fb7b-dgvss 2 /2 Running 0 43s autoscaler-96dc49858-b24bm 2 /2 Running 1 43s autoscaler-hpa-d887d4895-njtrb 1 /1 Running 0 43s controller-6bcdd87fd6-zz9fx 1 /1 Running 0 41s networking-istio-7fcd97cbf7-z2xmr 1 /1 Running 0 40s webhook-747b799559-4sj6q 1 /1 Running 0 41s NAME READY STATUS RESTARTS AGE eventing-controller-69ffcc6f7d-5l7th 1 /1 Running 0 83s eventing-webhook-6c56fcd86c-42dr8 1 /1 Running 0 81s imc-controller-6bcf5957b5-6ccp2 1 /1 Running 0 80s imc-dispatcher-f59b7c57-q9xcl 1 /1 Running 0 80s sources-controller-8596684d7b-jxkmd 1 /1 Running 0 83s If the age of all your pods has been reset and all pods are up and running, the upgrade was completed successfully. You might notice a status of Terminating for the old pods as they are cleaned up. If necessary, repeat the upgrade process until you reach your desired minor version number.","title":"Verifying the upgrade"},{"location":"install/collecting-logs/","text":"This document describes how to set up Fluent Bit , a log processor and forwarder, to collect your kubernetes logs in a central directory. This is not required for running Knative, but can be helpful with Knative Serving , which will automatically delete pods (and their associated logs) when they are no longer needed. Note that Fluent Bit supports exporting to a number of other log providers; if you already have an existing log provider (for example, Splunk, Datadog, ElasticSearch, or Stackdriver), then you may only need the second part of setting up and configuring log forwarders . Setting up log collection consists of two pieces: running a log forwarding DaemonSet on each node, and running a collector somewhere in the cluster (in our example, we use a StatefulSet which stores logs on a Kubernetes PersistentVolumeClaim, but you could also use a HostPath). Setting up the collector \u00b6 It's useful to set up the collector before the forwarders, because you'll need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready. The fluent-bit-collector.yaml defines a StatefulSet as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . You can apply the configuration with: kubectl apply --filename https://github.com/knative/docs/raw/main/docs/install/collecting-logs/fluent-bit-collector.yaml The default configuration will classify logs into Knative, apps (pods with an app= label which aren't Knative), and the default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Once the ConfigMap is updated, you'll need to restart Fluent Bit (for example, by deleting the pod and letting the StatefulSet recreate it). To access the logs through your web browser: kubectl port-forward --namespace logging service/log-collector 8080 :80 And then visit http://localhost:8080/. You can also open a shell in the nginx pod and search the logs using unix tools: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0 Setting up the forwarders \u00b6 For the most part, you can follow the Fluent Bit directions for installing on Kubernetes . Those directions will set up a Fluent Bit DaemonSet which forwards logs to ElasticSearch by default; when the directions call for creating the ConfigMap, you'll want to either replace the elasticsearch configuration with this fluent-bit-configmap.yaml or add the following block to the ConfigMap and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf . output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True If you are using a different log collection infrastructure (Splunk, for example), follow the directions in the FluentBit documentation on how to configure your forwarders.","title":"Index"},{"location":"install/collecting-logs/#setting-up-the-collector","text":"It's useful to set up the collector before the forwarders, because you'll need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready. The fluent-bit-collector.yaml defines a StatefulSet as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . You can apply the configuration with: kubectl apply --filename https://github.com/knative/docs/raw/main/docs/install/collecting-logs/fluent-bit-collector.yaml The default configuration will classify logs into Knative, apps (pods with an app= label which aren't Knative), and the default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Once the ConfigMap is updated, you'll need to restart Fluent Bit (for example, by deleting the pod and letting the StatefulSet recreate it). To access the logs through your web browser: kubectl port-forward --namespace logging service/log-collector 8080 :80 And then visit http://localhost:8080/. You can also open a shell in the nginx pod and search the logs using unix tools: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0","title":"Setting up the collector"},{"location":"install/collecting-logs/#setting-up-the-forwarders","text":"For the most part, you can follow the Fluent Bit directions for installing on Kubernetes . Those directions will set up a Fluent Bit DaemonSet which forwards logs to ElasticSearch by default; when the directions call for creating the ConfigMap, you'll want to either replace the elasticsearch configuration with this fluent-bit-configmap.yaml or add the following block to the ConfigMap and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf . output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True If you are using a different log collection infrastructure (Splunk, for example), follow the directions in the FluentBit documentation on how to configure your forwarders.","title":"Setting up the forwarders"},{"location":"install/operator/configuring-eventing-cr/","text":"Configuring the Eventing Operator custom resource \u00b6 You can configure the Knative Eventing operator by modifying settings in the KnativeEventing custom resource (CR). NOTE: Kubernetes spec level policies cannot be configured using the Knative Operators. Installing a specific version of Eventing \u00b6 Cluster administrators can install a specific version of Knative Eventing by using the spec.version field. For example, if you want to install Knative Eventing v0.19.0, you can apply the following KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : 0.19.0 If spec.version is not specified, the Knative Operator will install the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. If Knative Eventing is already managed by the Operator, updating the spec.version field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 0.18.x, you must upgrade to 0.19.x before upgrading to 0.20.x. Configuring Knative Eventing using ConfigMaps \u00b6 The Operator manages the Knative Eventing installation. It overwrites any updates to ConfigMaps which are used to configure Knative Eventing. The KnativeEventing CR allows you to set values for these ConfigMaps by using the Operator. All Knative Eventing ConfigMaps are created in the same namespace as the KnativeEventing CR. You can use the KnativeEventing CR as a unique entry point to edit all ConfigMaps. Knative Eventing has multiple ConfigMaps that are named with the prefix config- . The spec.config in the KnativeEventing CR has one <name> entry for each ConfigMap, named config-<name> , with a value which will be used for the ConfigMap data . Setting a default channel \u00b6 If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the default-ch-webhook ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 namespaceDefaults: my-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel spec: delivery: backoffDelay: PT0.5S backoffPolicy: exponential retry: 5 NOTE: The clusterDefault setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the namespaceDefaults setting. Setting the default channel for the broker \u00b6 If you are using a channel-based broker, you can change the default channel type for the broker from InMemoryChannel to KafkaChannel, by updating the config-br-default-channel ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1 Private repository and private secrets \u00b6 The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret . Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving. List of containers within each Deployment resource: Component Deployment name Container name Core eventing eventing-controller eventing-controller Core eventing eventing-webhook eventing-webhook Eventing Broker broker-controller eventing-controller In-Memory Channel imc-controller controller In-Memory Channel imc-dispatcher dispatcher The default field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example eventing-controller , you must use the override field to replace it, by specifying deployment/container as the unique key. Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the override field. Download images in a predefined format without secrets \u00b6 This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the example below: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are defined in the accepted format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . Push images to the following image tags: Deployment Container Docker image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest Define your the KnativeEventing CR with following content: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : default : docker.io/knative-images/${NAME}:latest override : broker-controller/eventing-controller : docker.io/knative-images-repo1/broker-eventing-controller:latest - `${NAME}` maps to the container name in each `Deployment` resource. - `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override` field to specify individually, by using `broker-controller/eventing-controller` as the key. Download images from different repositories without secrets \u00b6 If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR. For example, to define the following list of images: Deployment Container Docker Image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest The KnativeEventing CR must be modified to include the full list. For example: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable DISPATCHER_IMAGE , in the container controller , of the deployment imc-controller , and the target image is docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest , the KnativeEventing CR would be as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest DISPATCHER_IMAGE : docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest Download images with secrets \u00b6 If your image repository requires private secrets for access, you must append the imagePullSecrets attribute to the KnativeEventing CR. This example uses a secret named regcred . Refer to the Kubernetes documentation to create your own private secrets. After you create the secret, edit the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets requires a list of secrets. You can add multiple secrets to access the images: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ... Configuring the default broker class \u00b6 Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker. The field defaultBrokerClass indicates which class to use; if empty, the ChannelBasedBroker is used. The following example CR specifies MTChannelBasedBroker as the default: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : defaultBrokerClass : MTChannelBasedBroker System resource settings \u00b6 The KnativeEventing CR allows you to configure system resources for Knative system containers. Requests and limits can be configured for the following containers: eventing-controller eventing-webhook imc-controller imc-dispatcher mt-broker-ingress mt-broker-ingress mt-broker-controller To override resource settings for a specific container, you must create an entry in the spec.resources list with the container name and the Kubernetes resource settings . For example, the following KnativeEventing CR configures the eventing-webhook container to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : resources : - container : eventing-webhook requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi","title":"Configuring the Eventing Operator custom resource"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-eventing-operator-custom-resource","text":"You can configure the Knative Eventing operator by modifying settings in the KnativeEventing custom resource (CR). NOTE: Kubernetes spec level policies cannot be configured using the Knative Operators.","title":"Configuring the Eventing Operator custom resource"},{"location":"install/operator/configuring-eventing-cr/#installing-a-specific-version-of-eventing","text":"Cluster administrators can install a specific version of Knative Eventing by using the spec.version field. For example, if you want to install Knative Eventing v0.19.0, you can apply the following KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : 0.19.0 If spec.version is not specified, the Knative Operator will install the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. If Knative Eventing is already managed by the Operator, updating the spec.version field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 0.18.x, you must upgrade to 0.19.x before upgrading to 0.20.x.","title":"Installing a specific version of Eventing"},{"location":"install/operator/configuring-eventing-cr/#configuring-knative-eventing-using-configmaps","text":"The Operator manages the Knative Eventing installation. It overwrites any updates to ConfigMaps which are used to configure Knative Eventing. The KnativeEventing CR allows you to set values for these ConfigMaps by using the Operator. All Knative Eventing ConfigMaps are created in the same namespace as the KnativeEventing CR. You can use the KnativeEventing CR as a unique entry point to edit all ConfigMaps. Knative Eventing has multiple ConfigMaps that are named with the prefix config- . The spec.config in the KnativeEventing CR has one <name> entry for each ConfigMap, named config-<name> , with a value which will be used for the ConfigMap data .","title":"Configuring Knative Eventing using ConfigMaps"},{"location":"install/operator/configuring-eventing-cr/#setting-a-default-channel","text":"If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the default-ch-webhook ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 namespaceDefaults: my-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel spec: delivery: backoffDelay: PT0.5S backoffPolicy: exponential retry: 5 NOTE: The clusterDefault setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the namespaceDefaults setting.","title":"Setting a default channel"},{"location":"install/operator/configuring-eventing-cr/#setting-the-default-channel-for-the-broker","text":"If you are using a channel-based broker, you can change the default channel type for the broker from InMemoryChannel to KafkaChannel, by updating the config-br-default-channel ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1","title":"Setting the default channel for the broker"},{"location":"install/operator/configuring-eventing-cr/#private-repository-and-private-secrets","text":"The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret . Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving. List of containers within each Deployment resource: Component Deployment name Container name Core eventing eventing-controller eventing-controller Core eventing eventing-webhook eventing-webhook Eventing Broker broker-controller eventing-controller In-Memory Channel imc-controller controller In-Memory Channel imc-dispatcher dispatcher The default field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example eventing-controller , you must use the override field to replace it, by specifying deployment/container as the unique key. Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the override field.","title":"Private repository and private secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-in-a-predefined-format-without-secrets","text":"This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the example below: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are defined in the accepted format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . Push images to the following image tags: Deployment Container Docker image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest Define your the KnativeEventing CR with following content: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : default : docker.io/knative-images/${NAME}:latest override : broker-controller/eventing-controller : docker.io/knative-images-repo1/broker-eventing-controller:latest - `${NAME}` maps to the container name in each `Deployment` resource. - `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override` field to specify individually, by using `broker-controller/eventing-controller` as the key.","title":"Download images in a predefined format without secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-from-different-repositories-without-secrets","text":"If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR. For example, to define the following list of images: Deployment Container Docker Image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest The KnativeEventing CR must be modified to include the full list. For example: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable DISPATCHER_IMAGE , in the container controller , of the deployment imc-controller , and the target image is docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest , the KnativeEventing CR would be as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest DISPATCHER_IMAGE : docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest","title":"Download images from different repositories without secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-with-secrets","text":"If your image repository requires private secrets for access, you must append the imagePullSecrets attribute to the KnativeEventing CR. This example uses a secret named regcred . Refer to the Kubernetes documentation to create your own private secrets. After you create the secret, edit the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets requires a list of secrets. You can add multiple secrets to access the images: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ...","title":"Download images with secrets"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-default-broker-class","text":"Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker. The field defaultBrokerClass indicates which class to use; if empty, the ChannelBasedBroker is used. The following example CR specifies MTChannelBasedBroker as the default: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : defaultBrokerClass : MTChannelBasedBroker","title":"Configuring the default broker class"},{"location":"install/operator/configuring-eventing-cr/#system-resource-settings","text":"The KnativeEventing CR allows you to configure system resources for Knative system containers. Requests and limits can be configured for the following containers: eventing-controller eventing-webhook imc-controller imc-dispatcher mt-broker-ingress mt-broker-ingress mt-broker-controller To override resource settings for a specific container, you must create an entry in the spec.resources list with the container name and the Kubernetes resource settings . For example, the following KnativeEventing CR configures the eventing-webhook container to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : resources : - container : eventing-webhook requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi","title":"System resource settings"},{"location":"install/operator/configuring-serving-cr/","text":"Configuring the Serving Operator Custom Resource \u00b6 The Knative Serving operator can be configured with these options: Version Configuration Serving Configuration by ConfigMap Private repository and private secret SSL certificate for controller Knative ingress gateway Cluster local gateway High availability System Resource Settings Override system deployments Version Configuration \u00b6 Cluster administrators can install a specific version of Knative Serving by using the spec.version field. For example, if you want to install Knative Serving 0.16.0, you can apply the following KnativeServing custom resource: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: 0.16.0 If spec.version is not specified, the Knative Operator will install the latest available version of Knative Serving. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. For example, if the current version of the Knative Operator is 0.16.x, the earliest version of Knative Serving available through the Operator is 0.14.0. If Knative Serving is already managed by the Operator, updating the spec.version field in the KnativeServing resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version 0.14.x, you must upgrade to 0.15.x before upgrading to 0.16.x. Serving Configuration by ConfigMap \u00b6 The Operator manages the Knative Serving installation. It overwrites any updates to ConfigMaps which are used to configure Knative Serving. The KnativeServing custom resource (CR) allows you to set values for these ConfigMaps by using the Operator. Knative Serving has multiple ConfigMaps that are named with the prefix config- . The spec.config in the KnativeServing CR has one <name> entry for each ConfigMap, named config-<name> , with a value which will be used for the ConfigMap data . In the setup a custom domain example , you can see the content of the ConfigMap config-domain is: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: example.org: | selector: app: prod example.com: \"\" Using the operator, specify the ConfigMap config-domain using the operator CR: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: example.org: | selector: app: prod example.com: \"\" You can apply values to multiple ConfigMaps. This example sets stable-window to 60s in config-autoscaler as well as specifying config-domain : apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: example.org: | selector: app: prod example.com: \"\" autoscaler: stable-window: \"60s\" All the ConfigMaps are created in the same namespace as the operator CR. You can use the operator CR as the unique entry point to edit all of them. Private repository and private secrets \u00b6 You can use the spec.registry section of the operator CR to change the image references to point to a private registry or specify imagePullSecrets : default : this field defines a image reference template for all Knative images. The format is example-registry.io/custom/path/${NAME}:{CUSTOM-TAG} . If you use the same tag for all your images, the only difference is the image name. ${NAME} is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( activator , autoscaler , controller , webhook , autoscaler-hpa , networking-istio , and queue-proxy ), the default argument should be sufficient. override : a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by default . If a container's name does not match a key in override , the template in default is used. imagePullSecrets : a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details. Download images in a predefined format without secrets: \u00b6 This example shows how you can define custom image links that can be defined in the CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the example below: the custom tag v0.13.0 is used for all images all image links are accessible without using secrets images are pushed as docker.io/knative-images/${NAME}:{CUSTOM-TAG} First, you need to make sure your images pushed to the following image tags: Container Docker Image activator docker.io/knative-images/activator:v0.13.0 autoscaler docker.io/knative-images/autoscaler:v0.13.0 controller docker.io/knative-images/controller:v0.13.0 webhook docker.io/knative-images/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images/autoscaler-hpa:v0.13.0 networking-istio docker.io/knative-images/networking-istio:v0.13.0 queue-proxy docker.io/knative-images/queue-proxy:v0.13.0 Then, you need to define your operator CR with following content: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: default: docker.io/knative-images/${NAME}:v0.13.0 Download images individually without secrets: \u00b6 If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR. For example, to given the following images: Container Docker Image activator docker.io/knative-images-repo1/activator:v0.13.0 autoscaler docker.io/knative-images-repo2/autoscaler:v0.13.0 controller docker.io/knative-images-repo3/controller:v0.13.0 webhook docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 networking-istio docker.io/knative-images-repo6/prefix-networking-istio:v0.13.0 queue-proxy docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0 The operator CR should be modified to include the full list: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: override: activator: docker.io/knative-images-repo1/activator:v0.13.0 autoscaler: docker.io/knative-images-repo2/autoscaler:v0.13.0 controller: docker.io/knative-images-repo3/controller:v0.13.0 webhook: docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa: docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 networking-istio: docker.io/knative-images-repo6/prefix-networking-istio:v0.13.0 queue-proxy: docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0 Download images with secrets: \u00b6 If your image repository requires private secrets for access, include the imagePullSecrets attribute. This example uses a secret named regcred . You must create your own private secrets if these are required: From existing docker credentials From command line for docker credentials Create your own secret After you create this secret, edit your operator CR by appending the content below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: ... imagePullSecrets: - name: regcred The field imagePullSecrets expects a list of secrets. You can add multiple secrets to access the images as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: ... imagePullSecrets: - name: regcred - name: regcred-2 ... SSL certificate for controller \u00b6 To enable tag to digest resolution , the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret. Specify the following fields in spec.controller-custom-certs to select a custom registry certificate: name : the name of the ConfigMap or Secret. type : either the string \"ConfigMap\" or \"Secret\". If you create a ConfigMap named testCert containing the certificate, change your CR: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: controller-custom-certs: name: testCert type: ConfigMap Configuration of Knative ingress gateway \u00b6 To set up custom ingress gateway, follow Step 1: Create Gateway Service and Deployment Instance . Step 2: Update the Knative gateway \u00b6 Update spec.ingress.istio.knative-ingress-gateway to select the labels of the new ingress gateway: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-ingress-gateway: selector: custom: ingressgateway Step 3: Update Gateway ConfigMap \u00b6 Additionally, you will need to update the Istio ConfigMap: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-ingress-gateway: selector: custom: ingressgateway config: istio: gateway.knative-serving.knative-ingress-gateway: \"custom-ingressgateway.istio-system.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.. . Configuration of cluster local gateway \u00b6 Update spec.ingress.istio.knative-local-gateway to select the labels of the new cluster-local ingress gateway: Default local gateway name: \u00b6 Go through the guide here to use local cluster gateway, if you use the default gateway called knative-local-gateway . Non-default local gateway name: \u00b6 If you create custom local gateway with a name other than knative-local-gateway , update config.istio and the knative-local-gateway selector: This example shows a service and deployment knative-local-gateway in the namespace istio-system , with the label custom: custom-local-gw : apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-local-gateway: selector: custom: custom-local-gateway config: istio: local-gateway.knative-serving.knative-local-gateway: \"custom-local-gateway.istio-system.svc.cluster.local\" High availability \u00b6 By default, Knative Serving runs a single instance of each controller. The spec.high-availability field allows you to configure the number of replicas for the following leader-elected controllers: controller , autoscaler-hpa , networking-istio . This field also configures the HorizontalPodAutoscaler resources for the data plane ( activator ): The following configuration specifies a replica count of 3 for the controllers and a minimum of 3 activators (which may scale higher if needed): apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: high-availability: replicas: 3 System Resource Settings \u00b6 The operator custom resource allows you to configure system resources for the Knative system containers. Requests and limits can be configured for the following containers: activator , autoscaler , controller , webhook , autoscaler-hpa , networking-istio and queue-proxy . To override resource settings for a specific container, create an entry in the spec.resources list with the container name and the Kubernetes resource settings . For example, the following KnativeServing resource configures the activator to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: resources: - container: activator requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi If you would like to add another container autoscaler with the same configuration, you need to change your CR as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: resources: - container: activator requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi - container: autoscaler requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi Override system deployments \u00b6 If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in CR. Currently replicas , labels and annotations are supported. For example, the following KnativeServing resource overrides the webhook to have 3 replicass, mylabel: foo labels and myannotataions: bar annotations, while other system deployments have 2 replicas by spec.high-availability . apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: ks namespace: knative-serving spec: high-availability: replicas: 2 deployments: - name: webhook replicas: 3 labels: mylabel: foo annotations: myannotataions: bar NOTE: The labels and annotations settings override webhook's labels and annotations in deployment and pod both.","title":"Configuring the Serving Operator Custom Resource"},{"location":"install/operator/configuring-serving-cr/#configuring-the-serving-operator-custom-resource","text":"The Knative Serving operator can be configured with these options: Version Configuration Serving Configuration by ConfigMap Private repository and private secret SSL certificate for controller Knative ingress gateway Cluster local gateway High availability System Resource Settings Override system deployments","title":"Configuring the Serving Operator Custom Resource"},{"location":"install/operator/configuring-serving-cr/#version-configuration","text":"Cluster administrators can install a specific version of Knative Serving by using the spec.version field. For example, if you want to install Knative Serving 0.16.0, you can apply the following KnativeServing custom resource: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: 0.16.0 If spec.version is not specified, the Knative Operator will install the latest available version of Knative Serving. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. For example, if the current version of the Knative Operator is 0.16.x, the earliest version of Knative Serving available through the Operator is 0.14.0. If Knative Serving is already managed by the Operator, updating the spec.version field in the KnativeServing resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version 0.14.x, you must upgrade to 0.15.x before upgrading to 0.16.x.","title":"Version Configuration"},{"location":"install/operator/configuring-serving-cr/#serving-configuration-by-configmap","text":"The Operator manages the Knative Serving installation. It overwrites any updates to ConfigMaps which are used to configure Knative Serving. The KnativeServing custom resource (CR) allows you to set values for these ConfigMaps by using the Operator. Knative Serving has multiple ConfigMaps that are named with the prefix config- . The spec.config in the KnativeServing CR has one <name> entry for each ConfigMap, named config-<name> , with a value which will be used for the ConfigMap data . In the setup a custom domain example , you can see the content of the ConfigMap config-domain is: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: example.org: | selector: app: prod example.com: \"\" Using the operator, specify the ConfigMap config-domain using the operator CR: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: example.org: | selector: app: prod example.com: \"\" You can apply values to multiple ConfigMaps. This example sets stable-window to 60s in config-autoscaler as well as specifying config-domain : apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: example.org: | selector: app: prod example.com: \"\" autoscaler: stable-window: \"60s\" All the ConfigMaps are created in the same namespace as the operator CR. You can use the operator CR as the unique entry point to edit all of them.","title":"Serving Configuration by ConfigMap"},{"location":"install/operator/configuring-serving-cr/#private-repository-and-private-secrets","text":"You can use the spec.registry section of the operator CR to change the image references to point to a private registry or specify imagePullSecrets : default : this field defines a image reference template for all Knative images. The format is example-registry.io/custom/path/${NAME}:{CUSTOM-TAG} . If you use the same tag for all your images, the only difference is the image name. ${NAME} is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( activator , autoscaler , controller , webhook , autoscaler-hpa , networking-istio , and queue-proxy ), the default argument should be sufficient. override : a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by default . If a container's name does not match a key in override , the template in default is used. imagePullSecrets : a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details.","title":"Private repository and private secrets"},{"location":"install/operator/configuring-serving-cr/#download-images-in-a-predefined-format-without-secrets","text":"This example shows how you can define custom image links that can be defined in the CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the example below: the custom tag v0.13.0 is used for all images all image links are accessible without using secrets images are pushed as docker.io/knative-images/${NAME}:{CUSTOM-TAG} First, you need to make sure your images pushed to the following image tags: Container Docker Image activator docker.io/knative-images/activator:v0.13.0 autoscaler docker.io/knative-images/autoscaler:v0.13.0 controller docker.io/knative-images/controller:v0.13.0 webhook docker.io/knative-images/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images/autoscaler-hpa:v0.13.0 networking-istio docker.io/knative-images/networking-istio:v0.13.0 queue-proxy docker.io/knative-images/queue-proxy:v0.13.0 Then, you need to define your operator CR with following content: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: default: docker.io/knative-images/${NAME}:v0.13.0","title":"Download images in a predefined format without secrets:"},{"location":"install/operator/configuring-serving-cr/#download-images-individually-without-secrets","text":"If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR. For example, to given the following images: Container Docker Image activator docker.io/knative-images-repo1/activator:v0.13.0 autoscaler docker.io/knative-images-repo2/autoscaler:v0.13.0 controller docker.io/knative-images-repo3/controller:v0.13.0 webhook docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 networking-istio docker.io/knative-images-repo6/prefix-networking-istio:v0.13.0 queue-proxy docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0 The operator CR should be modified to include the full list: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: override: activator: docker.io/knative-images-repo1/activator:v0.13.0 autoscaler: docker.io/knative-images-repo2/autoscaler:v0.13.0 controller: docker.io/knative-images-repo3/controller:v0.13.0 webhook: docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa: docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 networking-istio: docker.io/knative-images-repo6/prefix-networking-istio:v0.13.0 queue-proxy: docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0","title":"Download images individually without secrets:"},{"location":"install/operator/configuring-serving-cr/#download-images-with-secrets","text":"If your image repository requires private secrets for access, include the imagePullSecrets attribute. This example uses a secret named regcred . You must create your own private secrets if these are required: From existing docker credentials From command line for docker credentials Create your own secret After you create this secret, edit your operator CR by appending the content below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: ... imagePullSecrets: - name: regcred The field imagePullSecrets expects a list of secrets. You can add multiple secrets to access the images as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: ... imagePullSecrets: - name: regcred - name: regcred-2 ...","title":"Download images with secrets:"},{"location":"install/operator/configuring-serving-cr/#ssl-certificate-for-controller","text":"To enable tag to digest resolution , the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret. Specify the following fields in spec.controller-custom-certs to select a custom registry certificate: name : the name of the ConfigMap or Secret. type : either the string \"ConfigMap\" or \"Secret\". If you create a ConfigMap named testCert containing the certificate, change your CR: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: controller-custom-certs: name: testCert type: ConfigMap","title":"SSL certificate for controller"},{"location":"install/operator/configuring-serving-cr/#configuration-of-knative-ingress-gateway","text":"To set up custom ingress gateway, follow Step 1: Create Gateway Service and Deployment Instance .","title":"Configuration of Knative ingress gateway"},{"location":"install/operator/configuring-serving-cr/#step-2-update-the-knative-gateway","text":"Update spec.ingress.istio.knative-ingress-gateway to select the labels of the new ingress gateway: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-ingress-gateway: selector: custom: ingressgateway","title":"Step 2: Update the Knative gateway"},{"location":"install/operator/configuring-serving-cr/#step-3-update-gateway-configmap","text":"Additionally, you will need to update the Istio ConfigMap: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-ingress-gateway: selector: custom: ingressgateway config: istio: gateway.knative-serving.knative-ingress-gateway: \"custom-ingressgateway.istio-system.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.. .","title":"Step 3: Update Gateway ConfigMap"},{"location":"install/operator/configuring-serving-cr/#configuration-of-cluster-local-gateway","text":"Update spec.ingress.istio.knative-local-gateway to select the labels of the new cluster-local ingress gateway:","title":"Configuration of cluster local gateway"},{"location":"install/operator/configuring-serving-cr/#default-local-gateway-name","text":"Go through the guide here to use local cluster gateway, if you use the default gateway called knative-local-gateway .","title":"Default local gateway name:"},{"location":"install/operator/configuring-serving-cr/#non-default-local-gateway-name","text":"If you create custom local gateway with a name other than knative-local-gateway , update config.istio and the knative-local-gateway selector: This example shows a service and deployment knative-local-gateway in the namespace istio-system , with the label custom: custom-local-gw : apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-local-gateway: selector: custom: custom-local-gateway config: istio: local-gateway.knative-serving.knative-local-gateway: \"custom-local-gateway.istio-system.svc.cluster.local\"","title":"Non-default local gateway name:"},{"location":"install/operator/configuring-serving-cr/#high-availability","text":"By default, Knative Serving runs a single instance of each controller. The spec.high-availability field allows you to configure the number of replicas for the following leader-elected controllers: controller , autoscaler-hpa , networking-istio . This field also configures the HorizontalPodAutoscaler resources for the data plane ( activator ): The following configuration specifies a replica count of 3 for the controllers and a minimum of 3 activators (which may scale higher if needed): apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: high-availability: replicas: 3","title":"High availability"},{"location":"install/operator/configuring-serving-cr/#system-resource-settings","text":"The operator custom resource allows you to configure system resources for the Knative system containers. Requests and limits can be configured for the following containers: activator , autoscaler , controller , webhook , autoscaler-hpa , networking-istio and queue-proxy . To override resource settings for a specific container, create an entry in the spec.resources list with the container name and the Kubernetes resource settings . For example, the following KnativeServing resource configures the activator to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: resources: - container: activator requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi If you would like to add another container autoscaler with the same configuration, you need to change your CR as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: resources: - container: activator requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi - container: autoscaler requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi","title":"System Resource Settings"},{"location":"install/operator/configuring-serving-cr/#override-system-deployments","text":"If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in CR. Currently replicas , labels and annotations are supported. For example, the following KnativeServing resource overrides the webhook to have 3 replicass, mylabel: foo labels and myannotataions: bar annotations, while other system deployments have 2 replicas by spec.high-availability . apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: ks namespace: knative-serving spec: high-availability: replicas: 2 deployments: - name: webhook replicas: 3 labels: mylabel: foo annotations: myannotataions: bar NOTE: The labels and annotations settings override webhook's labels and annotations in deployment and pod both.","title":"Override system deployments"},{"location":"reference/api/","text":"View the latest release \u00b6 The reference documentation for the latest release of the Knative is available at www.knative.dev . Source files \u00b6 The API source files are located at: Serving API Eventing API Generating API docs \u00b6 See the API build instructions in the Knative documentation maintainer section.","title":"Index"},{"location":"reference/api/#view-the-latest-release","text":"The reference documentation for the latest release of the Knative is available at www.knative.dev .","title":"View the latest release"},{"location":"reference/api/#source-files","text":"The API source files are located at: Serving API Eventing API","title":"Source files"},{"location":"reference/api/#generating-api-docs","text":"See the API build instructions in the Knative documentation maintainer section.","title":"Generating API docs"},{"location":"reference/api/serving/","text":"Packages: autoscaling.internal.knative.dev/v1alpha1 serving.knative.dev/v1 serving.knative.dev/v1alpha1 autoscaling.internal.knative.dev/v1alpha1 Package v1alpha1 contains the Autoscaling v1alpha1 API types. Resource Types: PodAutoscaler PodAutoscaler PodAutoscaler is a Knative abstraction that encapsulates the interface by which Knative components instantiate autoscalers. This definition is an abstraction that may be backed by multiple definitions. For more information, see the Knative Pluggability presentation: https://docs.google.com/presentation/d/10KWynvAJYuOEWy69VBa6bHJVCqIsz1TNdEKosNvcpPY/edit Field Description apiVersion string autoscaling.internal.knative.dev/v1alpha1 kind string PodAutoscaler metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec PodAutoscalerSpec (Optional) Spec holds the desired state of the PodAutoscaler (from the client). containerConcurrency int64 (Optional) ContainerConcurrency specifies the maximum allowed in-flight (concurrent) requests per container of the Revision. Defaults to 0 which means unlimited concurrency. scaleTargetRef Kubernetes core/v1.ObjectReference ScaleTargetRef defines the /scale-able resource that this PodAutoscaler is responsible for quickly right-sizing. reachability ReachabilityType (Optional) Reachability specifies whether or not the ScaleTargetRef can be reached (ie. has a route). Defaults to ReachabilityUnknown protocolType knative.dev/networking/pkg/apis/networking.ProtocolType The application-layer protocol. Matches ProtocolType inferred from the revision spec. status PodAutoscalerStatus (Optional) Status communicates the observed state of the PodAutoscaler (from the controller). Metric Metric represents a resource to configure the metric collector with. Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec MetricSpec (Optional) Spec holds the desired state of the Metric (from the client). stableWindow time.Duration StableWindow is the aggregation window for metrics in a stable state. panicWindow time.Duration PanicWindow is the aggregation window for metrics where quick reactions are needed. scrapeTarget string ScrapeTarget is the K8s service that publishes the metric endpoint. status MetricStatus (Optional) Status communicates the observed state of the Metric (from the controller). MetricSpec ( Appears on: Metric ) MetricSpec contains all values a metric collector needs to operate. Field Description stableWindow time.Duration StableWindow is the aggregation window for metrics in a stable state. panicWindow time.Duration PanicWindow is the aggregation window for metrics where quick reactions are needed. scrapeTarget string ScrapeTarget is the K8s service that publishes the metric endpoint. MetricStatus ( Appears on: Metric ) MetricStatus reflects the status of metric collection for this specific entity. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) PodAutoscalerSpec ( Appears on: PodAutoscaler ) PodAutoscalerSpec holds the desired state of the PodAutoscaler (from the client). Field Description containerConcurrency int64 (Optional) ContainerConcurrency specifies the maximum allowed in-flight (concurrent) requests per container of the Revision. Defaults to 0 which means unlimited concurrency. scaleTargetRef Kubernetes core/v1.ObjectReference ScaleTargetRef defines the /scale-able resource that this PodAutoscaler is responsible for quickly right-sizing. reachability ReachabilityType (Optional) Reachability specifies whether or not the ScaleTargetRef can be reached (ie. has a route). Defaults to ReachabilityUnknown protocolType knative.dev/networking/pkg/apis/networking.ProtocolType The application-layer protocol. Matches ProtocolType inferred from the revision spec. PodAutoscalerStatus ( Appears on: PodAutoscaler ) PodAutoscalerStatus communicates the observed state of the PodAutoscaler (from the controller). Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) serviceName string ServiceName is the K8s Service name that serves the revision, scaled by this PA. The service is created and owned by the ServerlessService object owned by this PA. metricsServiceName string MetricsServiceName is the K8s Service name that provides revision metrics. The service is managed by the PA object. desiredScale int32 DesiredScale shows the current desired number of replicas for the revision. actualScale int32 ActualScale shows the actual number of replicas for the revision. PodScalable PodScalable is a duck type that the resources referenced by the PodAutoscaler\u2019s ScaleTargetRef must implement. They must also implement the /scale sub-resource for use with /scale based implementations (e.g. HPA), but this further constrains the shape the referenced resources may take. Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec PodScalableSpec replicas int32 selector Kubernetes meta/v1.LabelSelector template Kubernetes core/v1.PodTemplateSpec status PodScalableStatus PodScalableSpec ( Appears on: PodScalable ) PodScalableSpec is the specification for the desired state of a PodScalable (or at least our shared portion). Field Description replicas int32 selector Kubernetes meta/v1.LabelSelector template Kubernetes core/v1.PodTemplateSpec PodScalableStatus ( Appears on: PodScalable ) PodScalableStatus is the observed state of a PodScalable (or at least our shared portion). Field Description replicas int32 ReachabilityType ( string alias) ( Appears on: PodAutoscalerSpec ) ReachabilityType is the enumeration type for the different states of reachability to the ScaleTarget of a PodAutoscaler Value Description \"Reachable\" ReachabilityReachable means the ScaleTarget is reachable, ie. it has an active route. \"\" ReachabilityUnknown means the reachability of the ScaleTarget is unknown. Used when the reachability cannot be determined, eg. during activation. \"Unreachable\" ReachabilityUnreachable means the ScaleTarget is not reachable, ie. it does not have an active route. serving.knative.dev/v1 Package v1 contains the Serving v1 API types. Resource Types: Configuration Revision Route Service Configuration Configuration represents the \u201cfloating HEAD\u201d of a linear history of Revisions. Users create new Revisions by updating the Configuration\u2019s spec. The \u201clatest created\u201d revision\u2019s name is available under status, as is the \u201clatest ready\u201d revision\u2019s name. See also: https://github.com/knative/serving/blob/main/docs/spec/overview.md#configuration Field Description apiVersion string serving.knative.dev/v1 kind string Configuration metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ConfigurationSpec (Optional) template RevisionTemplateSpec (Optional) Template holds the latest specification for the Revision to be stamped out. status ConfigurationStatus (Optional) Revision Revision is an immutable snapshot of code and configuration. A revision references a container image. Revisions are created by updates to a Configuration. See also: https://github.com/knative/serving/blob/main/docs/spec/overview.md#revision Field Description apiVersion string serving.knative.dev/v1 kind string Revision metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec RevisionSpec (Optional) PodSpec Kubernetes core/v1.PodSpec (Members of PodSpec are embedded into this type.) containerConcurrency int64 (Optional) ContainerConcurrency specifies the maximum allowed in-flight (concurrent) requests per container of the Revision. Defaults to 0 which means concurrency to the application is not limited, and the system decides the target concurrency for the autoscaler. timeoutSeconds int64 (Optional) TimeoutSeconds is the maximum duration in seconds that the request routing layer will wait for a request delivered to a container to begin replying (send network traffic). If unspecified, a system default will be provided. status RevisionStatus (Optional) Route Route is responsible for configuring ingress over a collection of Revisions. Some of the Revisions a Route distributes traffic over may be specified by referencing the Configuration responsible for creating them; in these cases the Route is additionally responsible for monitoring the Configuration for \u201clatest ready revision\u201d changes, and smoothly rolling out latest revisions. See also: https://github.com/knative/serving/blob/main/docs/spec/overview.md#route Field Description apiVersion string serving.knative.dev/v1 kind string Route metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec RouteSpec (Optional) Spec holds the desired state of the Route (from the client). traffic []TrafficTarget (Optional) Traffic specifies how to distribute traffic over a collection of revisions and configurations. status RouteStatus (Optional) Status communicates the observed state of the Route (from the controller). Service Service acts as a top-level container that manages a Route and Configuration which implement a network service. Service exists to provide a singular abstraction which can be access controlled, reasoned about, and which encapsulates software lifecycle decisions such as rollout policy and team resource ownership. Service acts only as an orchestrator of the underlying Routes and Configurations (much as a kubernetes Deployment orchestrates ReplicaSets), and its usage is optional but recommended. The Service\u2019s controller will track the statuses of its owned Configuration and Route, reflecting their statuses and conditions as its own. See also: https://github.com/knative/serving/blob/main/docs/spec/overview.md#service Field Description apiVersion string serving.knative.dev/v1 kind string Service metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ServiceSpec (Optional) ConfigurationSpec ConfigurationSpec (Members of ConfigurationSpec are embedded into this type.) ServiceSpec inlines an unrestricted ConfigurationSpec. RouteSpec RouteSpec (Members of RouteSpec are embedded into this type.) ServiceSpec inlines RouteSpec and restricts/defaults its fields via webhook. In particular, this spec can only reference this Service\u2019s configuration and revisions (which also influences defaults). status ServiceStatus (Optional) ConfigurationSpec ( Appears on: Configuration , ServiceSpec ) ConfigurationSpec holds the desired state of the Configuration (from the client). Field Description template RevisionTemplateSpec (Optional) Template holds the latest specification for the Revision to be stamped out. ConfigurationStatus ( Appears on: Configuration ) ConfigurationStatus communicates the observed state of the Configuration (from the controller). Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) ConfigurationStatusFields ConfigurationStatusFields (Members of ConfigurationStatusFields are embedded into this type.) ConfigurationStatusFields ( Appears on: ConfigurationStatus , ServiceStatus ) ConfigurationStatusFields holds the fields of Configuration\u2019s status that are not generally shared. This is defined separately and inlined so that other types can readily consume these fields via duck typing. Field Description latestReadyRevisionName string (Optional) LatestReadyRevisionName holds the name of the latest Revision stamped out from this Configuration that has had its \u201cReady\u201d condition become \u201cTrue\u201d. latestCreatedRevisionName string (Optional) LatestCreatedRevisionName is the last revision that was created from this Configuration. It might not be ready yet, for that use LatestReadyRevisionName. ContainerStatus ( Appears on: RevisionStatus ) ContainerStatus holds the information of container name and image digest value Field Description name string imageDigest string RevisionSpec ( Appears on: Revision , RevisionTemplateSpec ) RevisionSpec holds the desired state of the Revision (from the client). Field Description PodSpec Kubernetes core/v1.PodSpec (Members of PodSpec are embedded into this type.) containerConcurrency int64 (Optional) ContainerConcurrency specifies the maximum allowed in-flight (concurrent) requests per container of the Revision. Defaults to 0 which means concurrency to the application is not limited, and the system decides the target concurrency for the autoscaler. timeoutSeconds int64 (Optional) TimeoutSeconds is the maximum duration in seconds that the request routing layer will wait for a request delivered to a container to begin replying (send network traffic). If unspecified, a system default will be provided. RevisionStatus ( Appears on: Revision ) RevisionStatus communicates the observed state of the Revision (from the controller). Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) serviceName string (Optional) ServiceName holds the name of a core Kubernetes Service resource that load balances over the pods backing this Revision. Deprecated: revision service name is effectively equal to the revision name, as per #10540. 0.23 \u2014 stop populating 0.25 \u2014 remove. logUrl string (Optional) LogURL specifies the generated logging url for this particular revision based on the revision url template specified in the controller\u2019s config. imageDigest string (Optional) DeprecatedImageDigest holds the resolved digest for the image specified within .Spec.Container.Image. The digest is resolved during the creation of Revision. This field holds the digest value regardless of whether a tag or digest was originally specified in the Container object. It may be empty if the image comes from a registry listed to skip resolution. If multiple containers specified then DeprecatedImageDigest holds the digest for serving container. DEPRECATED: Use ContainerStatuses instead. TODO(savitaashture) Remove deprecatedImageDigest. ref https://kubernetes.io/docs/reference/using-api/deprecation-policy for deprecation. containerStatuses []ContainerStatus (Optional) ContainerStatuses is a slice of images present in .Spec.Container[*].Image to their respective digests and their container name. The digests are resolved during the creation of Revision. ContainerStatuses holds the container name and image digests for both serving and non serving containers. ref: http://bit.ly/image-digests actualReplicas int32 (Optional) ActualReplicas reflects the amount of ready pods running this revision. desiredReplicas int32 (Optional) DesiredReplicas reflects the desired amount of pods running this revision. RevisionTemplateSpec ( Appears on: ConfigurationSpec ) RevisionTemplateSpec describes the data a revision should have when created from a template. Based on: https://github.com/kubernetes/api/blob/e771f807/core/v1/types.go#L3179-L3190 Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec RevisionSpec (Optional) PodSpec Kubernetes core/v1.PodSpec (Members of PodSpec are embedded into this type.) containerConcurrency int64 (Optional) ContainerConcurrency specifies the maximum allowed in-flight (concurrent) requests per container of the Revision. Defaults to 0 which means concurrency to the application is not limited, and the system decides the target concurrency for the autoscaler. timeoutSeconds int64 (Optional) TimeoutSeconds is the maximum duration in seconds that the request routing layer will wait for a request delivered to a container to begin replying (send network traffic). If unspecified, a system default will be provided. RouteSpec ( Appears on: Route , ServiceSpec ) RouteSpec holds the desired state of the Route (from the client). Field Description traffic []TrafficTarget (Optional) Traffic specifies how to distribute traffic over a collection of revisions and configurations. RouteStatus ( Appears on: Route ) RouteStatus communicates the observed state of the Route (from the controller). Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) RouteStatusFields RouteStatusFields (Members of RouteStatusFields are embedded into this type.) RouteStatusFields ( Appears on: RouteStatus , ServiceStatus ) RouteStatusFields holds the fields of Route\u2019s status that are not generally shared. This is defined separately and inlined so that other types can readily consume these fields via duck typing. Field Description url knative.dev/pkg/apis.URL (Optional) URL holds the url that will distribute traffic over the provided traffic targets. It generally has the form http[s]://{route-name}.{route-namespace}.{cluster-level-suffix} address knative.dev/pkg/apis/duck/v1.Addressable (Optional) Address holds the information needed for a Route to be the target of an event. traffic []TrafficTarget (Optional) Traffic holds the configured traffic distribution. These entries will always contain RevisionName references. When ConfigurationName appears in the spec, this will hold the LatestReadyRevisionName that we last observed. RoutingState ( string alias) RoutingState represents states of a revision with regards to serving a route. Value Description \"active\" RoutingStateActive is a state for a revision which is actively referenced by a Route. \"pending\" RoutingStatePending is a state after a revision is created, but before its routing state has been determined. It is treated like active for the purposes of revision garbage collection. \"reserve\" RoutingStateReserve is a state for a revision which is no longer referenced by a Route, and is scaled down, but may be rapidly pinned to a route to be made active again. \"\" RoutingStateUnset is the empty value for routing state, this state is unexpected. ServiceSpec ( Appears on: Service ) ServiceSpec represents the configuration for the Service object. A Service\u2019s specification is the union of the specifications for a Route and Configuration. The Service restricts what can be expressed in these fields, e.g. the Route must reference the provided Configuration; however, these limitations also enable friendlier defaulting, e.g. Route never needs a Configuration name, and may be defaulted to the appropriate \u201crun latest\u201d spec. Field Description ConfigurationSpec ConfigurationSpec (Members of ConfigurationSpec are embedded into this type.) ServiceSpec inlines an unrestricted ConfigurationSpec. RouteSpec RouteSpec (Members of RouteSpec are embedded into this type.) ServiceSpec inlines RouteSpec and restricts/defaults its fields via webhook. In particular, this spec can only reference this Service\u2019s configuration and revisions (which also influences defaults). ServiceStatus ( Appears on: Service ) ServiceStatus represents the Status stanza of the Service resource. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) ConfigurationStatusFields ConfigurationStatusFields (Members of ConfigurationStatusFields are embedded into this type.) In addition to inlining ConfigurationSpec, we also inline the fields specific to ConfigurationStatus. RouteStatusFields RouteStatusFields (Members of RouteStatusFields are embedded into this type.) In addition to inlining RouteSpec, we also inline the fields specific to RouteStatus. TrafficTarget ( Appears on: RouteSpec , RouteStatusFields ) TrafficTarget holds a single entry of the routing table for a Route. Field Description tag string (Optional) Tag is optionally used to expose a dedicated url for referencing this target exclusively. revisionName string (Optional) RevisionName of a specific revision to which to send this portion of traffic. This is mutually exclusive with ConfigurationName. configurationName string (Optional) ConfigurationName of a configuration to whose latest revision we will send this portion of traffic. When the \u201cstatus.latestReadyRevisionName\u201d of the referenced configuration changes, we will automatically migrate traffic from the prior \u201clatest ready\u201d revision to the new one. This field is never set in Route\u2019s status, only its spec. This is mutually exclusive with RevisionName. latestRevision bool (Optional) LatestRevision may be optionally provided to indicate that the latest ready Revision of the Configuration should be used for this traffic target. When provided LatestRevision must be true if RevisionName is empty; it must be false when RevisionName is non-empty. percent int64 (Optional) Percent indicates that percentage based routing should be used and the value indicates the percent of traffic that is be routed to this Revision or Configuration. 0 (zero) mean no traffic, 100 means all traffic. When percentage based routing is being used the follow rules apply: - the sum of all percent values must equal 100 - when not specified, the implied value for percent is zero for that particular Revision or Configuration url knative.dev/pkg/apis.URL (Optional) URL displays the URL for accessing named traffic targets. URL is displayed in status, and is disallowed on spec. URL must contain a scheme (e.g. http://) and a hostname, but may not contain anything else (e.g. basic auth, url path, etc.) serving.knative.dev/v1alpha1 Package v1alpha1 contains the v1alpha1 versions of the serving apis. Api versions allow the api contract for a resource to be changed while keeping backward compatibility by support multiple concurrent versions of the same resource Resource Types: DomainMapping DomainMapping DomainMapping is a mapping from a custom hostname to an Addressable. Field Description apiVersion string serving.knative.dev/v1alpha1 kind string DomainMapping metadata Kubernetes meta/v1.ObjectMeta (Optional) Standard object\u2019s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata Refer to the Kubernetes API documentation for the fields of the metadata field. spec DomainMappingSpec (Optional) Spec is the desired state of the DomainMapping. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status ref knative.dev/pkg/apis/duck/v1.KReference Ref specifies the target of the Domain Mapping. The object identified by the Ref must be an Addressable with a URL of the form {name}.{namespace}.{domain} where {domain} is the cluster domain, and {name} and {namespace} are the name and namespace of a Kubernetes Service. This contract is satisfied by Knative types such as Knative Services and Knative Routes, and by Kubernetes Services. status DomainMappingStatus (Optional) Status is the current state of the DomainMapping. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status CannotConvertError CannotConvertError is returned when a field cannot be converted. Field Description Message string Field string DomainMappingSpec ( Appears on: DomainMapping ) DomainMappingSpec describes the DomainMapping the user wishes to exist. Field Description ref knative.dev/pkg/apis/duck/v1.KReference Ref specifies the target of the Domain Mapping. The object identified by the Ref must be an Addressable with a URL of the form {name}.{namespace}.{domain} where {domain} is the cluster domain, and {name} and {namespace} are the name and namespace of a Kubernetes Service. This contract is satisfied by Knative types such as Knative Services and Knative Routes, and by Kubernetes Services. DomainMappingStatus ( Appears on: DomainMapping ) DomainMappingStatus describes the current state of the DomainMapping. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) url knative.dev/pkg/apis.URL (Optional) URL is the URL of this DomainMapping. address knative.dev/pkg/apis/duck/v1.Addressable (Optional) Address holds the information needed for a DomainMapping to be the target of an event. Generated with gen-crd-api-reference-docs on git commit 813aa6596 .","title":"Serving"},{"location":"reference/api/eventing/eventing/","text":"Packages: duck.knative.dev/v1 duck.knative.dev/v1beta1 eventing.knative.dev/v1 eventing.knative.dev/v1beta1 flows.knative.dev/v1 flows.knative.dev/v1beta1 messaging.knative.dev/v1 messaging.knative.dev/v1beta1 sources.knative.dev/v1 sources.knative.dev/v1alpha1 sources.knative.dev/v1alpha2 sources.knative.dev/v1beta1 sources.knative.dev/v1beta2 duck.knative.dev/v1 Package v1 is the v1 version of the API. Resource Types: BackoffPolicyType ( string alias) ( Appears on: DeliverySpec ) BackoffPolicyType is the type for backoff policies Value Description \"exponential\" Exponential backoff policy \"linear\" Linear backoff policy Channelable Channelable is a skeleton type wrapping Subscribable and Addressable in the manner we expect resource writers defining compatible resources to embed it. We will typically use this type to deserialize Channelable ObjectReferences and access their subscription and address data. This is not a real resource. Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ChannelableSpec Spec is the part where the Channelable fulfills the Subscribable contract. SubscribableSpec SubscribableSpec (Members of SubscribableSpec are embedded into this type.) delivery DeliverySpec (Optional) DeliverySpec contains the default delivery spec for each subscription to this Channelable. Each subscription delivery spec, if any, overrides this global delivery spec. status ChannelableStatus ChannelableSpec ( Appears on: Channelable , ChannelSpec , InMemoryChannelSpec ) ChannelableSpec contains Spec of the Channelable object Field Description SubscribableSpec SubscribableSpec (Members of SubscribableSpec are embedded into this type.) delivery DeliverySpec (Optional) DeliverySpec contains the default delivery spec for each subscription to this Channelable. Each subscription delivery spec, if any, overrides this global delivery spec. ChannelableStatus ( Appears on: Channelable , ChannelStatus , InMemoryChannelStatus ) ChannelableStatus contains the Status of a Channelable object. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the part where the Channelable fulfills the Addressable contract. SubscribableStatus SubscribableStatus (Members of SubscribableStatus are embedded into this type.) Subscribers is populated with the statuses of each of the Channelable\u2019s subscribers. deadLetterChannel knative.dev/pkg/apis/duck/v1.KReference (Optional) DeadLetterChannel is a KReference and is set by the channel when it supports native error handling via a channel Failed messages are delivered here. DeliverySpec ( Appears on: ChannelableSpec , SubscriberSpec , BrokerSpec , TriggerSpec , TriggerSpec , ParallelBranch , SequenceStep , SubscriptionSpec ) DeliverySpec contains the delivery options for event senders, such as channelable and source. Field Description deadLetterSink knative.dev/pkg/apis/duck/v1.Destination (Optional) DeadLetterSink is the sink receiving event that could not be sent to a destination. retry int32 (Optional) Retry is the minimum number of retries the sender should attempt when sending an event before moving it to the dead letter sink. backoffPolicy BackoffPolicyType (Optional) BackoffPolicy is the retry backoff policy (linear, exponential). backoffDelay string (Optional) BackoffDelay is the delay before retrying. More information on Duration format: - https://www.iso.org/iso-8601-date-and-time-format.html - https://en.wikipedia.org/wiki/ISO_8601 For linear policy, backoff delay is backoffDelay* . For exponential policy, backoff delay is backoffDelay*2^ . DeliveryStatus DeliveryStatus contains the Status of an object supporting delivery options. Field Description deadLetterChannel knative.dev/pkg/apis/duck/v1.KReference (Optional) DeadLetterChannel is a KReference that is the reference to the native, platform specific channel where failed events are sent to. Subscribable Subscribable is a skeleton type wrapping Subscribable in the manner we expect resource writers defining compatible resources to embed it. We will typically use this type to deserialize SubscribableType ObjectReferences and access the Subscription data. This is not a real resource. Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SubscribableSpec SubscribableSpec is the part where Subscribable object is configured as to be compatible with Subscribable contract. subscribers []SubscriberSpec This is the list of subscriptions for this subscribable. status SubscribableStatus SubscribableStatus is the part where SubscribableStatus object is configured as to be compatible with Subscribable contract. SubscribableSpec ( Appears on: ChannelableSpec , Subscribable ) SubscribableSpec shows how we expect folks to embed Subscribable in their Spec field. Field Description subscribers []SubscriberSpec This is the list of subscriptions for this subscribable. SubscribableStatus ( Appears on: ChannelableStatus , Subscribable ) SubscribableStatus is the schema for the subscribable\u2019s status portion of the status section of the resource. Field Description subscribers []SubscriberStatus This is the list of subscription\u2019s statuses for this channel. SubscriberSpec ( Appears on: SubscribableSpec ) SubscriberSpec defines a single subscriber to a Subscribable. At least one of SubscriberURI and ReplyURI must be present Field Description uid k8s.io/apimachinery/pkg/types.UID (Optional) UID is used to understand the origin of the subscriber. generation int64 (Optional) Generation of the origin of the subscriber with uid:UID. subscriberUri knative.dev/pkg/apis.URL (Optional) SubscriberURI is the endpoint for the subscriber replyUri knative.dev/pkg/apis.URL (Optional) ReplyURI is the endpoint for the reply delivery DeliverySpec (Optional) DeliverySpec contains options controlling the event delivery SubscriberStatus ( Appears on: SubscribableStatus ) SubscriberStatus defines the status of a single subscriber to a Channel. Field Description uid k8s.io/apimachinery/pkg/types.UID (Optional) UID is used to understand the origin of the subscriber. observedGeneration int64 (Optional) Generation of the origin of the subscriber with uid:UID. ready Kubernetes core/v1.ConditionStatus Status of the subscriber. message string (Optional) A human readable message indicating details of Ready status. duck.knative.dev/v1beta1 Package v1beta1 is the v1beta1 version of the API. Resource Types: BackoffPolicyType ( string alias) ( Appears on: DeliverySpec ) BackoffPolicyType is the type for backoff policies Value Description \"exponential\" Exponential backoff policy \"linear\" Linear backoff policy Channelable Channelable is a skeleton type wrapping Subscribable and Addressable in the manner we expect resource writers defining compatible resources to embed it. We will typically use this type to deserialize Channelable ObjectReferences and access their subscription and address data. This is not a real resource. Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ChannelableSpec Spec is the part where the Channelable fulfills the Subscribable contract. SubscribableSpec SubscribableSpec (Members of SubscribableSpec are embedded into this type.) delivery DeliverySpec (Optional) DeliverySpec contains options controlling the event delivery status ChannelableStatus ChannelableSpec ( Appears on: Channelable , ChannelSpec , InMemoryChannelSpec ) ChannelableSpec contains Spec of the Channelable object Field Description SubscribableSpec SubscribableSpec (Members of SubscribableSpec are embedded into this type.) delivery DeliverySpec (Optional) DeliverySpec contains options controlling the event delivery ChannelableStatus ( Appears on: Channelable , ChannelStatus , InMemoryChannelStatus ) ChannelableStatus contains the Status of a Channelable object. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the part where the Channelable fulfills the Addressable contract. SubscribableStatus SubscribableStatus (Members of SubscribableStatus are embedded into this type.) Subscribers is populated with the statuses of each of the Channelable\u2019s subscribers. deadLetterChannel knative.dev/pkg/apis/duck/v1.KReference (Optional) DeadLetterChannel is a KReference and is set by the channel when it supports native error handling via a channel Failed messages are delivered here. DeliverySpec ( Appears on: ChannelableSpec , SubscriberSpec , BrokerSpec , ParallelBranch , SequenceStep , SubscriptionSpec ) DeliverySpec contains the delivery options for event senders, such as channelable and source. Field Description deadLetterSink knative.dev/pkg/apis/duck/v1.Destination (Optional) DeadLetterSink is the sink receiving event that could not be sent to a destination. retry int32 (Optional) Retry is the minimum number of retries the sender should attempt when sending an event before moving it to the dead letter sink. backoffPolicy BackoffPolicyType (Optional) BackoffPolicy is the retry backoff policy (linear, exponential). backoffDelay string (Optional) BackoffDelay is the delay before retrying. More information on Duration format: - https://www.iso.org/iso-8601-date-and-time-format.html - https://en.wikipedia.org/wiki/ISO_8601 For linear policy, backoff delay is backoffDelay* . For exponential policy, backoff delay is backoffDelay*2^ . DeliveryStatus DeliveryStatus contains the Status of an object supporting delivery options. Field Description deadLetterChannel knative.dev/pkg/apis/duck/v1.KReference (Optional) DeadLetterChannel is a KReference that is the reference to the native, platform specific channel where failed events are sent to. Subscribable Subscribable is a skeleton type wrapping Subscribable in the manner we expect resource writers defining compatible resources to embed it. We will typically use this type to deserialize SubscribableType ObjectReferences and access the Subscription data. This is not a real resource. Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SubscribableSpec SubscribableSpec is the part where Subscribable object is configured as to be compatible with Subscribable contract. subscribers []SubscriberSpec This is the list of subscriptions for this subscribable. status SubscribableStatus SubscribableStatus is the part where SubscribableStatus object is configured as to be compatible with Subscribable contract. SubscribableSpec ( Appears on: ChannelableSpec , Subscribable ) SubscribableSpec shows how we expect folks to embed Subscribable in their Spec field. Field Description subscribers []SubscriberSpec This is the list of subscriptions for this subscribable. SubscribableStatus ( Appears on: ChannelableStatus , Subscribable ) SubscribableStatus is the schema for the subscribable\u2019s status portion of the status section of the resource. Field Description subscribers []SubscriberStatus This is the list of subscription\u2019s statuses for this channel. SubscriberSpec ( Appears on: SubscribableSpec ) SubscriberSpec defines a single subscriber to a Subscribable. At least one of SubscriberURI and ReplyURI must be present Field Description uid k8s.io/apimachinery/pkg/types.UID (Optional) UID is used to understand the origin of the subscriber. generation int64 (Optional) Generation of the origin of the subscriber with uid:UID. subscriberUri knative.dev/pkg/apis.URL (Optional) SubscriberURI is the endpoint for the subscriber replyUri knative.dev/pkg/apis.URL (Optional) ReplyURI is the endpoint for the reply delivery DeliverySpec (Optional) DeliverySpec contains options controlling the event delivery SubscriberStatus ( Appears on: SubscribableStatus ) SubscriberStatus defines the status of a single subscriber to a Channel. Field Description uid k8s.io/apimachinery/pkg/types.UID (Optional) UID is used to understand the origin of the subscriber. observedGeneration int64 (Optional) Generation of the origin of the subscriber with uid:UID. ready Kubernetes core/v1.ConditionStatus Status of the subscriber. message string (Optional) A human readable message indicating details of Ready status. eventing.knative.dev/v1 Package v1 is the v1 version of the API. Resource Types: Broker Trigger Broker Broker collects a pool of events that are consumable using Triggers. Brokers provide a well-known endpoint for event delivery that senders can use with minimal knowledge of the event routing strategy. Subscribers use Triggers to request delivery of events from a Broker\u2019s pool to a specific URL or Addressable endpoint. Field Description apiVersion string eventing.knative.dev/v1 kind string Broker metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec BrokerSpec Spec defines the desired state of the Broker. config knative.dev/pkg/apis/duck/v1.KReference (Optional) Config is a KReference to the configuration that specifies configuration options for this Broker. For example, this could be a pointer to a ConfigMap. delivery DeliverySpec (Optional) Delivery contains the delivery spec for each trigger to this Broker. Each trigger delivery spec, if any, overrides this global delivery spec. status BrokerStatus (Optional) Status represents the current state of the Broker. This data may be out of date. Trigger Trigger represents a request to have events delivered to a subscriber from a Broker\u2019s event pool. Field Description apiVersion string eventing.knative.dev/v1 kind string Trigger metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec TriggerSpec Spec defines the desired state of the Trigger. broker string Broker is the broker that this trigger receives events from. filter TriggerFilter (Optional) Filter is the filter to apply against all events from the Broker. Only events that pass this filter will be sent to the Subscriber. If not specified, will default to allowing all events. subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber is the addressable that receives events from the Broker that pass the Filter. It is required. delivery DeliverySpec (Optional) Delivery contains the delivery spec for this specific trigger. status TriggerStatus (Optional) Status represents the current state of the Trigger. This data may be out of date. BrokerSpec ( Appears on: Broker ) Field Description config knative.dev/pkg/apis/duck/v1.KReference (Optional) Config is a KReference to the configuration that specifies configuration options for this Broker. For example, this could be a pointer to a ConfigMap. delivery DeliverySpec (Optional) Delivery contains the delivery spec for each trigger to this Broker. Each trigger delivery spec, if any, overrides this global delivery spec. BrokerStatus ( Appears on: Broker ) BrokerStatus represents the current state of a Broker. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Broker that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. address knative.dev/pkg/apis/duck/v1.Addressable Broker is Addressable. It exposes the endpoint as an URI to get events delivered into the Broker mesh. TriggerFilter ( Appears on: TriggerSpec ) Field Description attributes TriggerFilterAttributes (Optional) Attributes filters events by exact match on event context attributes. Each key in the map is compared with the equivalent key in the event context. An event passes the filter if all values are equal to the specified values. Nested context attributes are not supported as keys. Only string values are supported. TriggerFilterAttributes ( map[string]string alias) ( Appears on: TriggerFilter ) TriggerFilterAttributes is a map of context attribute names to values for filtering by equality. Only exact matches will pass the filter. You can use the value \u201c to indicate all strings match. TriggerSpec ( Appears on: Trigger ) Field Description broker string Broker is the broker that this trigger receives events from. filter TriggerFilter (Optional) Filter is the filter to apply against all events from the Broker. Only events that pass this filter will be sent to the Subscriber. If not specified, will default to allowing all events. subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber is the addressable that receives events from the Broker that pass the Filter. It is required. delivery DeliverySpec (Optional) Delivery contains the delivery spec for this specific trigger. TriggerStatus ( Appears on: Trigger ) TriggerStatus represents the current state of a Trigger. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Trigger that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. subscriberUri knative.dev/pkg/apis.URL (Optional) SubscriberURI is the resolved URI of the receiver for this Trigger. eventing.knative.dev/v1beta1 Package v1beta1 is the v1beta1 version of the API. Resource Types: Broker EventType Trigger Broker Broker collects a pool of events that are consumable using Triggers. Brokers provide a well-known endpoint for event delivery that senders can use with minimal knowledge of the event routing strategy. Receivers use Triggers to request delivery of events from a Broker\u2019s pool to a specific URL or Addressable endpoint. Field Description apiVersion string eventing.knative.dev/v1beta1 kind string Broker metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec BrokerSpec Spec defines the desired state of the Broker. config knative.dev/pkg/apis/duck/v1.KReference (Optional) Config is a KReference to the configuration that specifies configuration options for this Broker. For example, this could be a pointer to a ConfigMap. delivery DeliverySpec (Optional) Delivery is the delivery specification for Events within the Broker mesh. This includes things like retries, DLQ, etc. status BrokerStatus (Optional) Status represents the current state of the Broker. This data may be out of date. EventType EventType represents a type of event that can be consumed from a Broker. Field Description apiVersion string eventing.knative.dev/v1beta1 kind string EventType metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec EventTypeSpec Spec defines the desired state of the EventType. type string Type represents the CloudEvents type. It is authoritative. source knative.dev/pkg/apis.URL (Optional) Source is a URI, it represents the CloudEvents source. schema knative.dev/pkg/apis.URL (Optional) Schema is a URI, it represents the CloudEvents schemaurl extension attribute. It may be a JSON schema, a protobuf schema, etc. It is optional. schemaData string (Optional) SchemaData allows the CloudEvents schema to be stored directly in the EventType. Content is dependent on the encoding. Optional attribute. The contents are not validated or manipulated by the system. broker string (Optional) TODO remove https://github.com/knative/eventing/issues/2750 Broker refers to the Broker that can provide the EventType. description string (Optional) Description is an optional field used to describe the EventType, in any meaningful way. status EventTypeStatus (Optional) Status represents the current state of the EventType. This data may be out of date. TODO might be removed https://github.com/knative/eventing/issues/2750 Trigger Trigger represents a request to have events delivered to a consumer from a Broker\u2019s event pool. Field Description apiVersion string eventing.knative.dev/v1beta1 kind string Trigger metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec TriggerSpec Spec defines the desired state of the Trigger. broker string Broker is the broker that this trigger receives events from. If not specified, will default to \u2018default\u2019. filter TriggerFilter (Optional) Filter is the filter to apply against all events from the Broker. Only events that pass this filter will be sent to the Subscriber. If not specified, will default to allowing all events. subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber is the addressable that receives events from the Broker that pass the Filter. It is required. delivery DeliverySpec (Optional) Delivery contains the delivery spec for this specific trigger. status TriggerStatus (Optional) Status represents the current state of the Trigger. This data may be out of date. BrokerSpec ( Appears on: Broker ) Field Description config knative.dev/pkg/apis/duck/v1.KReference (Optional) Config is a KReference to the configuration that specifies configuration options for this Broker. For example, this could be a pointer to a ConfigMap. delivery DeliverySpec (Optional) Delivery is the delivery specification for Events within the Broker mesh. This includes things like retries, DLQ, etc. BrokerStatus ( Appears on: Broker ) BrokerStatus represents the current state of a Broker. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. address knative.dev/pkg/apis/duck/v1.Addressable Broker is Addressable. It exposes the endpoint as an URI to get events delivered into the Broker mesh. EventTypeSpec ( Appears on: EventType ) Field Description type string Type represents the CloudEvents type. It is authoritative. source knative.dev/pkg/apis.URL (Optional) Source is a URI, it represents the CloudEvents source. schema knative.dev/pkg/apis.URL (Optional) Schema is a URI, it represents the CloudEvents schemaurl extension attribute. It may be a JSON schema, a protobuf schema, etc. It is optional. schemaData string (Optional) SchemaData allows the CloudEvents schema to be stored directly in the EventType. Content is dependent on the encoding. Optional attribute. The contents are not validated or manipulated by the system. broker string (Optional) TODO remove https://github.com/knative/eventing/issues/2750 Broker refers to the Broker that can provide the EventType. description string (Optional) Description is an optional field used to describe the EventType, in any meaningful way. EventTypeStatus ( Appears on: EventType ) EventTypeStatus represents the current state of a EventType. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. TriggerFilter ( Appears on: TriggerSpec ) Field Description attributes TriggerFilterAttributes (Optional) Attributes filters events by exact match on event context attributes. Each key in the map is compared with the equivalent key in the event context. An event passes the filter if all values are equal to the specified values. Nested context attributes are not supported as keys. Only string values are supported. TriggerFilterAttributes ( map[string]string alias) ( Appears on: TriggerFilter ) TriggerFilterAttributes is a map of context attribute names to values for filtering by equality. Only exact matches will pass the filter. You can use the value \u201c to indicate all strings match. TriggerSpec ( Appears on: Trigger ) Field Description broker string Broker is the broker that this trigger receives events from. If not specified, will default to \u2018default\u2019. filter TriggerFilter (Optional) Filter is the filter to apply against all events from the Broker. Only events that pass this filter will be sent to the Subscriber. If not specified, will default to allowing all events. subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber is the addressable that receives events from the Broker that pass the Filter. It is required. delivery DeliverySpec (Optional) Delivery contains the delivery spec for this specific trigger. TriggerStatus ( Appears on: Trigger ) TriggerStatus represents the current state of a Trigger. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. subscriberUri knative.dev/pkg/apis.URL (Optional) SubscriberURI is the resolved URI of the receiver for this Trigger. flows.knative.dev/v1 Package v1 is the v1 version of the API. Resource Types: Parallel Parallel defines conditional branches that will be wired in series through Channels and Subscriptions. Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ParallelSpec Spec defines the desired state of the Parallel. branches []ParallelBranch Branches is the list of Filter/Subscribers pairs. channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of a case Subscriber gets sent to when the case does not have a Reply status ParallelStatus (Optional) Status represents the current state of the Parallel. This data may be out of date. ParallelBranch ( Appears on: ParallelSpec ) Field Description filter knative.dev/pkg/apis/duck/v1.Destination (Optional) Filter is the expression guarding the branch subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber receiving the event when the filter passes reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of Subscriber of this case gets sent to. If not specified, sent the result to the Parallel Reply delivery DeliverySpec (Optional) Delivery is the delivery specification for events to the subscriber This includes things like retries, DLQ, etc. ParallelBranchStatus ( Appears on: ParallelStatus ) ParallelBranchStatus represents the current state of a Parallel branch Field Description filterSubscriptionStatus ParallelSubscriptionStatus FilterSubscriptionStatus corresponds to the filter subscription status. filterChannelStatus ParallelChannelStatus FilterChannelStatus corresponds to the filter channel status. subscriberSubscriptionStatus ParallelSubscriptionStatus SubscriptionStatus corresponds to the subscriber subscription status. ParallelChannelStatus ( Appears on: ParallelBranchStatus , ParallelStatus ) Field Description channel Kubernetes core/v1.ObjectReference Channel is the reference to the underlying channel. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Channel is ready or not. ParallelSpec ( Appears on: Parallel ) Field Description branches []ParallelBranch Branches is the list of Filter/Subscribers pairs. channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of a case Subscriber gets sent to when the case does not have a Reply ParallelStatus ( Appears on: Parallel ) ParallelStatus represents the current state of a Parallel. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. ingressChannelStatus ParallelChannelStatus IngressChannelStatus corresponds to the ingress channel status. branchStatuses []ParallelBranchStatus BranchStatuses is an array of corresponding to branch statuses. Matches the Spec.Branches array in the order. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the starting point to this Parallel. Sending to this will target the first subscriber. It generally has the form {channel}.{namespace}.svc.{cluster domain name} ParallelSubscriptionStatus ( Appears on: ParallelBranchStatus ) Field Description subscription Kubernetes core/v1.ObjectReference Subscription is the reference to the underlying Subscription. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Subscription is ready or not. Sequence Sequence defines a sequence of Subscribers that will be wired in series through Channels and Subscriptions. Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec SequenceSpec Spec defines the desired state of the Sequence. steps []SequenceStep Steps is the list of Destinations (processors / functions) that will be called in the order provided. Each step has its own delivery options channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of the last Subscriber gets sent to. status SequenceStatus (Optional) Status represents the current state of the Sequence. This data may be out of date. SequenceChannelStatus ( Appears on: SequenceStatus ) Field Description channel Kubernetes core/v1.ObjectReference Channel is the reference to the underlying channel. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Channel is ready or not. SequenceSpec ( Appears on: Sequence ) Field Description steps []SequenceStep Steps is the list of Destinations (processors / functions) that will be called in the order provided. Each step has its own delivery options channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of the last Subscriber gets sent to. SequenceStatus ( Appears on: Sequence ) SequenceStatus represents the current state of a Sequence. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. subscriptionStatuses []SequenceSubscriptionStatus SubscriptionStatuses is an array of corresponding Subscription statuses. Matches the Spec.Steps array in the order. channelStatuses []SequenceChannelStatus ChannelStatuses is an array of corresponding Channel statuses. Matches the Spec.Steps array in the order. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the starting point to this Sequence. Sending to this will target the first subscriber. It generally has the form {channel}.{namespace}.svc.{cluster domain name} SequenceStep ( Appears on: SequenceSpec ) Field Description Destination knative.dev/pkg/apis/duck/v1.Destination (Members of Destination are embedded into this type.) Subscriber receiving the step event delivery DeliverySpec (Optional) Delivery is the delivery specification for events to the subscriber This includes things like retries, DLQ, etc. SequenceSubscriptionStatus ( Appears on: SequenceStatus ) Field Description subscription Kubernetes core/v1.ObjectReference Subscription is the reference to the underlying Subscription. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Subscription is ready or not. flows.knative.dev/v1beta1 Package v1beta1 is the v1beta1 version of the API. Resource Types: Parallel Parallel defines conditional branches that will be wired in series through Channels and Subscriptions. Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ParallelSpec Spec defines the desired state of the Parallel. branches []ParallelBranch Branches is the list of Filter/Subscribers pairs. channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of a case Subscriber gets sent to when the case does not have a Reply status ParallelStatus (Optional) Status represents the current state of the Parallel. This data may be out of date. ParallelBranch ( Appears on: ParallelSpec ) Field Description filter knative.dev/pkg/apis/duck/v1.Destination (Optional) Filter is the expression guarding the branch subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber receiving the event when the filter passes reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of Subscriber of this case gets sent to. If not specified, sent the result to the Parallel Reply delivery DeliverySpec (Optional) Delivery is the delivery specification for events to the subscriber This includes things like retries, DLQ, etc. Needed for Roundtripping v1alpha1 <-> v1beta1. ParallelBranchStatus ( Appears on: ParallelStatus ) ParallelBranchStatus represents the current state of a Parallel branch Field Description filterSubscriptionStatus ParallelSubscriptionStatus FilterSubscriptionStatus corresponds to the filter subscription status. filterChannelStatus ParallelChannelStatus FilterChannelStatus corresponds to the filter channel status. subscriberSubscriptionStatus ParallelSubscriptionStatus SubscriptionStatus corresponds to the subscriber subscription status. ParallelChannelStatus ( Appears on: ParallelBranchStatus , ParallelStatus ) Field Description channel Kubernetes core/v1.ObjectReference Channel is the reference to the underlying channel. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Channel is ready or not. ParallelSpec ( Appears on: Parallel ) Field Description branches []ParallelBranch Branches is the list of Filter/Subscribers pairs. channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of a case Subscriber gets sent to when the case does not have a Reply ParallelStatus ( Appears on: Parallel ) ParallelStatus represents the current state of a Parallel. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. ingressChannelStatus ParallelChannelStatus IngressChannelStatus corresponds to the ingress channel status. branchStatuses []ParallelBranchStatus BranchStatuses is an array of corresponding to branch statuses. Matches the Spec.Branches array in the order. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the starting point to this Parallel. Sending to this will target the first subscriber. It generally has the form {channel}.{namespace}.svc.{cluster domain name} ParallelSubscriptionStatus ( Appears on: ParallelBranchStatus ) Field Description subscription Kubernetes core/v1.ObjectReference Subscription is the reference to the underlying Subscription. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Subscription is ready or not. Sequence Sequence defines a sequence of Subscribers that will be wired in series through Channels and Subscriptions. Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec SequenceSpec Spec defines the desired state of the Sequence. steps []SequenceStep Steps is the list of Destinations (processors / functions) that will be called in the order provided. Each step has its own delivery options channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of the last Subscriber gets sent to. status SequenceStatus (Optional) Status represents the current state of the Sequence. This data may be out of date. SequenceChannelStatus ( Appears on: SequenceStatus ) Field Description channel Kubernetes core/v1.ObjectReference Channel is the reference to the underlying channel. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Channel is ready or not. SequenceSpec ( Appears on: Sequence ) Field Description steps []SequenceStep Steps is the list of Destinations (processors / functions) that will be called in the order provided. Each step has its own delivery options channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of the last Subscriber gets sent to. SequenceStatus ( Appears on: Sequence ) SequenceStatus represents the current state of a Sequence. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. subscriptionStatuses []SequenceSubscriptionStatus SubscriptionStatuses is an array of corresponding Subscription statuses. Matches the Spec.Steps array in the order. channelStatuses []SequenceChannelStatus ChannelStatuses is an array of corresponding Channel statuses. Matches the Spec.Steps array in the order. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the starting point to this Sequence. Sending to this will target the first subscriber. It generally has the form {channel}.{namespace}.svc.{cluster domain name} SequenceStep ( Appears on: SequenceSpec ) Field Description Destination knative.dev/pkg/apis/duck/v1.Destination (Members of Destination are embedded into this type.) Subscriber receiving the step event delivery DeliverySpec (Optional) Delivery is the delivery specification for events to the subscriber This includes things like retries, DLQ, etc. SequenceSubscriptionStatus ( Appears on: SequenceStatus ) Field Description subscription Kubernetes core/v1.ObjectReference Subscription is the reference to the underlying Subscription. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Subscription is ready or not. messaging.knative.dev/v1 Package v1 is the v1 version of the API. Resource Types: Channel InMemoryChannel Subscription Channel Channel represents a generic Channel. It is normally used when we want a Channel, but don\u2019t need a specific Channel implementation. Field Description apiVersion string messaging.knative.dev/v1 kind string Channel metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ChannelSpec Spec defines the desired state of the Channel. channelTemplate ChannelTemplateSpec ChannelTemplate specifies which Channel CRD to use to create the CRD Channel backing this Channel. This is immutable after creation. Normally this is set by the Channel defaulter, not directly by the user. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to ChannelableSpec status ChannelStatus (Optional) Status represents the current state of the Channel. This data may be out of date. InMemoryChannel InMemoryChannel is a resource representing an in memory channel Field Description apiVersion string messaging.knative.dev/v1 kind string InMemoryChannel metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec InMemoryChannelSpec Spec defines the desired state of the Channel. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to Duck type Channelable. status InMemoryChannelStatus (Optional) Status represents the current state of the Channel. This data may be out of date. Subscription Subscription routes events received on a Channel to a DNS name and corresponds to the subscriptions.channels.knative.dev CRD. Field Description apiVersion string messaging.knative.dev/v1 kind string Subscription metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SubscriptionSpec channel Kubernetes core/v1.ObjectReference Reference to a channel that will be used to create the subscription You can specify only the following fields of the ObjectReference: - Kind - APIVersion - Name The resource pointed by this ObjectReference must meet the contract to the ChannelableSpec duck type. If the resource does not meet this contract it will be reflected in the Subscription\u2019s status. This field is immutable. We have no good answer on what happens to the events that are currently in the channel being consumed from and what the semantics there should be. For now, you can always delete the Subscription and recreate it to point to a different channel, giving the user more control over what semantics should be used (drain the channel first, possibly have events dropped, etc.) subscriber knative.dev/pkg/apis/duck/v1.Destination (Optional) Subscriber is reference to (optional) function for processing events. Events from the Channel will be delivered here and replies are sent to a Destination as specified by the Reply. reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply specifies (optionally) how to handle events returned from the Subscriber target. delivery DeliverySpec (Optional) Delivery configuration status SubscriptionStatus ChannelDefaulter ChannelDefaulter sets the default Channel CRD and Arguments on Channels that do not specify any implementation. ChannelSpec ( Appears on: Channel ) ChannelSpec defines which subscribers have expressed interest in receiving events from this Channel. It also defines the ChannelTemplate to use in order to create the CRD Channel backing this Channel. Field Description channelTemplate ChannelTemplateSpec ChannelTemplate specifies which Channel CRD to use to create the CRD Channel backing this Channel. This is immutable after creation. Normally this is set by the Channel defaulter, not directly by the user. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to ChannelableSpec ChannelStatus ( Appears on: Channel ) ChannelStatus represents the current state of a Channel. Field Description ChannelableStatus ChannelableStatus (Members of ChannelableStatus are embedded into this type.) Channel conforms to ChannelableStatus channel knative.dev/pkg/apis/duck/v1.KReference Channel is an KReference to the Channel CRD backing this Channel. ChannelTemplateSpec ( Appears on: ParallelSpec , SequenceSpec , ChannelSpec ) Field Description spec k8s.io/apimachinery/pkg/runtime.RawExtension (Optional) Spec defines the Spec to use for each channel created. Passed in verbatim to the Channel CRD as Spec section. InMemoryChannelSpec ( Appears on: InMemoryChannel ) InMemoryChannelSpec defines which subscribers have expressed interest in receiving events from this InMemoryChannel. arguments for a Channel. Field Description ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to Duck type Channelable. InMemoryChannelStatus ( Appears on: InMemoryChannel ) ChannelStatus represents the current state of a Channel. Field Description ChannelableStatus ChannelableStatus (Members of ChannelableStatus are embedded into this type.) Channel conforms to Duck type Channelable. SubscriptionSpec ( Appears on: Subscription ) SubscriptionSpec specifies the Channel for incoming events, a Subscriber target for processing those events and where to put the result of the processing. Only From (where the events are coming from) is always required. You can optionally only Process the events (results in no output events) by leaving out the Result. You can also perform an identity transformation on the incoming events by leaving out the Subscriber and only specifying Result. The following are all valid specifications: channel \u2013[subscriber]\u2013> reply Sink, no outgoing events: channel \u2013 subscriber no-op function (identity transformation): channel \u2013> reply Field Description channel Kubernetes core/v1.ObjectReference Reference to a channel that will be used to create the subscription You can specify only the following fields of the ObjectReference: - Kind - APIVersion - Name The resource pointed by this ObjectReference must meet the contract to the ChannelableSpec duck type. If the resource does not meet this contract it will be reflected in the Subscription\u2019s status. This field is immutable. We have no good answer on what happens to the events that are currently in the channel being consumed from and what the semantics there should be. For now, you can always delete the Subscription and recreate it to point to a different channel, giving the user more control over what semantics should be used (drain the channel first, possibly have events dropped, etc.) subscriber knative.dev/pkg/apis/duck/v1.Destination (Optional) Subscriber is reference to (optional) function for processing events. Events from the Channel will be delivered here and replies are sent to a Destination as specified by the Reply. reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply specifies (optionally) how to handle events returned from the Subscriber target. delivery DeliverySpec (Optional) Delivery configuration SubscriptionStatus ( Appears on: Subscription ) SubscriptionStatus (computed) for a subscription Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. physicalSubscription SubscriptionStatusPhysicalSubscription PhysicalSubscription is the fully resolved values that this Subscription represents. SubscriptionStatusPhysicalSubscription ( Appears on: SubscriptionStatus ) SubscriptionStatusPhysicalSubscription represents the fully resolved values for this Subscription. Field Description subscriberUri knative.dev/pkg/apis.URL SubscriberURI is the fully resolved URI for spec.subscriber. replyUri knative.dev/pkg/apis.URL ReplyURI is the fully resolved URI for the spec.reply. deadLetterSinkUri knative.dev/pkg/apis.URL ReplyURI is the fully resolved URI for the spec.delivery.deadLetterSink. messaging.knative.dev/v1beta1 Package v1beta1 is the v1beta1 version of the API. Resource Types: Channel InMemoryChannel Subscription Channel Channel represents a generic Channel. It is normally used when we want a Channel, but don\u2019t need a specific Channel implementation. Field Description apiVersion string messaging.knative.dev/v1beta1 kind string Channel metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ChannelSpec Spec defines the desired state of the Channel. channelTemplate ChannelTemplateSpec ChannelTemplate specifies which Channel CRD to use to create the CRD Channel backing this Channel. This is immutable after creation. Normally this is set by the Channel defaulter, not directly by the user. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to ChannelableSpec status ChannelStatus (Optional) Status represents the current state of the Channel. This data may be out of date. InMemoryChannel InMemoryChannel is a resource representing an in memory channel Field Description apiVersion string messaging.knative.dev/v1beta1 kind string InMemoryChannel metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec InMemoryChannelSpec Spec defines the desired state of the Channel. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to Duck type Channelable. status InMemoryChannelStatus (Optional) Status represents the current state of the Channel. This data may be out of date. Subscription Subscription routes events received on a Channel to a DNS name and corresponds to the subscriptions.channels.knative.dev CRD. Field Description apiVersion string messaging.knative.dev/v1beta1 kind string Subscription metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SubscriptionSpec channel Kubernetes core/v1.ObjectReference Reference to a channel that will be used to create the subscription You can specify only the following fields of the ObjectReference: - Kind - APIVersion - Name The resource pointed by this ObjectReference must meet the contract to the ChannelableSpec duck type. If the resource does not meet this contract it will be reflected in the Subscription\u2019s status. This field is immutable. We have no good answer on what happens to the events that are currently in the channel being consumed from and what the semantics there should be. For now, you can always delete the Subscription and recreate it to point to a different channel, giving the user more control over what semantics should be used (drain the channel first, possibly have events dropped, etc.) subscriber knative.dev/pkg/apis/duck/v1.Destination (Optional) Subscriber is reference to (optional) function for processing events. Events from the Channel will be delivered here and replies are sent to a Destination as specified by the Reply. reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply specifies (optionally) how to handle events returned from the Subscriber target. delivery DeliverySpec (Optional) Delivery configuration status SubscriptionStatus ChannelDefaulter ChannelDefaulter sets the default Channel CRD and Arguments on Channels that do not specify any implementation. ChannelSpec ( Appears on: Channel ) ChannelSpec defines which subscribers have expressed interest in receiving events from this Channel. It also defines the ChannelTemplate to use in order to create the CRD Channel backing this Channel. Field Description channelTemplate ChannelTemplateSpec ChannelTemplate specifies which Channel CRD to use to create the CRD Channel backing this Channel. This is immutable after creation. Normally this is set by the Channel defaulter, not directly by the user. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to ChannelableSpec ChannelStatus ( Appears on: Channel ) ChannelStatus represents the current state of a Channel. Field Description ChannelableStatus ChannelableStatus (Members of ChannelableStatus are embedded into this type.) Channel conforms to ChannelableStatus channel knative.dev/pkg/apis/duck/v1.KReference Channel is an KReference to the Channel CRD backing this Channel. ChannelTemplateSpec ( Appears on: ParallelSpec , SequenceSpec , ChannelSpec ) Field Description spec k8s.io/apimachinery/pkg/runtime.RawExtension (Optional) Spec defines the Spec to use for each channel created. Passed in verbatim to the Channel CRD as Spec section. InMemoryChannelSpec ( Appears on: InMemoryChannel ) InMemoryChannelSpec defines which subscribers have expressed interest in receiving events from this InMemoryChannel. arguments for a Channel. Field Description ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to Duck type Channelable. InMemoryChannelStatus ( Appears on: InMemoryChannel ) ChannelStatus represents the current state of a Channel. Field Description ChannelableStatus ChannelableStatus (Members of ChannelableStatus are embedded into this type.) Channel conforms to Duck type Channelable. SubscriptionSpec ( Appears on: Subscription ) SubscriptionSpec specifies the Channel for incoming events, a Subscriber target for processing those events and where to put the result of the processing. Only From (where the events are coming from) is always required. You can optionally only Process the events (results in no output events) by leaving out the Result. You can also perform an identity transformation on the incoming events by leaving out the Subscriber and only specifying Result. The following are all valid specifications: channel \u2013[subscriber]\u2013> reply Sink, no outgoing events: channel \u2013 subscriber no-op function (identity transformation): channel \u2013> reply Field Description channel Kubernetes core/v1.ObjectReference Reference to a channel that will be used to create the subscription You can specify only the following fields of the ObjectReference: - Kind - APIVersion - Name The resource pointed by this ObjectReference must meet the contract to the ChannelableSpec duck type. If the resource does not meet this contract it will be reflected in the Subscription\u2019s status. This field is immutable. We have no good answer on what happens to the events that are currently in the channel being consumed from and what the semantics there should be. For now, you can always delete the Subscription and recreate it to point to a different channel, giving the user more control over what semantics should be used (drain the channel first, possibly have events dropped, etc.) subscriber knative.dev/pkg/apis/duck/v1.Destination (Optional) Subscriber is reference to (optional) function for processing events. Events from the Channel will be delivered here and replies are sent to a Destination as specified by the Reply. reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply specifies (optionally) how to handle events returned from the Subscriber target. delivery DeliverySpec (Optional) Delivery configuration SubscriptionStatus ( Appears on: Subscription ) SubscriptionStatus (computed) for a subscription Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. physicalSubscription SubscriptionStatusPhysicalSubscription PhysicalSubscription is the fully resolved values that this Subscription represents. SubscriptionStatusPhysicalSubscription ( Appears on: SubscriptionStatus ) SubscriptionStatusPhysicalSubscription represents the fully resolved values for this Subscription. Field Description subscriberUri knative.dev/pkg/apis.URL SubscriberURI is the fully resolved URI for spec.subscriber. replyUri knative.dev/pkg/apis.URL ReplyURI is the fully resolved URI for the spec.reply. deadLetterSinkUri knative.dev/pkg/apis.URL ReplyURI is the fully resolved URI for the spec.delivery.deadLetterSink. sources.knative.dev/v1 Package v1 contains API Schema definitions for the sources v1 API group. Resource Types: ApiServerSource ContainerSource SinkBinding ApiServerSource ApiServerSource is the Schema for the apiserversources API Field Description apiVersion string sources.knative.dev/v1 kind string ApiServerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ApiServerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. status ApiServerSourceStatus ContainerSource ContainerSource is the Schema for the containersources API Field Description apiVersion string sources.knative.dev/v1 kind string ContainerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ContainerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created status ContainerSourceStatus SinkBinding SinkBinding describes a Binding that is also a Source. The sink (from the Source duck) is resolved to a URL and then projected into the subject by augmenting the runtime contract of the referenced containers to have a K_SINK environment variable holding the endpoint to which to send cloud events. Field Description apiVersion string sources.knative.dev/v1 kind string SinkBinding metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SinkBindingSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. status SinkBindingStatus APIVersionKind ( Appears on: ApiServerSourceSpec ) APIVersionKind is an APIVersion and Kind tuple. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds APIVersionKindSelector ( Appears on: ApiServerSourceSpec ) APIVersionKindSelector is an APIVersion Kind tuple with a LabelSelector. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds selector Kubernetes meta/v1.LabelSelector (Optional) LabelSelector filters this source to objects to those resources pass the label selector. More info: http://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors ApiServerSourceSpec ( Appears on: ApiServerSource ) ApiServerSourceSpec defines the desired state of ApiServerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. ApiServerSourceStatus ( Appears on: ApiServerSource ) ApiServerSourceStatus defines the observed state of ApiServerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. ContainerSourceSpec ( Appears on: ContainerSource ) ContainerSourceSpec defines the desired state of ContainerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created ContainerSourceStatus ( Appears on: ContainerSource ) ContainerSourceStatus defines the observed state of ContainerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. SinkBindingSpec ( Appears on: SinkBinding ) SinkBindingSpec holds the desired state of the SinkBinding (from the client). Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. SinkBindingStatus ( Appears on: SinkBinding ) SinkBindingStatus communicates the observed state of the SinkBinding (from the controller). Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. sources.knative.dev/v1alpha1 Package v1alpha1 contains API Schema definitions for the sources v1alpha1 API group Resource Types: ApiServerSource SinkBinding ApiServerSource ApiServerSource is the Schema for the apiserversources API Field Description apiVersion string sources.knative.dev/v1alpha1 kind string ApiServerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ApiServerSourceSpec resources []ApiServerResource Resources is the list of resources to watch serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. sink knative.dev/pkg/apis/duck/v1beta1.Destination (Optional) Sink is a reference to an object that will resolve to a domain name to use as the sink. ceOverrides knative.dev/pkg/apis/duck/v1.CloudEventOverrides (Optional) CloudEventOverrides defines overrides to control the output format and modifications of the event sent to the sink. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string Mode is the mode the receive adapter controller runs under: Ref or Resource. Ref sends only the reference to the resource. Resource send the full resource. status ApiServerSourceStatus SinkBinding SinkBinding describes a Binding that is also a Source. The sink (from the Source duck) is resolved to a URL and then projected into the subject by augmenting the runtime contract of the referenced containers to have a K_SINK environment variable holding the endpoint to which to send cloud events. Field Description apiVersion string sources.knative.dev/v1alpha1 kind string SinkBinding metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SinkBindingSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) BindingSpec knative.dev/pkg/apis/duck/v1alpha1.BindingSpec (Members of BindingSpec are embedded into this type.) status SinkBindingStatus ApiServerResource ( Appears on: ApiServerSourceSpec ) ApiServerResource defines the resource to watch Field Description apiVersion string API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds labelSelector Kubernetes meta/v1.LabelSelector LabelSelector restricts this source to objects with the selected labels More info: http://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors controllerSelector Kubernetes meta/v1.OwnerReference ControllerSelector restricts this source to objects with a controlling owner reference of the specified kind. Only apiVersion and kind are used. Both are optional. Deprecated: Per-resource owner refs will no longer be supported in v1alpha2, please use Spec.Owner as a GKV. controller bool If true, send an event referencing the object controlling the resource Deprecated: Per-resource controller flag will no longer be supported in v1alpha2, please use Spec.Owner as a GKV. ApiServerSourceSpec ( Appears on: ApiServerSource ) ApiServerSourceSpec defines the desired state of ApiServerSource Field Description resources []ApiServerResource Resources is the list of resources to watch serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. sink knative.dev/pkg/apis/duck/v1beta1.Destination (Optional) Sink is a reference to an object that will resolve to a domain name to use as the sink. ceOverrides knative.dev/pkg/apis/duck/v1.CloudEventOverrides (Optional) CloudEventOverrides defines overrides to control the output format and modifications of the event sent to the sink. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string Mode is the mode the receive adapter controller runs under: Ref or Resource. Ref sends only the reference to the resource. Resource send the full resource. ApiServerSourceStatus ( Appears on: ApiServerSource ) ApiServerSourceStatus defines the observed state of ApiServerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. SinkBindingSpec ( Appears on: SinkBinding ) SinkBindingSpec holds the desired state of the SinkBinding (from the client). Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) BindingSpec knative.dev/pkg/apis/duck/v1alpha1.BindingSpec (Members of BindingSpec are embedded into this type.) SinkBindingStatus ( Appears on: SinkBinding ) SinkBindingStatus communicates the observed state of the SinkBinding (from the controller). Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) sources.knative.dev/v1alpha2 Package v1alpha2 contains API Schema definitions for the sources v1beta1 API group Resource Types: ApiServerSource ContainerSource PingSource SinkBinding ApiServerSource ApiServerSource is the Schema for the apiserversources API Field Description apiVersion string sources.knative.dev/v1alpha2 kind string ApiServerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ApiServerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. status ApiServerSourceStatus ContainerSource ContainerSource is the Schema for the containersources API Field Description apiVersion string sources.knative.dev/v1alpha2 kind string ContainerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ContainerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created status ContainerSourceStatus PingSource PingSource is the Schema for the PingSources API. Field Description apiVersion string sources.knative.dev/v1alpha2 kind string PingSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec PingSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cronjob schedule. Defaults to * * * * * . jsonData string (Optional) JsonData is json encoded data used as the body of the event posted to the sink. Default is empty. If set, datacontenttype will also be set to \u201capplication/json\u201d. status PingSourceStatus SinkBinding SinkBinding describes a Binding that is also a Source. The sink (from the Source duck) is resolved to a URL and then projected into the subject by augmenting the runtime contract of the referenced containers to have a K_SINK environment variable holding the endpoint to which to send cloud events. Field Description apiVersion string sources.knative.dev/v1alpha2 kind string SinkBinding metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SinkBindingSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1alpha1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1alpha1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. status SinkBindingStatus APIVersionKind ( Appears on: ApiServerSourceSpec , ApiServerSourceSpec ) APIVersionKind is an APIVersion and Kind tuple. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds APIVersionKindSelector ( Appears on: ApiServerSourceSpec ) APIVersionKindSelector is an APIVersion Kind tuple with a LabelSelector. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds selector Kubernetes meta/v1.LabelSelector (Optional) LabelSelector filters this source to objects to those resources pass the label selector. More info: http://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors ApiServerSourceSpec ( Appears on: ApiServerSource ) ApiServerSourceSpec defines the desired state of ApiServerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. ApiServerSourceStatus ( Appears on: ApiServerSource ) ApiServerSourceStatus defines the observed state of ApiServerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. ContainerSourceSpec ( Appears on: ContainerSource ) ContainerSourceSpec defines the desired state of ContainerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created ContainerSourceStatus ( Appears on: ContainerSource ) ContainerSourceStatus defines the observed state of ContainerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. PingSourceSpec ( Appears on: PingSource ) PingSourceSpec defines the desired state of the PingSource. Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cronjob schedule. Defaults to * * * * * . jsonData string (Optional) JsonData is json encoded data used as the body of the event posted to the sink. Default is empty. If set, datacontenttype will also be set to \u201capplication/json\u201d. PingSourceStatus ( Appears on: PingSource ) PingSourceStatus defines the observed state of PingSource. Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. SinkBindingSpec ( Appears on: SinkBinding ) SinkBindingSpec holds the desired state of the SinkBinding (from the client). Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1alpha1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1alpha1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. SinkBindingStatus ( Appears on: SinkBinding ) SinkBindingStatus communicates the observed state of the SinkBinding (from the controller). Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. sources.knative.dev/v1beta1 Package v1beta1 contains API Schema definitions for the sources v1beta1 API group. Resource Types: ApiServerSource ContainerSource PingSource SinkBinding ApiServerSource ApiServerSource is the Schema for the apiserversources API Field Description apiVersion string sources.knative.dev/v1beta1 kind string ApiServerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ApiServerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. status ApiServerSourceStatus ContainerSource ContainerSource is the Schema for the containersources API Field Description apiVersion string sources.knative.dev/v1beta1 kind string ContainerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ContainerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created status ContainerSourceStatus PingSource PingSource is the Schema for the PingSources API. Field Description apiVersion string sources.knative.dev/v1beta1 kind string PingSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec PingSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cronjob schedule. Defaults to * * * * * . timezone string Timezone modifies the actual time relative to the specified timezone. Defaults to the system time zone. More general information about time zones: https://www.iana.org/time-zones List of valid timezone values: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones jsonData string (Optional) JsonData is json encoded data used as the body of the event posted to the sink. Default is empty. If set, datacontenttype will also be set to \u201capplication/json\u201d. status PingSourceStatus SinkBinding SinkBinding describes a Binding that is also a Source. The sink (from the Source duck) is resolved to a URL and then projected into the subject by augmenting the runtime contract of the referenced containers to have a K_SINK environment variable holding the endpoint to which to send cloud events. Field Description apiVersion string sources.knative.dev/v1beta1 kind string SinkBinding metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SinkBindingSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1beta1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1beta1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. status SinkBindingStatus APIVersionKind ( Appears on: ApiServerSourceSpec ) APIVersionKind is an APIVersion and Kind tuple. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds APIVersionKindSelector ( Appears on: ApiServerSourceSpec ) APIVersionKindSelector is an APIVersion Kind tuple with a LabelSelector. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds selector Kubernetes meta/v1.LabelSelector (Optional) LabelSelector filters this source to objects to those resources pass the label selector. More info: http://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors ApiServerSourceSpec ( Appears on: ApiServerSource ) ApiServerSourceSpec defines the desired state of ApiServerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. ApiServerSourceStatus ( Appears on: ApiServerSource ) ApiServerSourceStatus defines the observed state of ApiServerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. ContainerSourceSpec ( Appears on: ContainerSource ) ContainerSourceSpec defines the desired state of ContainerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created ContainerSourceStatus ( Appears on: ContainerSource ) ContainerSourceStatus defines the observed state of ContainerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. PingSourceSpec ( Appears on: PingSource ) PingSourceSpec defines the desired state of the PingSource. Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cronjob schedule. Defaults to * * * * * . timezone string Timezone modifies the actual time relative to the specified timezone. Defaults to the system time zone. More general information about time zones: https://www.iana.org/time-zones List of valid timezone values: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones jsonData string (Optional) JsonData is json encoded data used as the body of the event posted to the sink. Default is empty. If set, datacontenttype will also be set to \u201capplication/json\u201d. PingSourceStatus ( Appears on: PingSource ) PingSourceStatus defines the observed state of PingSource. Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. SinkBindingSpec ( Appears on: SinkBinding ) SinkBindingSpec holds the desired state of the SinkBinding (from the client). Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1beta1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1beta1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. SinkBindingStatus ( Appears on: SinkBinding ) SinkBindingStatus communicates the observed state of the SinkBinding (from the controller). Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. sources.knative.dev/v1beta2 Package v1beta2 contains API Schema definitions for the sources v1beta2 API group. Resource Types: PingSource PingSource PingSource is the Schema for the PingSources API. Field Description apiVersion string sources.knative.dev/v1beta2 kind string PingSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec PingSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cron schedule. Defaults to * * * * * . timezone string Timezone modifies the actual time relative to the specified timezone. Defaults to the system time zone. More general information about time zones: https://www.iana.org/time-zones List of valid timezone values: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones contentType string (Optional) ContentType is the media type of Data or DataBase64. Default is empty. data string (Optional) Data is data used as the body of the event posted to the sink. Default is empty. Mutually exclusive with DataBase64. dataBase64 string (Optional) DataBase64 is the base64-encoded string of the actual event\u2019s body posted to the sink. Default is empty. Mutually exclusive with Data. status PingSourceStatus PingSourceSpec ( Appears on: PingSource ) PingSourceSpec defines the desired state of the PingSource. Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cron schedule. Defaults to * * * * * . timezone string Timezone modifies the actual time relative to the specified timezone. Defaults to the system time zone. More general information about time zones: https://www.iana.org/time-zones List of valid timezone values: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones contentType string (Optional) ContentType is the media type of Data or DataBase64. Default is empty. data string (Optional) Data is data used as the body of the event posted to the sink. Default is empty. Mutually exclusive with DataBase64. dataBase64 string (Optional) DataBase64 is the base64-encoded string of the actual event\u2019s body posted to the sink. Default is empty. Mutually exclusive with Data. PingSourceStatus ( Appears on: PingSource ) PingSourceStatus defines the observed state of PingSource. Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. Generated with gen-crd-api-reference-docs on git commit 8f35d4254 .","title":"Eventing"},{"location":"serving/","text":"Knative Serving \u00b6 Knative Serving builds on Kubernetes and Istio to support deploying and serving of serverless applications and functions. Serving is easy to get started with and scales to support advanced scenarios. The Knative Serving project provides middleware primitives that enable: Rapid deployment of serverless containers Automatic scaling up and down to zero Routing and network programming for Istio components Point-in-time snapshots of deployed code and configurations Serving resources \u00b6 Knative Serving defines a set of objects as Kubernetes Custom Resource Definitions (CRDs). These objects are used to define and control how your serverless workload behaves on the cluster: Service : The service.serving.knative.dev resource automatically manages the whole lifecycle of your workload. It controls the creation of other objects to ensure that your app has a route, a configuration, and a new revision for each update of the service. Service can be defined to always route traffic to the latest revision or to a pinned revision. Route : The route.serving.knative.dev resource maps a network endpoint to one or more revisions. You can manage the traffic in several ways, including fractional traffic and named routes. Configuration : The configuration.serving.knative.dev resource maintains the desired state for your deployment. It provides a clean separation between code and configuration and follows the Twelve-Factor App methodology. Modifying a configuration creates a new revision. Revision : The revision.serving.knative.dev resource is a point-in-time snapshot of the code and configuration for each modification made to the workload. Revisions are immutable objects and can be retained for as long as useful. Knative Serving Revisions can be automatically scaled up and down according to incoming traffic. See Configuring the Autoscaler for more information. Getting Started \u00b6 To get started with Serving, check out one of the hello world sample projects. These projects use the Service resource, which manages all of the details for you. With the Service resource, a deployed service will automatically have a matching route and configuration created. Each time the Service is updated, a new revision is created. For more information on the resources and their interactions, see the Resource Types Overview in the Knative Serving repository. More samples and demos \u00b6 Knative Serving code samples Debugging Knative Serving issues \u00b6 Debugging Application Issues Configuration and Networking \u00b6 Configuring cluster local routes Using a custom domain Assigning a static IP address for Knative on Google Kubernetes Engine Using subroutes Observability \u00b6 Serving Metrics API Known Issues \u00b6 See the Knative Serving Issues page for a full list of known issues.","title":"Overview"},{"location":"serving/#knative-serving","text":"Knative Serving builds on Kubernetes and Istio to support deploying and serving of serverless applications and functions. Serving is easy to get started with and scales to support advanced scenarios. The Knative Serving project provides middleware primitives that enable: Rapid deployment of serverless containers Automatic scaling up and down to zero Routing and network programming for Istio components Point-in-time snapshots of deployed code and configurations","title":"Knative Serving"},{"location":"serving/#serving-resources","text":"Knative Serving defines a set of objects as Kubernetes Custom Resource Definitions (CRDs). These objects are used to define and control how your serverless workload behaves on the cluster: Service : The service.serving.knative.dev resource automatically manages the whole lifecycle of your workload. It controls the creation of other objects to ensure that your app has a route, a configuration, and a new revision for each update of the service. Service can be defined to always route traffic to the latest revision or to a pinned revision. Route : The route.serving.knative.dev resource maps a network endpoint to one or more revisions. You can manage the traffic in several ways, including fractional traffic and named routes. Configuration : The configuration.serving.knative.dev resource maintains the desired state for your deployment. It provides a clean separation between code and configuration and follows the Twelve-Factor App methodology. Modifying a configuration creates a new revision. Revision : The revision.serving.knative.dev resource is a point-in-time snapshot of the code and configuration for each modification made to the workload. Revisions are immutable objects and can be retained for as long as useful. Knative Serving Revisions can be automatically scaled up and down according to incoming traffic. See Configuring the Autoscaler for more information.","title":"Serving resources"},{"location":"serving/#getting-started","text":"To get started with Serving, check out one of the hello world sample projects. These projects use the Service resource, which manages all of the details for you. With the Service resource, a deployed service will automatically have a matching route and configuration created. Each time the Service is updated, a new revision is created. For more information on the resources and their interactions, see the Resource Types Overview in the Knative Serving repository.","title":"Getting Started"},{"location":"serving/#more-samples-and-demos","text":"Knative Serving code samples","title":"More samples and demos"},{"location":"serving/#debugging-knative-serving-issues","text":"Debugging Application Issues","title":"Debugging Knative Serving issues"},{"location":"serving/#configuration-and-networking","text":"Configuring cluster local routes Using a custom domain Assigning a static IP address for Knative on Google Kubernetes Engine Using subroutes","title":"Configuration and Networking"},{"location":"serving/#observability","text":"Serving Metrics API","title":"Observability"},{"location":"serving/#known-issues","text":"See the Knative Serving Issues page for a full list of known issues.","title":"Known Issues"},{"location":"serving/accessing-traces/","text":"Accessing request traces \u00b6 Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests. Configuring Traces \u00b6 You can update the configuration file for tracing in config-tracing.yaml . Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or stackdriver). You can quickly explore and update the ConfigMap object with the following command: kubectl -n knative-serving edit configmap config-tracing Zipkin \u00b6 In order to access request traces, you use the Zipkin visualization tool. To open the Zipkin UI, enter the following command: kubectl proxy This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster. Navigate to the Zipkin UI . Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call. Jaeger \u00b6 In order to access request traces, you use the Jaeger visualization tool. To open the Jaeger UI, enter the following command: kubectl proxy This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster. Navigate to the Jaeger UI . Select the service of interest and click \"Find Traces\" to see the latest traces. Click on a trace to see a detailed view of a specific call.","title":"Accessing request traces"},{"location":"serving/accessing-traces/#accessing-request-traces","text":"Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests.","title":"Accessing request traces"},{"location":"serving/accessing-traces/#configuring-traces","text":"You can update the configuration file for tracing in config-tracing.yaml . Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or stackdriver). You can quickly explore and update the ConfigMap object with the following command: kubectl -n knative-serving edit configmap config-tracing","title":"Configuring Traces"},{"location":"serving/accessing-traces/#zipkin","text":"In order to access request traces, you use the Zipkin visualization tool. To open the Zipkin UI, enter the following command: kubectl proxy This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster. Navigate to the Zipkin UI . Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call.","title":"Zipkin"},{"location":"serving/accessing-traces/#jaeger","text":"In order to access request traces, you use the Jaeger visualization tool. To open the Jaeger UI, enter the following command: kubectl proxy This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster. Navigate to the Jaeger UI . Select the service of interest and click \"Find Traces\" to see the latest traces. Click on a trace to see a detailed view of a specific call.","title":"Jaeger"},{"location":"serving/config-ha/","text":"Configuring high-availability components \u00b6 Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable. When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader. Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica. Disabling leader election \u00b6 For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: --disable-ha . This option will go away when HA graduates to \"stable\". Scaling the control plane \u00b6 With the exception of the activator component you can scale up any deployment running in knative-serving (or kourier-system ) with a command like: $ kubectl -n knative-serving scale deployment <deployment-name> --replicas=2 Setting --replicas to a value of 2 enables HA. You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 controller deployments, set --replicas=3 . Setting --replicas=1 disables HA. NOTE: If you scale down the autoscaler component, you may observe inaccurate autoscaling results for some revisions for a period of time up to the stable-window value. This is because when an autoscaler pod is terminating, ownership of the revisions belonging to that pod is passed to other autoscaler pods that are on stand by. The autoscaler pods that take over ownership of those revisions use the stable-window time to build the scaling metrics state for those revisions. Scaling the data plane \u00b6 The scale of the activator component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running: $ kubectl get hpa activator -n knative-serving The possible output will be something like: NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE activator Deployment/activator 2%/100% 5 15 11 346d By default minReplicas and maxReplicas are set to 1 and 20 , correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to minScale=9 and maxScale=19 using the following command: $ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}' To set the activator scale to a particular value, just set minScale and maxScale to the same desired value. It is recommended for production deployments to run at least 3 activator instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.","title":"Configuring high-availability components"},{"location":"serving/config-ha/#configuring-high-availability-components","text":"Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable. When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader. Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica.","title":"Configuring high-availability components"},{"location":"serving/config-ha/#disabling-leader-election","text":"For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: --disable-ha . This option will go away when HA graduates to \"stable\".","title":"Disabling leader election"},{"location":"serving/config-ha/#scaling-the-control-plane","text":"With the exception of the activator component you can scale up any deployment running in knative-serving (or kourier-system ) with a command like: $ kubectl -n knative-serving scale deployment <deployment-name> --replicas=2 Setting --replicas to a value of 2 enables HA. You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 controller deployments, set --replicas=3 . Setting --replicas=1 disables HA. NOTE: If you scale down the autoscaler component, you may observe inaccurate autoscaling results for some revisions for a period of time up to the stable-window value. This is because when an autoscaler pod is terminating, ownership of the revisions belonging to that pod is passed to other autoscaler pods that are on stand by. The autoscaler pods that take over ownership of those revisions use the stable-window time to build the scaling metrics state for those revisions.","title":"Scaling the control plane"},{"location":"serving/config-ha/#scaling-the-data-plane","text":"The scale of the activator component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running: $ kubectl get hpa activator -n knative-serving The possible output will be something like: NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE activator Deployment/activator 2%/100% 5 15 11 346d By default minReplicas and maxReplicas are set to 1 and 20 , correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to minScale=9 and maxScale=19 using the following command: $ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}' To set the activator scale to a particular value, just set minScale and maxScale to the same desired value. It is recommended for production deployments to run at least 3 activator instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.","title":"Scaling the data plane"},{"location":"serving/creating-domain-mappings/","text":"Setting up a custom domain per Service \u00b6 Knative services are automatically given a default domain name based on the cluster configuration, e.g. \"mysvc.mynamespace.mydomain\". You can also map a single custom domain name that you own to a specific Knative service using the domain mapping feature, if enabled. For example, if you own the example.org domain name, and configure its DNS to reference your Knative cluster, you can use domain mapping to have this domain be served by a Knative service. Before you begin \u00b6 You need to enable the domain mapping feature, as well as a supported Knative Ingress implementation to use it. See Install optional Serving extensions . Create a Knative service that you can map a domain to. You will need a domain name to map, and the ability to change its DNS to point to your Knative Cluster. The details of this step are dependant on your domain registrar. Creating a Domain Mapping \u00b6 To create a mapping from a custom domain name that you control to a Knative Service, you need to create a YAML file that defines a Domain Mapping. This YAML file specifies the domain name to map and the Knative Service to use to service requests. You will also need to point the domain name at your Knative cluster using the tools provided by your domain registrar. Domain Mappings map a single, non-wildcard domain to a specific Knative Service. For example in the example yaml below, the \"example.org\" Domain Mapping maps only \"example.org\" and not \"www.example.org\". You can create multiple Domain Mappings to map multiple domains and subdomains. Procedure \u00b6 Create a new file named domainmapping.yaml containing the following information. apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : example.org namespace : default spec : ref : name : helloworld-go kind : Service apiVersion : serving.knative.dev/v1 name (metadata): The domain name you wish to map to the Knative service. namespace : The namespace that both domain mapping and the Knative service use. name (ref): The Knative service which should be used to service requests for the custom domain name. You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form {name}.{namespace}.{clusterdomain} where {name} and {namespace} are the name and namespace of a Kubernetes service, and {clusterdomain} is the cluster domain. Objects conforming to this contract include Knative services and Routes, and Kubernetes services. From the directory where the new domainmapping.yaml file was created, deploy the domain mapping by applying the domainmapping.yaml file. kubectl apply -f domainmapping.yaml You will also need to point the example.org domain name at the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.","title":"Setting up a custom domain per Service"},{"location":"serving/creating-domain-mappings/#setting-up-a-custom-domain-per-service","text":"Knative services are automatically given a default domain name based on the cluster configuration, e.g. \"mysvc.mynamespace.mydomain\". You can also map a single custom domain name that you own to a specific Knative service using the domain mapping feature, if enabled. For example, if you own the example.org domain name, and configure its DNS to reference your Knative cluster, you can use domain mapping to have this domain be served by a Knative service.","title":"Setting up a custom domain per Service"},{"location":"serving/creating-domain-mappings/#before-you-begin","text":"You need to enable the domain mapping feature, as well as a supported Knative Ingress implementation to use it. See Install optional Serving extensions . Create a Knative service that you can map a domain to. You will need a domain name to map, and the ability to change its DNS to point to your Knative Cluster. The details of this step are dependant on your domain registrar.","title":"Before you begin"},{"location":"serving/creating-domain-mappings/#creating-a-domain-mapping","text":"To create a mapping from a custom domain name that you control to a Knative Service, you need to create a YAML file that defines a Domain Mapping. This YAML file specifies the domain name to map and the Knative Service to use to service requests. You will also need to point the domain name at your Knative cluster using the tools provided by your domain registrar. Domain Mappings map a single, non-wildcard domain to a specific Knative Service. For example in the example yaml below, the \"example.org\" Domain Mapping maps only \"example.org\" and not \"www.example.org\". You can create multiple Domain Mappings to map multiple domains and subdomains.","title":"Creating a Domain Mapping"},{"location":"serving/creating-domain-mappings/#procedure","text":"Create a new file named domainmapping.yaml containing the following information. apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : example.org namespace : default spec : ref : name : helloworld-go kind : Service apiVersion : serving.knative.dev/v1 name (metadata): The domain name you wish to map to the Knative service. namespace : The namespace that both domain mapping and the Knative service use. name (ref): The Knative service which should be used to service requests for the custom domain name. You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form {name}.{namespace}.{clusterdomain} where {name} and {namespace} are the name and namespace of a Kubernetes service, and {clusterdomain} is the cluster domain. Objects conforming to this contract include Knative services and Routes, and Kubernetes services. From the directory where the new domainmapping.yaml file was created, deploy the domain mapping by applying the domainmapping.yaml file. kubectl apply -f domainmapping.yaml You will also need to point the example.org domain name at the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.","title":"Procedure"},{"location":"serving/debugging-application-issues/","text":"Debugging issues with your application \u00b6 You deployed your app to Knative Serving, but it isn't working as expected. Go through this step-by-step guide to understand what failed. Check command-line output \u00b6 Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed. This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100: Error from server (InternalError): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}} to: &{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false} for: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100. ERROR: Non-zero return code '1' from command: Process exited with status 1 Check Route status \u00b6 Run the following command to get the status of the Route object with which you deployed your application: kubectl get route <route-name> --output yaml The conditions in status provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting . Check Ingress/Istio routing \u00b6 To list all Ingress resources and their corresponding labels, run the following command: kubectl get ingresses.networking.internal.knative.dev -o = custom-columns = 'NAME:.metadata.name,LABELS:.metadata.labels' NAME LABELS helloworld-go map [ serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go ] The labels serving.knative.dev/route and serving.knative.dev/routeNamespace indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status. Otherwise, run the following command to look at the ClusterIngress created for your Route kubectl get ingresses.networking.internal.knative.dev <INGRESS_NAME> --output yaml particularly, look at the status: section. If the Ingress is working correctly, we should see the condition with type=Ready to have status=True . Otherwise, there will be error messages. Now, if Ingress shows status Ready , there must be a corresponding VirtualService. Run the following command: kubectl get virtualservice -l networking.internal.knative.dev/ingress = <INGRESS_NAME> -n <INGRESS_NAMESPACE> --output yaml the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate. If you are familar with Istio and istioctl , you may try using istioctl to look deeper using Istio guide . Check Ingress status \u00b6 Knative uses a LoadBalancer service called istio-ingressgateway Service. To check the IP address of your Ingress, use kubectl get svc -n istio-system istio-ingressgateway If there is no external IP address, use kubectl describe svc istio-ingressgateway -n istio-system to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue. Check Revision status \u00b6 If you configure your Route with Configuration , run the following command to get the name of the Revision created for you deployment (look up the configuration name in the Route .yaml file): kubectl get configuration <configuration-name> --output jsonpath = \"{.status.latestCreatedRevisionName}\" If you configure your Route with Revision directly, look up the revision name in the Route yaml file. Then run the following command: kubectl get revision <revision-name> --output yaml A ready Revision should have the following condition in status : conditions : - reason : ServiceReady status : \"True\" type : Ready If you see this condition, check the following to continue debugging: Check Pod status Check Istio routing If you see other conditions, look up the meaning of the conditions in Knative Error Conditions and Reporting . Note: some of them are not implemented yet. An alternative is to check Pod status . Check Pod status \u00b6 To get the Pod s for all your deployments: kubectl get pods This command should list all Pod s with brief status. For example: NAME READY STATUS RESTARTS AGE configuration-example-00001-deployment-659747ff99-9bvr4 2/2 Running 0 3h configuration-example-00002-deployment-5f475b7849-gxcht 1/2 CrashLoopBackOff 2 36s Choose one and use the following command to see detailed information for its status . Some useful fields are conditions and containerStatuses : kubectl get pod <pod-name> --output yaml","title":"Debugging issues with your application"},{"location":"serving/debugging-application-issues/#debugging-issues-with-your-application","text":"You deployed your app to Knative Serving, but it isn't working as expected. Go through this step-by-step guide to understand what failed.","title":"Debugging issues with your application"},{"location":"serving/debugging-application-issues/#check-command-line-output","text":"Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed. This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100: Error from server (InternalError): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}} to: &{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false} for: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100. ERROR: Non-zero return code '1' from command: Process exited with status 1","title":"Check command-line output"},{"location":"serving/debugging-application-issues/#check-route-status","text":"Run the following command to get the status of the Route object with which you deployed your application: kubectl get route <route-name> --output yaml The conditions in status provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting .","title":"Check Route status"},{"location":"serving/debugging-application-issues/#check-ingressistio-routing","text":"To list all Ingress resources and their corresponding labels, run the following command: kubectl get ingresses.networking.internal.knative.dev -o = custom-columns = 'NAME:.metadata.name,LABELS:.metadata.labels' NAME LABELS helloworld-go map [ serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go ] The labels serving.knative.dev/route and serving.knative.dev/routeNamespace indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status. Otherwise, run the following command to look at the ClusterIngress created for your Route kubectl get ingresses.networking.internal.knative.dev <INGRESS_NAME> --output yaml particularly, look at the status: section. If the Ingress is working correctly, we should see the condition with type=Ready to have status=True . Otherwise, there will be error messages. Now, if Ingress shows status Ready , there must be a corresponding VirtualService. Run the following command: kubectl get virtualservice -l networking.internal.knative.dev/ingress = <INGRESS_NAME> -n <INGRESS_NAMESPACE> --output yaml the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate. If you are familar with Istio and istioctl , you may try using istioctl to look deeper using Istio guide .","title":"Check Ingress/Istio routing"},{"location":"serving/debugging-application-issues/#check-ingress-status","text":"Knative uses a LoadBalancer service called istio-ingressgateway Service. To check the IP address of your Ingress, use kubectl get svc -n istio-system istio-ingressgateway If there is no external IP address, use kubectl describe svc istio-ingressgateway -n istio-system to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue.","title":"Check Ingress status"},{"location":"serving/debugging-application-issues/#check-revision-status","text":"If you configure your Route with Configuration , run the following command to get the name of the Revision created for you deployment (look up the configuration name in the Route .yaml file): kubectl get configuration <configuration-name> --output jsonpath = \"{.status.latestCreatedRevisionName}\" If you configure your Route with Revision directly, look up the revision name in the Route yaml file. Then run the following command: kubectl get revision <revision-name> --output yaml A ready Revision should have the following condition in status : conditions : - reason : ServiceReady status : \"True\" type : Ready If you see this condition, check the following to continue debugging: Check Pod status Check Istio routing If you see other conditions, look up the meaning of the conditions in Knative Error Conditions and Reporting . Note: some of them are not implemented yet. An alternative is to check Pod status .","title":"Check Revision status"},{"location":"serving/debugging-application-issues/#check-pod-status","text":"To get the Pod s for all your deployments: kubectl get pods This command should list all Pod s with brief status. For example: NAME READY STATUS RESTARTS AGE configuration-example-00001-deployment-659747ff99-9bvr4 2/2 Running 0 3h configuration-example-00002-deployment-5f475b7849-gxcht 1/2 CrashLoopBackOff 2 36s Choose one and use the following command to see detailed information for its status . Some useful fields are conditions and containerStatuses : kubectl get pod <pod-name> --output yaml","title":"Check Pod status"},{"location":"serving/feature-flags/","text":"Feature/Extension Flags \u00b6 Knative is deliberate about the concepts it incorporates into its core API. The API aims to be portable and abstracts away the specificities of each users' implementation. That being said, the Knative API should empower users to surface extra features and extensions possible within their platform of choice. This document introduces two concepts: * Feature: a way to stage the introduction of features to the Knative API. * Extension: a way to extend Knative beyond the portable concepts of the Knative API. Control \u00b6 Features and extensions are controlled by flags defined in the config-features ConfigMap in the knative-serving namespace. Flags can have the following values: * Enabled: the feature is enabled. * Allowed: the feature may be enabled (e.g. using an annotation or looser validation). * Disabled: the feature cannot be enabled. These three states don't make sense for all features. Let's consider two types of features: multi-container and kubernetes.podspec-dryrun . multi-container allows the user to specify more than one container in the Knative Service spec. In this case, Enabled and Allowed are equivalent because using this feature requires to actually use it in the Knative Service spec. If a single container is specified, whether the feature is enabled or not doesn't change anything. kubernetes.podspec-dryrun changes the behavior of the Kubernetes implementation of the Knative API, but it has nothing to do with the Knative API itself. In this case, Enabled means the feature will be enabled unconditionally, Allowed means that the feature will be enabled only when specified with an annotation, and Disabled means that the feature cannot be used at all. Lifecyle \u00b6 Features and extensions go through 3 similar phases (Alpha, Beta, GA) but with important differences. Alpha means: * Might be buggy. Enabling the feature may expose bugs. * Support for feature may be dropped at any time without notice. * The API may change in incompatible ways in a later software release without notice. * Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta means: * The feature is well tested. Enabling the feature is considered safe. * Support for the overall feature will not be dropped, though details may change. * The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, or re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. * Recommended for only non-business-critical uses because of potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you may be able to relax this restriction. General Availability (GA) means: * Stable versions of features/extensions will appear in released software for many subsequent versions. Feature \u00b6 Features use flags to safely introduce new changes to the Knative API. Eventually, each feature will graduate to become fully part of the Knative API, and the flag guard will be removed. Alpha \u00b6 Disabled by default. Beta \u00b6 Enabled by default. GA \u00b6 The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed. Extension \u00b6 An extension may surface details of a specific Knative implementation or features of the underlying environment. It is never intended for inclusion in the core Knative API due to its lack of portability. Each extension will always be controlled by a flag and never enabled by default. Alpha \u00b6 Disabled by default. Beta \u00b6 Allowed by default. GA \u00b6 Allowed by default. Available Flags \u00b6 Multi Containers \u00b6 Type : feature ConfigMap key: multi-container This flag allows specifying multiple \"user containers\" in a Knative Service spec. Only one container can handle the requests, and therefore exactly one container must have a port specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : first-container image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 - name : second-container image : gcr.io/knative-samples/helloworld-java Kubernetes Node Affinity \u00b6 Type : extension ConfigMap key: kubernetes.podspec-affinity This extension controls whether node affinity can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name operator : In values : - e2e-az1 - e2e-az2 Kubernetes Host Aliases \u00b6 Type : extension ConfigMap key: kubernetes.podspec-hostaliases This flag controls whether host aliases can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\" Kubernetes Node Selector \u00b6 Type : extension ConfigMap key: kubernetes.podspec-nodeselector This flag controls whether node selector can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : nodeSelector : labelName : labelValue Kubernetes Toleration \u00b6 Type : extension ConfigMap key: kubernetes.podspec-tolerations This flag controls whether tolerations can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\" Kubernetes FieldRef \u00b6 Type : extension ConfigMap key: kubernetes.podspec-fieldref This flag controls whether the Downward API (env based) can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName Kubernetes Dry Run \u00b6 Type : extension ConfigMap key: kubernetes.podspec-dryrun This flag controls whether Knative will try to validate the Pod spec derived from the Knative Service spec using the Kubernetes API server before accepting the object. When \"enabled\", the server will always run the extra validation. When \"allowed\", the server will not run the dry-run validation by default. However, clients may enable the behavior on an individual Service by attaching the following metadata annotation: \"features.knative.dev/podspec-dryrun\":\"enabled\". apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/podspec-dryrun\":\"enabled ... spec : template : spec : ... Kubernetes Runtime Class \u00b6 Type : extension ConfigMap key: kubernetes.podspec-runtimeclass This flag controls whether the runtime class can be used or not. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : runtimeClassName : myclass ... Kubernetes Security Context \u00b6 Type : extension ConfigMap key: kubernetes.podspec-securitycontext This flag controls whether a subset of the security context can be used. When set to \"enabled\" or \"allowed\" it allows the following PodSecurityContext properties: - FSGroup - RunAsGroup - RunAsNonRoot - SupplementalGroups - RunAsUser When set to \"enabled\" or \"allowed\" it allows the following Container SecurityContext properties: - RunAsNonRoot - RunAsGroup - RunAsUser (already allowed without this flag) This flag should be used with caution as the PodSecurityContext properties may have a side-effect on non-user sidecar containers that come from Knative or your service mesh apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : securityContext : runAsUser : 1000 ... Responsive Revision Garbage Collector \u00b6 Type : extension ConfigMap key: responsive-revision-gc This flag controls whether new responsive garbage collection is enabled. This feature labels revisions in real-time as they become referenced and dereferenced by Routes. This allows us to reap revisions shortly after they are no longer active. Tag Header Based Routing \u00b6 Type : extension ConfigMap key: tag-header-based-routing This flags controls whether tag header based routing is enabled.","title":"Feature/Extension Flags"},{"location":"serving/feature-flags/#featureextension-flags","text":"Knative is deliberate about the concepts it incorporates into its core API. The API aims to be portable and abstracts away the specificities of each users' implementation. That being said, the Knative API should empower users to surface extra features and extensions possible within their platform of choice. This document introduces two concepts: * Feature: a way to stage the introduction of features to the Knative API. * Extension: a way to extend Knative beyond the portable concepts of the Knative API.","title":"Feature/Extension Flags"},{"location":"serving/feature-flags/#control","text":"Features and extensions are controlled by flags defined in the config-features ConfigMap in the knative-serving namespace. Flags can have the following values: * Enabled: the feature is enabled. * Allowed: the feature may be enabled (e.g. using an annotation or looser validation). * Disabled: the feature cannot be enabled. These three states don't make sense for all features. Let's consider two types of features: multi-container and kubernetes.podspec-dryrun . multi-container allows the user to specify more than one container in the Knative Service spec. In this case, Enabled and Allowed are equivalent because using this feature requires to actually use it in the Knative Service spec. If a single container is specified, whether the feature is enabled or not doesn't change anything. kubernetes.podspec-dryrun changes the behavior of the Kubernetes implementation of the Knative API, but it has nothing to do with the Knative API itself. In this case, Enabled means the feature will be enabled unconditionally, Allowed means that the feature will be enabled only when specified with an annotation, and Disabled means that the feature cannot be used at all.","title":"Control"},{"location":"serving/feature-flags/#lifecyle","text":"Features and extensions go through 3 similar phases (Alpha, Beta, GA) but with important differences. Alpha means: * Might be buggy. Enabling the feature may expose bugs. * Support for feature may be dropped at any time without notice. * The API may change in incompatible ways in a later software release without notice. * Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta means: * The feature is well tested. Enabling the feature is considered safe. * Support for the overall feature will not be dropped, though details may change. * The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, or re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. * Recommended for only non-business-critical uses because of potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you may be able to relax this restriction. General Availability (GA) means: * Stable versions of features/extensions will appear in released software for many subsequent versions.","title":"Lifecyle"},{"location":"serving/feature-flags/#feature","text":"Features use flags to safely introduce new changes to the Knative API. Eventually, each feature will graduate to become fully part of the Knative API, and the flag guard will be removed.","title":"Feature"},{"location":"serving/feature-flags/#alpha","text":"Disabled by default.","title":"Alpha"},{"location":"serving/feature-flags/#beta","text":"Enabled by default.","title":"Beta"},{"location":"serving/feature-flags/#ga","text":"The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed.","title":"GA"},{"location":"serving/feature-flags/#extension","text":"An extension may surface details of a specific Knative implementation or features of the underlying environment. It is never intended for inclusion in the core Knative API due to its lack of portability. Each extension will always be controlled by a flag and never enabled by default.","title":"Extension"},{"location":"serving/feature-flags/#alpha_1","text":"Disabled by default.","title":"Alpha"},{"location":"serving/feature-flags/#beta_1","text":"Allowed by default.","title":"Beta"},{"location":"serving/feature-flags/#ga_1","text":"Allowed by default.","title":"GA"},{"location":"serving/feature-flags/#available-flags","text":"","title":"Available Flags"},{"location":"serving/feature-flags/#multi-containers","text":"Type : feature ConfigMap key: multi-container This flag allows specifying multiple \"user containers\" in a Knative Service spec. Only one container can handle the requests, and therefore exactly one container must have a port specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : first-container image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 - name : second-container image : gcr.io/knative-samples/helloworld-java","title":"Multi Containers"},{"location":"serving/feature-flags/#kubernetes-node-affinity","text":"Type : extension ConfigMap key: kubernetes.podspec-affinity This extension controls whether node affinity can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name operator : In values : - e2e-az1 - e2e-az2","title":"Kubernetes Node Affinity"},{"location":"serving/feature-flags/#kubernetes-host-aliases","text":"Type : extension ConfigMap key: kubernetes.podspec-hostaliases This flag controls whether host aliases can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\"","title":"Kubernetes Host Aliases"},{"location":"serving/feature-flags/#kubernetes-node-selector","text":"Type : extension ConfigMap key: kubernetes.podspec-nodeselector This flag controls whether node selector can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : nodeSelector : labelName : labelValue","title":"Kubernetes Node Selector"},{"location":"serving/feature-flags/#kubernetes-toleration","text":"Type : extension ConfigMap key: kubernetes.podspec-tolerations This flag controls whether tolerations can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\"","title":"Kubernetes Toleration"},{"location":"serving/feature-flags/#kubernetes-fieldref","text":"Type : extension ConfigMap key: kubernetes.podspec-fieldref This flag controls whether the Downward API (env based) can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName","title":"Kubernetes FieldRef"},{"location":"serving/feature-flags/#kubernetes-dry-run","text":"Type : extension ConfigMap key: kubernetes.podspec-dryrun This flag controls whether Knative will try to validate the Pod spec derived from the Knative Service spec using the Kubernetes API server before accepting the object. When \"enabled\", the server will always run the extra validation. When \"allowed\", the server will not run the dry-run validation by default. However, clients may enable the behavior on an individual Service by attaching the following metadata annotation: \"features.knative.dev/podspec-dryrun\":\"enabled\". apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/podspec-dryrun\":\"enabled ... spec : template : spec : ...","title":"Kubernetes Dry Run"},{"location":"serving/feature-flags/#kubernetes-runtime-class","text":"Type : extension ConfigMap key: kubernetes.podspec-runtimeclass This flag controls whether the runtime class can be used or not. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : runtimeClassName : myclass ...","title":"Kubernetes Runtime Class"},{"location":"serving/feature-flags/#kubernetes-security-context","text":"Type : extension ConfigMap key: kubernetes.podspec-securitycontext This flag controls whether a subset of the security context can be used. When set to \"enabled\" or \"allowed\" it allows the following PodSecurityContext properties: - FSGroup - RunAsGroup - RunAsNonRoot - SupplementalGroups - RunAsUser When set to \"enabled\" or \"allowed\" it allows the following Container SecurityContext properties: - RunAsNonRoot - RunAsGroup - RunAsUser (already allowed without this flag) This flag should be used with caution as the PodSecurityContext properties may have a side-effect on non-user sidecar containers that come from Knative or your service mesh apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : securityContext : runAsUser : 1000 ...","title":"Kubernetes Security Context"},{"location":"serving/feature-flags/#responsive-revision-garbage-collector","text":"Type : extension ConfigMap key: responsive-revision-gc This flag controls whether new responsive garbage collection is enabled. This feature labels revisions in real-time as they become referenced and dereferenced by Routes. This allows us to reap revisions shortly after they are no longer active.","title":"Responsive Revision Garbage Collector"},{"location":"serving/feature-flags/#tag-header-based-routing","text":"Type : extension ConfigMap key: tag-header-based-routing This flags controls whether tag header based routing is enabled.","title":"Tag Header Based Routing"},{"location":"serving/getting-started-knative-app/","text":"Getting Started with App Deployment \u00b6 This guide shows you how to deploy an app using Knative, then interact with it using cURL requests. Before you begin \u00b6 You need: A Kubernetes cluster with Knative Serving installed . An image of the app that you'd like to deploy available on a container registry. The image of the sample app used in this guide is available on Google Container Registry. Sample application \u00b6 This guide demonstrates the basic workflow for deploying the Hello World sample app (Go) from the Google Container Registry . You can use these steps as a guide for deploying your container images from other registries like Docker Hub . To deploy a local container image, you need to disable image tag resolution by running the following command: # Set to dev.local/local-image when deploying local container images docker tag local-image dev.local/local-image Learn more about image tag resolution. The Hello World sample app reads in an env variable, TARGET , then prints \"Hello World: \\${TARGET}!\". If TARGET isn't defined, it will print \"NOT SPECIFIED\". Creating your Deployment with the Knative CLI \u00b6 The easiest way to deploy a Knative Service is by using the Knative CLI kn . Prerequisite: Install the kn binary as described in Installing the Knative CLI It will create a corresponding resource description internally as when using a YAML file directly. kn provides a command-line mechanism for managing Services. It allows you to configure every aspect of a Service. The only mandatory flag for creating a Service is --image with the container image reference as value. To create a Service directly at the cluster, use: # Create a Knative service with the Knative CLI kn kn service create helloworld-go --image gcr.io/knative-samples/helloworld-go --env TARGET = \"Go Sample v1\" If you want to deploy the sample app, leave the --image config as-is. If you're deploying an image of your app, update the name of the Service and the value of the --image flag accordingly. Now that you have deployed the service, Knative will perform the following steps: Create a new immutable revision for this version of the app. Perform network programming to create a route, ingress, service, and load balancer for your app. Automatically scale your pods up and down based on traffic, including to zero active pods. Creating your Deployment with YAML \u00b6 Alternatively, to deploy an app using Knative, you can also create the configuration in a YAML file that defines a service. For more information about the Service object, see the Resource Types documentation . This configuration file specifies metadata about the application, points to the hosted image of the app for deployment, and allows the deployment to be configured. For more information about what configuration options are available, see the Serving spec documentation . To create the same application as in the previous kn example, create a new file named service.yaml , then copy and paste the following content into it: apiVersion : serving.knative.dev/v1 # Current version of Knative kind : Service metadata : name : helloworld-go # The name of the app namespace : default # The namespace the app will use spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go # Reference to the image of the app env : - name : TARGET # The environment variable printed out by the sample app value : \"Go Sample v1\" If you want to deploy the sample app, leave the config file as-is. If you're deploying an image of your app, update the name of the Service ( .metadata.name ) and the reference to the container image ( .spec.containers[].image ) accordingly. From the directory where the new service.yaml file was created, apply the configuration: kubectl apply --filename service.yaml Now that you have deployed the service, Knative will perform the following steps: Create a new immutable revision for this version of the app. Perform network programming to create a route, ingress, service, and load balancer for your app. Automatically scale your pods up and down based on traffic, including to zero active pods. Interacting with your app \u00b6 To see if your app has been deployed successfully, you need the URL created by Knative. To find the URL for your service, use either kn or kubectl kn kn service describe helloworld-go This will return something like Name helloworld-go Namespace default Age 12m URL http://helloworld-go.default.34.83.80.117.xip.io Revisions: 100% @latest (helloworld-go-dyqsj-1) [1] (39s) Image: gcr.io/knative-samples/helloworld-go (pinned to 946b7c) Conditions: OK TYPE AGE REASON ++ Ready 25s ++ ConfigurationsReady 26s ++ RoutesReady 25s kubectl kubectl get ksvc helloworld-go The command will return the following: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.34.83.80.117.xip.io helloworld-go-96dtk helloworld-go-96dtk True Note: If your URL includes example.com then consult the setup instructions for configuring DNS (e.g. with xip.io ), or using a Custom Domain . If you changed the name from helloworld-go to something else when creating the .yaml file, replace helloworld-go in the above commands with the name you entered. Now you can make a request to your app and see the results. Replace the URL with the one returned by the command in the previous step. # curl http://helloworld-go.default.34.83.80.117.xip.io Hello World: Go Sample v1! If you deployed your app, you might want to customize this cURL request to interact with your application. It can take a few seconds for Knative to scale up your application and return a response. Note: Add -v option to get more detail if the curl command failed. You've successfully deployed your first application using Knative! Cleaning up \u00b6 To remove the sample app from your cluster, delete the service record: kn service delete helloworld-go Alternatively, you can also delete the service with kubectl via the definition file or by name. # Delete with the KService given in the yaml file: kubectl delete --filename service.yaml # Or just delete it by name: kubectl delete kservice helloworld-go","title":"Getting Started with App Deployment"},{"location":"serving/getting-started-knative-app/#getting-started-with-app-deployment","text":"This guide shows you how to deploy an app using Knative, then interact with it using cURL requests.","title":"Getting Started with App Deployment"},{"location":"serving/getting-started-knative-app/#before-you-begin","text":"You need: A Kubernetes cluster with Knative Serving installed . An image of the app that you'd like to deploy available on a container registry. The image of the sample app used in this guide is available on Google Container Registry.","title":"Before you begin"},{"location":"serving/getting-started-knative-app/#sample-application","text":"This guide demonstrates the basic workflow for deploying the Hello World sample app (Go) from the Google Container Registry . You can use these steps as a guide for deploying your container images from other registries like Docker Hub . To deploy a local container image, you need to disable image tag resolution by running the following command: # Set to dev.local/local-image when deploying local container images docker tag local-image dev.local/local-image Learn more about image tag resolution. The Hello World sample app reads in an env variable, TARGET , then prints \"Hello World: \\${TARGET}!\". If TARGET isn't defined, it will print \"NOT SPECIFIED\".","title":"Sample application"},{"location":"serving/getting-started-knative-app/#creating-your-deployment-with-the-knative-cli","text":"The easiest way to deploy a Knative Service is by using the Knative CLI kn . Prerequisite: Install the kn binary as described in Installing the Knative CLI It will create a corresponding resource description internally as when using a YAML file directly. kn provides a command-line mechanism for managing Services. It allows you to configure every aspect of a Service. The only mandatory flag for creating a Service is --image with the container image reference as value. To create a Service directly at the cluster, use: # Create a Knative service with the Knative CLI kn kn service create helloworld-go --image gcr.io/knative-samples/helloworld-go --env TARGET = \"Go Sample v1\" If you want to deploy the sample app, leave the --image config as-is. If you're deploying an image of your app, update the name of the Service and the value of the --image flag accordingly. Now that you have deployed the service, Knative will perform the following steps: Create a new immutable revision for this version of the app. Perform network programming to create a route, ingress, service, and load balancer for your app. Automatically scale your pods up and down based on traffic, including to zero active pods.","title":"Creating your Deployment with the Knative CLI"},{"location":"serving/getting-started-knative-app/#creating-your-deployment-with-yaml","text":"Alternatively, to deploy an app using Knative, you can also create the configuration in a YAML file that defines a service. For more information about the Service object, see the Resource Types documentation . This configuration file specifies metadata about the application, points to the hosted image of the app for deployment, and allows the deployment to be configured. For more information about what configuration options are available, see the Serving spec documentation . To create the same application as in the previous kn example, create a new file named service.yaml , then copy and paste the following content into it: apiVersion : serving.knative.dev/v1 # Current version of Knative kind : Service metadata : name : helloworld-go # The name of the app namespace : default # The namespace the app will use spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go # Reference to the image of the app env : - name : TARGET # The environment variable printed out by the sample app value : \"Go Sample v1\" If you want to deploy the sample app, leave the config file as-is. If you're deploying an image of your app, update the name of the Service ( .metadata.name ) and the reference to the container image ( .spec.containers[].image ) accordingly. From the directory where the new service.yaml file was created, apply the configuration: kubectl apply --filename service.yaml Now that you have deployed the service, Knative will perform the following steps: Create a new immutable revision for this version of the app. Perform network programming to create a route, ingress, service, and load balancer for your app. Automatically scale your pods up and down based on traffic, including to zero active pods.","title":"Creating your Deployment with YAML"},{"location":"serving/getting-started-knative-app/#interacting-with-your-app","text":"To see if your app has been deployed successfully, you need the URL created by Knative. To find the URL for your service, use either kn or kubectl kn kn service describe helloworld-go This will return something like Name helloworld-go Namespace default Age 12m URL http://helloworld-go.default.34.83.80.117.xip.io Revisions: 100% @latest (helloworld-go-dyqsj-1) [1] (39s) Image: gcr.io/knative-samples/helloworld-go (pinned to 946b7c) Conditions: OK TYPE AGE REASON ++ Ready 25s ++ ConfigurationsReady 26s ++ RoutesReady 25s kubectl kubectl get ksvc helloworld-go The command will return the following: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.34.83.80.117.xip.io helloworld-go-96dtk helloworld-go-96dtk True Note: If your URL includes example.com then consult the setup instructions for configuring DNS (e.g. with xip.io ), or using a Custom Domain . If you changed the name from helloworld-go to something else when creating the .yaml file, replace helloworld-go in the above commands with the name you entered. Now you can make a request to your app and see the results. Replace the URL with the one returned by the command in the previous step. # curl http://helloworld-go.default.34.83.80.117.xip.io Hello World: Go Sample v1! If you deployed your app, you might want to customize this cURL request to interact with your application. It can take a few seconds for Knative to scale up your application and return a response. Note: Add -v option to get more detail if the curl command failed. You've successfully deployed your first application using Knative!","title":"Interacting with your app"},{"location":"serving/getting-started-knative-app/#cleaning-up","text":"To remove the sample app from your cluster, delete the service record: kn service delete helloworld-go Alternatively, you can also delete the service with kubectl via the definition file or by name. # Delete with the KService given in the yaml file: kubectl delete --filename service.yaml # Or just delete it by name: kubectl delete kservice helloworld-go","title":"Cleaning up"},{"location":"serving/gke-assigning-static-ip-address/","text":"Assigning a static IP address for Knative on Kubernetes Engine \u00b6 If you are running Knative on Google Kubernetes Engine and want to use a custom domain with your apps, you need to configure a static IP address to ensure that your custom domain mapping doesn't break. Knative configures an Istio Gateway CRD named knative-ingress-gateway under the knative-serving namespace to serve all incoming traffic within the Knative service mesh. The IP address to access the gateway is the external IP address of the \"istio-ingressgateway\" service under the istio-system namespace. Therefore, in order to set a static IP for the gateway you must to set the external IP address of the istio-ingressgateway service to a static IP. If you have configured a custom ingress gateway , replace istio-ingressgateway with the name of your gateway service in the steps below. Step 1: Reserve a static IP address \u00b6 You can reserve a regional static IP address using the Google Cloud SDK or the Google Cloud Platform console. Using the Google Cloud SDK: Enter the following command, replacing IP_NAME and REGION with appropriate values. For example, select the us-west1 region if you deployed your cluster to the us-west1-c zone: gcloud beta compute addresses create IP_NAME --region = REGION For example: gcloud beta compute addresses create knative-ip --region = us-west1 Enter the following command to get the newly created static IP address: gcloud beta compute addresses list In the GCP console : Enter a name for your static address. For IP version , choose IPv4. For Type , choose Regional . From the Region drop-down, choose the region where your Knative cluster is running. For example, select the us-west1 region if you deployed your cluster to the us-west1-c zone. Leave the Attached To field set to None since we'll attach the IP address through a config-map later. Copy the External Address of the static IP you created. Step 2: Update the external IP of istio-ingressgateway service \u00b6 Run following command to configure the external IP of the istio-ingressgateway service to the static IP that you reserved: INGRESSGATEWAY = istio-ingressgateway kubectl patch svc $INGRESSGATEWAY --namespace istio-system --patch '{\"spec\": { \"loadBalancerIP\": \"<your-reserved-static-ip>\" }}' Step 3: Verify the static IP address of istio-ingressgateway service \u00b6 Run the following command to ensure that the external IP of the ingressgateway service has been updated: kubectl get svc $INGRESSGATEWAY --namespace istio-system The output should show the assigned static IP address under the EXTERNAL-IP column: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE xxxxxxx-ingressgateway LoadBalancer 12.34.567.890 98.765.43.210 80:32380/TCP,443:32390/TCP,32400:32400/TCP 5m Note: Updating the external IP address can take several minutes. The external IP address should have a value now in the In use by column and should not be None anymore:","title":"Assigning static IPs - GKE"},{"location":"serving/gke-assigning-static-ip-address/#assigning-a-static-ip-address-for-knative-on-kubernetes-engine","text":"If you are running Knative on Google Kubernetes Engine and want to use a custom domain with your apps, you need to configure a static IP address to ensure that your custom domain mapping doesn't break. Knative configures an Istio Gateway CRD named knative-ingress-gateway under the knative-serving namespace to serve all incoming traffic within the Knative service mesh. The IP address to access the gateway is the external IP address of the \"istio-ingressgateway\" service under the istio-system namespace. Therefore, in order to set a static IP for the gateway you must to set the external IP address of the istio-ingressgateway service to a static IP. If you have configured a custom ingress gateway , replace istio-ingressgateway with the name of your gateway service in the steps below.","title":"Assigning a static IP address for Knative on Kubernetes Engine"},{"location":"serving/gke-assigning-static-ip-address/#step-1-reserve-a-static-ip-address","text":"You can reserve a regional static IP address using the Google Cloud SDK or the Google Cloud Platform console. Using the Google Cloud SDK: Enter the following command, replacing IP_NAME and REGION with appropriate values. For example, select the us-west1 region if you deployed your cluster to the us-west1-c zone: gcloud beta compute addresses create IP_NAME --region = REGION For example: gcloud beta compute addresses create knative-ip --region = us-west1 Enter the following command to get the newly created static IP address: gcloud beta compute addresses list In the GCP console : Enter a name for your static address. For IP version , choose IPv4. For Type , choose Regional . From the Region drop-down, choose the region where your Knative cluster is running. For example, select the us-west1 region if you deployed your cluster to the us-west1-c zone. Leave the Attached To field set to None since we'll attach the IP address through a config-map later. Copy the External Address of the static IP you created.","title":"Step 1: Reserve a static IP address"},{"location":"serving/gke-assigning-static-ip-address/#step-2-update-the-external-ip-of-istio-ingressgateway-service","text":"Run following command to configure the external IP of the istio-ingressgateway service to the static IP that you reserved: INGRESSGATEWAY = istio-ingressgateway kubectl patch svc $INGRESSGATEWAY --namespace istio-system --patch '{\"spec\": { \"loadBalancerIP\": \"<your-reserved-static-ip>\" }}'","title":"Step 2: Update the external IP of istio-ingressgateway service"},{"location":"serving/gke-assigning-static-ip-address/#step-3-verify-the-static-ip-address-of-istio-ingressgateway-service","text":"Run the following command to ensure that the external IP of the ingressgateway service has been updated: kubectl get svc $INGRESSGATEWAY --namespace istio-system The output should show the assigned static IP address under the EXTERNAL-IP column: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE xxxxxxx-ingressgateway LoadBalancer 12.34.567.890 98.765.43.210 80:32380/TCP,443:32390/TCP,32400:32400/TCP 5m Note: Updating the external IP address can take several minutes. The external IP address should have a value now in the In use by column and should not be None anymore:","title":"Step 3: Verify the static IP address of istio-ingressgateway service"},{"location":"serving/installing-cert-manager/","text":"Installing cert-manager for TLS certificates \u00b6 Install the Cert-Manager tool to obtain TLS certificates that you can use for secure HTTPS connections in Knative. For more information about enabling HTTPS connections in Knative, see Configuring HTTPS with TLS certificates . You can use cert-manager to either manually obtain certificates, or to enable Knative for automatic certificate provisioning. Complete instructions about automatic certificate provisioning are provided in Enabling automatic TLS cert provisioning . Regardless of if your want to manually obtain certificates, or configure Knative for automatic provisioning, you can use the following steps to install cert-manager. Before you begin \u00b6 You must meet the following requirements to install cert-manager for Knative: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Knative currently supports cert-manager version 1.0.0 and higher. Downloading and installing cert-manager \u00b6 Follow the steps from the official cert-manager website to download and install cert-manager Installation steps Completing the Knative configuration for TLS support \u00b6 Before you can use a TLS certificate for secure connections, you must finish configuring Knative: Manual : If you installed cert-manager to manually obtain certificates, continue to the following topic for instructions about creating a Kubernetes secret: Manually adding a TLS certificate Automatic : If you installed cert-manager to use for automatic certificate provisioning, continue to the following topic to enable that feature: Enabling automatic TLS certificate provisioning in Knative","title":"Installing cert-manager"},{"location":"serving/installing-cert-manager/#installing-cert-manager-for-tls-certificates","text":"Install the Cert-Manager tool to obtain TLS certificates that you can use for secure HTTPS connections in Knative. For more information about enabling HTTPS connections in Knative, see Configuring HTTPS with TLS certificates . You can use cert-manager to either manually obtain certificates, or to enable Knative for automatic certificate provisioning. Complete instructions about automatic certificate provisioning are provided in Enabling automatic TLS cert provisioning . Regardless of if your want to manually obtain certificates, or configure Knative for automatic provisioning, you can use the following steps to install cert-manager.","title":"Installing cert-manager for TLS certificates"},{"location":"serving/installing-cert-manager/#before-you-begin","text":"You must meet the following requirements to install cert-manager for Knative: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Knative currently supports cert-manager version 1.0.0 and higher.","title":"Before you begin"},{"location":"serving/installing-cert-manager/#downloading-and-installing-cert-manager","text":"Follow the steps from the official cert-manager website to download and install cert-manager Installation steps","title":"Downloading and installing cert-manager"},{"location":"serving/installing-cert-manager/#completing-the-knative-configuration-for-tls-support","text":"Before you can use a TLS certificate for secure connections, you must finish configuring Knative: Manual : If you installed cert-manager to manually obtain certificates, continue to the following topic for instructions about creating a Kubernetes secret: Manually adding a TLS certificate Automatic : If you installed cert-manager to use for automatic certificate provisioning, continue to the following topic to enable that feature: Enabling automatic TLS certificate provisioning in Knative","title":"Completing the Knative configuration for TLS support"},{"location":"serving/istio-authorization/","text":"Enabling requests to Knative services when additional authorization policies are enabled \u00b6 Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods. Before you begin \u00b6 You must meet the following prerequisites to use Istio AuthorizationPolicy: Istio must be used for your Knative Ingress. See Install a networking layer . Istio sidecar injection must be enabled. See the Istio Documentation . Mutual TLS in Knative \u00b6 Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS. Generally, mutual TLS can be configured normally as in Istio's documentation . However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the knative-serving namespace: kubectl label namespace knative-serving istio-injection=enabled If the activator isn't injected: In PERMISSIVE mode, you'll see requests appear without the expected X-Forwarded-Client-Cert header when forwarded by the activator. $ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers { \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip\", \"Forwarded\": \"for=10.72.0.30;proto=http\", \"Host\": \"httpbin.knative.svc.cluster.local\", \"K-Proxy-Request\": \"activator\", \"User-Agent\": \"curl/7.58.0\", \"X-B3-Parentspanid\": \"b240bdb1c29ae638\", \"X-B3-Sampled\": \"0\", \"X-B3-Spanid\": \"416960c27be6d484\", \"X-B3-Traceid\": \"750362ce9d878281b240bdb1c29ae638\", \"X-Envoy-Attempt-Count\": \"1\", \"X-Envoy-Internal\": \"true\" } } In STRICT mode, requests will simply be rejected. To understand when requests are forwarded through the activator, see documentation on the TargetBurstCapacity setting. This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator. For example, the following policy allows requests from within pods in the serving-tests namespace to other pods in the serving-tests namespace. apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allow-serving-tests namespace: serving-tests spec: action: ALLOW rules: - from: - source: namespaces: [\"serving-tests\"] Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as knative-serving , which is the namespace of the activator. Currently, the easiest way around this is to explicitly allow requests from the knative-serving namespace, for example by adding it to the list in the above policy: apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allow-serving-tests namespace: serving-tests spec: action: ALLOW rules: - from: - source: namespaces: [\"serving-tests\", \"knative-serving\"] Health checking and metrics collection \u00b6 In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths . Allowing access from system pods by paths \u00b6 Knative system pods access your application using the following paths: /metrics /healthz The /metrics path allows the autoscaler pod to collect metrics. The /healthz path allows system pods to probe the service. You can add the /metrics and /healthz paths to the AuthorizationPolicy as shown in the example: $ cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allowlist-by-paths namespace: serving-tests spec: action: ALLOW rules: - to: - operation: paths: - /metrics # The path to collect metrics by system pod. - /healthz # The path to probe by system pod. EOF","title":"Enabling requests to Knative services when additional authorization policies are enabled"},{"location":"serving/istio-authorization/#enabling-requests-to-knative-services-when-additional-authorization-policies-are-enabled","text":"Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods.","title":"Enabling requests to Knative services when additional authorization policies are enabled"},{"location":"serving/istio-authorization/#before-you-begin","text":"You must meet the following prerequisites to use Istio AuthorizationPolicy: Istio must be used for your Knative Ingress. See Install a networking layer . Istio sidecar injection must be enabled. See the Istio Documentation .","title":"Before you begin"},{"location":"serving/istio-authorization/#mutual-tls-in-knative","text":"Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS. Generally, mutual TLS can be configured normally as in Istio's documentation . However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the knative-serving namespace: kubectl label namespace knative-serving istio-injection=enabled If the activator isn't injected: In PERMISSIVE mode, you'll see requests appear without the expected X-Forwarded-Client-Cert header when forwarded by the activator. $ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers { \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip\", \"Forwarded\": \"for=10.72.0.30;proto=http\", \"Host\": \"httpbin.knative.svc.cluster.local\", \"K-Proxy-Request\": \"activator\", \"User-Agent\": \"curl/7.58.0\", \"X-B3-Parentspanid\": \"b240bdb1c29ae638\", \"X-B3-Sampled\": \"0\", \"X-B3-Spanid\": \"416960c27be6d484\", \"X-B3-Traceid\": \"750362ce9d878281b240bdb1c29ae638\", \"X-Envoy-Attempt-Count\": \"1\", \"X-Envoy-Internal\": \"true\" } } In STRICT mode, requests will simply be rejected. To understand when requests are forwarded through the activator, see documentation on the TargetBurstCapacity setting. This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator. For example, the following policy allows requests from within pods in the serving-tests namespace to other pods in the serving-tests namespace. apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allow-serving-tests namespace: serving-tests spec: action: ALLOW rules: - from: - source: namespaces: [\"serving-tests\"] Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as knative-serving , which is the namespace of the activator. Currently, the easiest way around this is to explicitly allow requests from the knative-serving namespace, for example by adding it to the list in the above policy: apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allow-serving-tests namespace: serving-tests spec: action: ALLOW rules: - from: - source: namespaces: [\"serving-tests\", \"knative-serving\"]","title":"Mutual TLS in Knative"},{"location":"serving/istio-authorization/#health-checking-and-metrics-collection","text":"In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths .","title":"Health checking and metrics collection"},{"location":"serving/istio-authorization/#allowing-access-from-system-pods-by-paths","text":"Knative system pods access your application using the following paths: /metrics /healthz The /metrics path allows the autoscaler pod to collect metrics. The /healthz path allows system pods to probe the service. You can add the /metrics and /healthz paths to the AuthorizationPolicy as shown in the example: $ cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allowlist-by-paths namespace: serving-tests spec: action: ALLOW rules: - to: - operation: paths: - /metrics # The path to collect metrics by system pod. - /healthz # The path to probe by system pod. EOF","title":"Allowing access from system pods by paths"},{"location":"serving/knative-kubernetes-services/","text":"Kubernetes services \u00b6 This guide describes the Kubernetes Services that are active when running Knative Serving. Before You Begin \u00b6 This guide assumes that you have installed Knative Serving. If you have not, instructions on how to do this are located here . Verify that you have the proper components in your cluster. To view the services installed in your cluster, use the command: $ kubectl get services -n knative-serving This should return the following output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE activator-service ClusterIP 10 .96.61.11 <none> 80 /TCP,81/TCP,9090/TCP 1h autoscaler ClusterIP 10 .104.217.223 <none> 8080 /TCP,9090/TCP 1h controller ClusterIP 10 .101.39.220 <none> 9090 /TCP 1h webhook ClusterIP 10 .107.144.50 <none> 443 /TCP 1h To view the deployments in your cluster, use the following command: $ kubectl get deployments -n knative-serving This should return the following output: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE activator 1 1 1 1 1h autoscaler 1 1 1 1 1h controller 1 1 1 1 1h networking-certmanager 1 1 1 1 1h networking-istio 1 1 1 1 1h webhook 1 1 1 1 1h These services and deployments are installed by the serving.yaml file during install. The next section describes their function. Components \u00b6 Service: activator \u00b6 The activator is responsible for receiving & buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics. Service: autoscaler \u00b6 The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic. Service: controller \u00b6 The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs). Service: webhook \u00b6 The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsitent and invalid objects, and validates and mutates Kubernetes API calls. Deployment: networking-certmanager \u00b6 The certmanager reconciles cluster ingresses into cert manager objects. Deployment: networking-istio \u00b6 The networking-istio deployment reconciles a cluster's ingress into an Istio virtual service . What's Next \u00b6 For a deeper look at the services and deployments involved in Knative Serving, click here . For a high-level analysis of Serving, look at the documentation here . Check out the Knative Serving code samples here for more hands-on tutorials.","title":"Kubernetes services"},{"location":"serving/knative-kubernetes-services/#kubernetes-services","text":"This guide describes the Kubernetes Services that are active when running Knative Serving.","title":"Kubernetes services"},{"location":"serving/knative-kubernetes-services/#before-you-begin","text":"This guide assumes that you have installed Knative Serving. If you have not, instructions on how to do this are located here . Verify that you have the proper components in your cluster. To view the services installed in your cluster, use the command: $ kubectl get services -n knative-serving This should return the following output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE activator-service ClusterIP 10 .96.61.11 <none> 80 /TCP,81/TCP,9090/TCP 1h autoscaler ClusterIP 10 .104.217.223 <none> 8080 /TCP,9090/TCP 1h controller ClusterIP 10 .101.39.220 <none> 9090 /TCP 1h webhook ClusterIP 10 .107.144.50 <none> 443 /TCP 1h To view the deployments in your cluster, use the following command: $ kubectl get deployments -n knative-serving This should return the following output: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE activator 1 1 1 1 1h autoscaler 1 1 1 1 1h controller 1 1 1 1 1h networking-certmanager 1 1 1 1 1h networking-istio 1 1 1 1 1h webhook 1 1 1 1 1h These services and deployments are installed by the serving.yaml file during install. The next section describes their function.","title":"Before You Begin"},{"location":"serving/knative-kubernetes-services/#components","text":"","title":"Components"},{"location":"serving/knative-kubernetes-services/#service-activator","text":"The activator is responsible for receiving & buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics.","title":"Service: activator"},{"location":"serving/knative-kubernetes-services/#service-autoscaler","text":"The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic.","title":"Service: autoscaler"},{"location":"serving/knative-kubernetes-services/#service-controller","text":"The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs).","title":"Service: controller"},{"location":"serving/knative-kubernetes-services/#service-webhook","text":"The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsitent and invalid objects, and validates and mutates Kubernetes API calls.","title":"Service: webhook"},{"location":"serving/knative-kubernetes-services/#deployment-networking-certmanager","text":"The certmanager reconciles cluster ingresses into cert manager objects.","title":"Deployment: networking-certmanager"},{"location":"serving/knative-kubernetes-services/#deployment-networking-istio","text":"The networking-istio deployment reconciles a cluster's ingress into an Istio virtual service .","title":"Deployment: networking-istio"},{"location":"serving/knative-kubernetes-services/#whats-next","text":"For a deeper look at the services and deployments involved in Knative Serving, click here . For a high-level analysis of Serving, look at the documentation here . Check out the Knative Serving code samples here for more hands-on tutorials.","title":"What's Next"},{"location":"serving/metrics/","text":"Metrics API \u00b6 NOTE: The metrics API may change in the future, this serves as a snapshot of the current metrics. Admin \u00b6 Administrators can monitor Serving control plane based on the metrics exposed by each Serving component. Metrics are listed next. Activator \u00b6 The following metrics allow the user to understand how application responds when traffic goes through the activator eg. scaling from zero. For example high request latency means that requests are taken too much time be fulfilled. | Metric Name | Description | Type | Tags | Unit | Status | |:-|:-|:-|:-|:-|:-| | request_concurrency | Concurrent requests that are routed to Activator These are requests reported by the concurrency reporter which may not be done yet. This is the average concurrency over a reporting period | Gauge | configuration_name container_name namespace_name pod_name revision_name service_name | Dimensionless | Stable | | request_count | The number of requests that are routed to Activator. These are requests that have been fulfilled from the activator handler. | Counter | configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name | Dimensionless | Stable | | request_latencies | The response time in millisecond for the fulfilled routed requests | Histogram | configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name | Milliseconds | Stable | Autoscaler \u00b6 Autoscaler component exposes a number of metrics related to its decisions per revision. For example at any given time user can monitor the desired pods the Autoscaler wants to allocate for a service, the average number of requests per second during the stable window, whether autoscaler is in panic mode (KPA) etc. To read more about how autoscaler works check here . | Metric Name | Description | Type | Tags | Unit | Status | |:-|:-|:-|:-|:-|:-| | desired_pods | Number of pods autoscaler wants to allocate | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | excess_burst_capacity | Excess burst capacity overserved over the stable window | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | stable_request_concurrency | Average of requests count per observed pod over the stable window | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | panic_request_concurrency | Average of requests count per observed pod over the panic window | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | target_concurrency_per_pod | The desired number of concurrent requests for each pod | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | stable_requests_per_second | Average requests-per-second per observed pod over the stable window | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | panic_requests_per_second | Average requests-per-second per observed pod over the panic window | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | target_requests_per_second | The desired requests-per-second for each pod | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | panic_mode | 1 if autoscaler is in panic mode, 0 otherwise | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | requested_pods | Number of pods autoscaler requested from Kubernetes | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | actual_pods | Number of pods that are allocated currently in ready state | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | not_ready_pods | Number of pods that are not ready currently | Gauge | configuration_name= namespace_name= revision_name service_name | Dimensionless | Stable | | pending_pods | Number of pods that are pending currently | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | terminating_pods | Number of pods that are terminating currently | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | Controller \u00b6 The following metrics are emitted by any component that implements a controller logic. The metrics show details about the reconciliation operations and the workqueue behavior on which reconciliation requests are enqueued. Metric Name Description Type Tags Unit Status work_queue_depth Depth of the work queue Gauge reconciler Dimensionless Stable reconcile_count Number of reconcile operations Counter reconciler success Dimensionless Stable reconcile_latency Latency of reconcile operations Histogram reconciler success Milliseconds Stable workqueue_adds_total Total number of adds handled by workqueue Counter name Dimensionless Stable workqueue_depth Current depth of workqueue Gauge reconciler Dimensionless Stable workqueue_queue_latency_seconds How long in seconds an item stays in workqueue before being requested Histogram name Seconds Stable workqueue_retries_total Total number of retries handled by workqueue Counter name Dimensionless Stable workqueue_work_duration_seconds How long in seconds processing an item from a workqueue takes. Histogram name Seconds Stable workqueue_unfinished_work_seconds How long in seconds the outstanding workqueue items have been in flight (total). Histogram name Seconds Stable workqueue_longest_running_processor_seconds How long in seconds the longest outstanding workqueue item has been in flight Histogram name Seconds Stable Webhook \u00b6 Webhook metrics report useful info about operations eg. CREATE on Serving resources and if admission was allowed. For example if a big number of operations fail this could be an issue with the submitted user resource. | Metric Name | Description | Type | Tags | Unit | Status | |:-|:-|:-|:-|:-|:-| | request_count | The number of requests that are routed to webhook | Counter | admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version | Dimensionless | Stable | | request_latencies | The response time in milliseconds | Histogram | admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version | Milliseconds | Stable | Go Runtime - memstats \u00b6 Each Knative Serving control plane process emits a number of Go runtime memory statistics (shown next). As a baseline for monitoring purproses, user could start with a subset of the metrics: current allocations (go_alloc), total allocations (go_total_alloc), system memory (go_sys), mallocs (go_mallocs), frees (go_frees) and garbage collection total pause time (total_gc_pause_ns), next gc target heap size (go_next_gc) and number of garbage collection cycles (num_gc). | Metric Name | Description | Type | Tags | Unit | Status | |:-|:-|:-|:-|:-|:-| | go_alloc | The number of bytes of allocated heap objects (same as heap_alloc) | Gauge | name | Dimensionless | Stable | | go_total_alloc | The cumulative bytes allocated for heap objects | Gauge | name | Dimensionless | Stable | | go_sys | The total bytes of memory obtained from the OS | Gauge | name | Dimensionless | Stable | | go_lookups | The number of pointer lookups performed by the runtime | Gauge | name | Dimensionless | Stable | | go_mallocs | The cumulative count of heap objects allocated | Gauge | name | Dimensionless | Stable | | go_frees | The cumulative count of heap objects freed | Gauge | name | Dimensionless | Stable | | go_heap_alloc | The number of bytes of allocated heap objects | Gauge | name | Dimensionless | Stable | | go_heap_sys | The number of bytes of heap memory obtained from the OS | Gauge | name | Dimensionless | Stable | | go_heap_idle | The number of bytes in idle (unused) spans | Gauge | name | Dimensionless | Stable | | go_heap_in_use | The number of bytes in in-use spans | Gauge | name | Dimensionless | Stable | | go_heap_released | The number of bytes of physical memory returned to the OS | Gauge | name | Dimensionless | Stable | | go_heap_objects | The number of allocated heap objects | Gauge | name | Dimensionless | Stable | | go_stack_in_use | The number of bytes in stack spans | Gauge | name | Dimensionless | Stable | | go_stack_sys | The number of bytes of stack memory obtained from the OS | Gauge | name | Dimensionless | Stable | | go_mspan_in_use | The number of bytes of allocated mspan structures | Gauge | name | Dimensionless | Stable | | go_mspan_sys | The number of bytes of memory obtained from the OS for mspan structures | Gauge | name | Dimensionless | Stable | | go_mcache_in_use | The number of bytes of allocated mcache structures | Gauge | name | Dimensionless | Stable | | go_mcache_sys | The number of bytes of memory obtained from the OS for mcache structures | Gauge | name | Dimensionless | Stable | | go_bucket_hash_sys | The number of bytes of memory in profiling bucket hash tables. | Gauge | name | Dimensionless | Stable | | go_gc_sys | The number of bytes of memory in garbage collection metadata | Gauge | name | Dimensionless | Stable | | go_other_sys | The number of bytes of memory in miscellaneous off-heap runtime allocations | Gauge | name | Dimensionless | Stable | | go_next_gc | The target heap size of the next GC cycle | Gauge | name | Dimensionless | Stable | | go_last_gc | The time the last garbage collection finished, as nanoseconds since 1970 (the UNIX epoch) | Gauge | name | Nanoseconds | Stable | | go_total_gc_pause_ns | The cumulative nanoseconds in GC stop-the-world pauses since the program started | Gauge | name | Nanoseconds | Stable | | go_num_gc | The number of completed GC cycles. | Gauge | name | Dimensionless | Stable | | go_num_forced_gc | The number of GC cycles that were forced by the application calling the GC function. | Gauge | name | Dimensionless | Stable | | go_gc_cpu_fraction | The fraction of this program's available CPU time used by the GC since the program started | Gauge | name | Dimensionless | Stable | NOTE: name tag is empty. Developer - User Services \u00b6 Every Knative service has a proxy container that proxies the connections to the application container. A number of metrics are reported for the queue peroxy performance. Using the following metrics application developers, devops and others, could measure if requests are queued at the proxy side (need for backpressure) and what is the actual delay in serving requests at the application side. Queue proxy \u00b6 Requests endpoint Metric Name Description Type Tags Unit Status revision_request_count The number of requests that are routed to queue-proxy Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_request_latencies The response time in millisecond Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_app_request_count The number of requests that are routed to user-container Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_app_request_latencies The response time in millisecond Histogram configuration_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_queue_depth The current number of items in the serving and waiting queue, or not reported if unlimited concurrency Gauge configuration_name event-display container_name namespace_name pod_name response_code_class revision_name service_name Dimensionless Stable","title":"Metrics API"},{"location":"serving/metrics/#metrics-api","text":"NOTE: The metrics API may change in the future, this serves as a snapshot of the current metrics.","title":"Metrics API"},{"location":"serving/metrics/#admin","text":"Administrators can monitor Serving control plane based on the metrics exposed by each Serving component. Metrics are listed next.","title":"Admin"},{"location":"serving/metrics/#activator","text":"The following metrics allow the user to understand how application responds when traffic goes through the activator eg. scaling from zero. For example high request latency means that requests are taken too much time be fulfilled. | Metric Name | Description | Type | Tags | Unit | Status | |:-|:-|:-|:-|:-|:-| | request_concurrency | Concurrent requests that are routed to Activator These are requests reported by the concurrency reporter which may not be done yet. This is the average concurrency over a reporting period | Gauge | configuration_name container_name namespace_name pod_name revision_name service_name | Dimensionless | Stable | | request_count | The number of requests that are routed to Activator. These are requests that have been fulfilled from the activator handler. | Counter | configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name | Dimensionless | Stable | | request_latencies | The response time in millisecond for the fulfilled routed requests | Histogram | configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name | Milliseconds | Stable |","title":"Activator"},{"location":"serving/metrics/#autoscaler","text":"Autoscaler component exposes a number of metrics related to its decisions per revision. For example at any given time user can monitor the desired pods the Autoscaler wants to allocate for a service, the average number of requests per second during the stable window, whether autoscaler is in panic mode (KPA) etc. To read more about how autoscaler works check here . | Metric Name | Description | Type | Tags | Unit | Status | |:-|:-|:-|:-|:-|:-| | desired_pods | Number of pods autoscaler wants to allocate | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | excess_burst_capacity | Excess burst capacity overserved over the stable window | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | stable_request_concurrency | Average of requests count per observed pod over the stable window | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | panic_request_concurrency | Average of requests count per observed pod over the panic window | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | target_concurrency_per_pod | The desired number of concurrent requests for each pod | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | stable_requests_per_second | Average requests-per-second per observed pod over the stable window | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | panic_requests_per_second | Average requests-per-second per observed pod over the panic window | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | target_requests_per_second | The desired requests-per-second for each pod | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | panic_mode | 1 if autoscaler is in panic mode, 0 otherwise | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | requested_pods | Number of pods autoscaler requested from Kubernetes | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | actual_pods | Number of pods that are allocated currently in ready state | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | not_ready_pods | Number of pods that are not ready currently | Gauge | configuration_name= namespace_name= revision_name service_name | Dimensionless | Stable | | pending_pods | Number of pods that are pending currently | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable | | terminating_pods | Number of pods that are terminating currently | Gauge | configuration_name namespace_name revision_name service_name | Dimensionless | Stable |","title":"Autoscaler"},{"location":"serving/metrics/#controller","text":"The following metrics are emitted by any component that implements a controller logic. The metrics show details about the reconciliation operations and the workqueue behavior on which reconciliation requests are enqueued. Metric Name Description Type Tags Unit Status work_queue_depth Depth of the work queue Gauge reconciler Dimensionless Stable reconcile_count Number of reconcile operations Counter reconciler success Dimensionless Stable reconcile_latency Latency of reconcile operations Histogram reconciler success Milliseconds Stable workqueue_adds_total Total number of adds handled by workqueue Counter name Dimensionless Stable workqueue_depth Current depth of workqueue Gauge reconciler Dimensionless Stable workqueue_queue_latency_seconds How long in seconds an item stays in workqueue before being requested Histogram name Seconds Stable workqueue_retries_total Total number of retries handled by workqueue Counter name Dimensionless Stable workqueue_work_duration_seconds How long in seconds processing an item from a workqueue takes. Histogram name Seconds Stable workqueue_unfinished_work_seconds How long in seconds the outstanding workqueue items have been in flight (total). Histogram name Seconds Stable workqueue_longest_running_processor_seconds How long in seconds the longest outstanding workqueue item has been in flight Histogram name Seconds Stable","title":"Controller"},{"location":"serving/metrics/#webhook","text":"Webhook metrics report useful info about operations eg. CREATE on Serving resources and if admission was allowed. For example if a big number of operations fail this could be an issue with the submitted user resource. | Metric Name | Description | Type | Tags | Unit | Status | |:-|:-|:-|:-|:-|:-| | request_count | The number of requests that are routed to webhook | Counter | admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version | Dimensionless | Stable | | request_latencies | The response time in milliseconds | Histogram | admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version | Milliseconds | Stable |","title":"Webhook"},{"location":"serving/metrics/#go-runtime-memstats","text":"Each Knative Serving control plane process emits a number of Go runtime memory statistics (shown next). As a baseline for monitoring purproses, user could start with a subset of the metrics: current allocations (go_alloc), total allocations (go_total_alloc), system memory (go_sys), mallocs (go_mallocs), frees (go_frees) and garbage collection total pause time (total_gc_pause_ns), next gc target heap size (go_next_gc) and number of garbage collection cycles (num_gc). | Metric Name | Description | Type | Tags | Unit | Status | |:-|:-|:-|:-|:-|:-| | go_alloc | The number of bytes of allocated heap objects (same as heap_alloc) | Gauge | name | Dimensionless | Stable | | go_total_alloc | The cumulative bytes allocated for heap objects | Gauge | name | Dimensionless | Stable | | go_sys | The total bytes of memory obtained from the OS | Gauge | name | Dimensionless | Stable | | go_lookups | The number of pointer lookups performed by the runtime | Gauge | name | Dimensionless | Stable | | go_mallocs | The cumulative count of heap objects allocated | Gauge | name | Dimensionless | Stable | | go_frees | The cumulative count of heap objects freed | Gauge | name | Dimensionless | Stable | | go_heap_alloc | The number of bytes of allocated heap objects | Gauge | name | Dimensionless | Stable | | go_heap_sys | The number of bytes of heap memory obtained from the OS | Gauge | name | Dimensionless | Stable | | go_heap_idle | The number of bytes in idle (unused) spans | Gauge | name | Dimensionless | Stable | | go_heap_in_use | The number of bytes in in-use spans | Gauge | name | Dimensionless | Stable | | go_heap_released | The number of bytes of physical memory returned to the OS | Gauge | name | Dimensionless | Stable | | go_heap_objects | The number of allocated heap objects | Gauge | name | Dimensionless | Stable | | go_stack_in_use | The number of bytes in stack spans | Gauge | name | Dimensionless | Stable | | go_stack_sys | The number of bytes of stack memory obtained from the OS | Gauge | name | Dimensionless | Stable | | go_mspan_in_use | The number of bytes of allocated mspan structures | Gauge | name | Dimensionless | Stable | | go_mspan_sys | The number of bytes of memory obtained from the OS for mspan structures | Gauge | name | Dimensionless | Stable | | go_mcache_in_use | The number of bytes of allocated mcache structures | Gauge | name | Dimensionless | Stable | | go_mcache_sys | The number of bytes of memory obtained from the OS for mcache structures | Gauge | name | Dimensionless | Stable | | go_bucket_hash_sys | The number of bytes of memory in profiling bucket hash tables. | Gauge | name | Dimensionless | Stable | | go_gc_sys | The number of bytes of memory in garbage collection metadata | Gauge | name | Dimensionless | Stable | | go_other_sys | The number of bytes of memory in miscellaneous off-heap runtime allocations | Gauge | name | Dimensionless | Stable | | go_next_gc | The target heap size of the next GC cycle | Gauge | name | Dimensionless | Stable | | go_last_gc | The time the last garbage collection finished, as nanoseconds since 1970 (the UNIX epoch) | Gauge | name | Nanoseconds | Stable | | go_total_gc_pause_ns | The cumulative nanoseconds in GC stop-the-world pauses since the program started | Gauge | name | Nanoseconds | Stable | | go_num_gc | The number of completed GC cycles. | Gauge | name | Dimensionless | Stable | | go_num_forced_gc | The number of GC cycles that were forced by the application calling the GC function. | Gauge | name | Dimensionless | Stable | | go_gc_cpu_fraction | The fraction of this program's available CPU time used by the GC since the program started | Gauge | name | Dimensionless | Stable | NOTE: name tag is empty.","title":"Go Runtime - memstats"},{"location":"serving/metrics/#developer-user-services","text":"Every Knative service has a proxy container that proxies the connections to the application container. A number of metrics are reported for the queue peroxy performance. Using the following metrics application developers, devops and others, could measure if requests are queued at the proxy side (need for backpressure) and what is the actual delay in serving requests at the application side.","title":"Developer - User Services"},{"location":"serving/metrics/#queue-proxy","text":"Requests endpoint Metric Name Description Type Tags Unit Status revision_request_count The number of requests that are routed to queue-proxy Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_request_latencies The response time in millisecond Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_app_request_count The number of requests that are routed to user-container Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_app_request_latencies The response time in millisecond Histogram configuration_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_queue_depth The current number of items in the serving and waiting queue, or not reported if unlimited concurrency Gauge configuration_name event-display container_name namespace_name pod_name response_code_class revision_name service_name Dimensionless Stable","title":"Queue proxy"},{"location":"serving/rolling-out-latest-revision/","text":"Gradually rolling out latest Revisions \u00b6 If your traffic configuration points to a Configuration target, rather than revision target, it means that when a new Revision is created and ready 100% of that target's traffic will be immediately shifted to the new revision, which might not be ready to accept that scale with a single pod and with cold starts taking some time it is possible to end up in a situation where a lot of requests are backed up either at QP or Activator and after a while they might expire or QP might outright reject the requests. To mitigate this problem Knative as of 0.20 release Knative provides users with a possibility to gradually shift the traffic to the latest revision. This is governed by a single parameter which denotes rollout-duration . The affected Configuration targets will be rolled out to 1% of traffic first and then in equal incremental steps for the rest of the assigned traffic. Note, that the rollout is purely time based and does not interact with the Autoscaling subsystem. This feature is available to untagged and tagged traffic targets configured for both Kservices and Kservice-less Routes. Configuring gradual Rollout \u00b6 This value currently can be configured on the cluster level (starting v0.20) via a setting in the config-network ConfigMap or per Kservice or Route using an annotation (staring v.0.21). Per-revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default annotations : serving.knative.dev/rolloutDuration : \"380s\" ... Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : rolloutDuration : \"380s\" # Value in seconds. Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : rolloutDuration : \"380s\" Route Status updates \u00b6 During the rollout the system will update the Route and Kservice status. Both traffic and conditions status fields will be affected. For example, a possible rollout of the following traffic configuration traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. would be (if inspecting the route status): traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. and then, presuming steps of 18%: traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. and so on until final state is achieved: traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. During the rollout the Route and (Kservice, if present) status conditions will be the following: ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready ... Multiple Rollouts \u00b6 If a new revision is created while the rollout is in progress then the system would start shifting the traffic immediately to the newest revision and it will drain the incomplete rollouts from newest to the oldest.","title":"Gradually rolling out latest Revisions"},{"location":"serving/rolling-out-latest-revision/#gradually-rolling-out-latest-revisions","text":"If your traffic configuration points to a Configuration target, rather than revision target, it means that when a new Revision is created and ready 100% of that target's traffic will be immediately shifted to the new revision, which might not be ready to accept that scale with a single pod and with cold starts taking some time it is possible to end up in a situation where a lot of requests are backed up either at QP or Activator and after a while they might expire or QP might outright reject the requests. To mitigate this problem Knative as of 0.20 release Knative provides users with a possibility to gradually shift the traffic to the latest revision. This is governed by a single parameter which denotes rollout-duration . The affected Configuration targets will be rolled out to 1% of traffic first and then in equal incremental steps for the rest of the assigned traffic. Note, that the rollout is purely time based and does not interact with the Autoscaling subsystem. This feature is available to untagged and tagged traffic targets configured for both Kservices and Kservice-less Routes.","title":"Gradually rolling out latest Revisions"},{"location":"serving/rolling-out-latest-revision/#configuring-gradual-rollout","text":"This value currently can be configured on the cluster level (starting v0.20) via a setting in the config-network ConfigMap or per Kservice or Route using an annotation (staring v.0.21). Per-revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default annotations : serving.knative.dev/rolloutDuration : \"380s\" ... Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : rolloutDuration : \"380s\" # Value in seconds. Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : rolloutDuration : \"380s\"","title":"Configuring gradual Rollout"},{"location":"serving/rolling-out-latest-revision/#route-status-updates","text":"During the rollout the system will update the Route and Kservice status. Both traffic and conditions status fields will be affected. For example, a possible rollout of the following traffic configuration traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. would be (if inspecting the route status): traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. and then, presuming steps of 18%: traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. and so on until final state is achieved: traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. During the rollout the Route and (Kservice, if present) status conditions will be the following: ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready ...","title":"Route Status updates"},{"location":"serving/rolling-out-latest-revision/#multiple-rollouts","text":"If a new revision is created while the rollout is in progress then the system would start shifting the traffic immediately to the newest revision and it will drain the incomplete rollouts from newest to the oldest.","title":"Multiple Rollouts"},{"location":"serving/setting-up-custom-ingress-gateway/","text":"Setting up custom ingress gateway \u00b6 Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the knative-ingress-gateway Gateway under the knative-serving namespace. By default, we use Istio gateway service istio-ingressgateway under istio-system namespace as its underlying service. You can replace the service with that of your own as follows. Step 1: Create Gateway Service and Deployment Instance \u00b6 You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default istio-ingressgateway to custom-ingressgateway as follows. apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : values : global : proxy : autoInject : disabled useMCP : false # The third-party-jwt is not enabled on all k8s. # See: https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens jwtPolicy : first-party-jwt addonComponents : pilot : enabled : true prometheus : enabled : false components : ingressGateways : - name : custom-ingressgateway enabled : true namespace : custom-ns label : istio : custom-gateway Step 2: Update Knative Gateway \u00b6 Update gateway instance knative-ingress-gateway under knative-serving namespace: kubectl edit gateway knative-ingress-gateway -n knative-serving Replace the label selector with the label of your service: istio: ingressgateway For the service above, it should be updated to: istio: custom-gateway If there is a change in service ports (compared with that of istio-ingressgateway ), update the port info in the gateway accordingly. Step 3: Update Gateway Configmap \u00b6 Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving Replace the istio-ingressgateway.istio-system.svc.cluster.local field with the fully qualified url of your service. gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" For the service above, it should be updated to: gateway.knative-serving.knative-ingress-gateway: custom-ingressgateway.custom-ns.svc.cluster.local","title":"Configuring the ingress gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#setting-up-custom-ingress-gateway","text":"Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the knative-ingress-gateway Gateway under the knative-serving namespace. By default, we use Istio gateway service istio-ingressgateway under istio-system namespace as its underlying service. You can replace the service with that of your own as follows.","title":"Setting up custom ingress gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#step-1-create-gateway-service-and-deployment-instance","text":"You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default istio-ingressgateway to custom-ingressgateway as follows. apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : values : global : proxy : autoInject : disabled useMCP : false # The third-party-jwt is not enabled on all k8s. # See: https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens jwtPolicy : first-party-jwt addonComponents : pilot : enabled : true prometheus : enabled : false components : ingressGateways : - name : custom-ingressgateway enabled : true namespace : custom-ns label : istio : custom-gateway","title":"Step 1: Create Gateway Service and Deployment Instance"},{"location":"serving/setting-up-custom-ingress-gateway/#step-2-update-knative-gateway","text":"Update gateway instance knative-ingress-gateway under knative-serving namespace: kubectl edit gateway knative-ingress-gateway -n knative-serving Replace the label selector with the label of your service: istio: ingressgateway For the service above, it should be updated to: istio: custom-gateway If there is a change in service ports (compared with that of istio-ingressgateway ), update the port info in the gateway accordingly.","title":"Step 2: Update Knative Gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#step-3-update-gateway-configmap","text":"Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving Replace the istio-ingressgateway.istio-system.svc.cluster.local field with the fully qualified url of your service. gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" For the service above, it should be updated to: gateway.knative-serving.knative-ingress-gateway: custom-ingressgateway.custom-ns.svc.cluster.local","title":"Step 3: Update Gateway Configmap"},{"location":"serving/tag-resolution/","text":"Enabling tag to digest resolution \u00b6 Knative serving resolves image tags to a digest when you create a revision. This gives knative revisions some very nice properties, e.g. your deployments will be consistent, you don't have to worry about \"immutable tags\", etc. For more info, see Why we resolve tags in Knative . Unfortunately, this means that the knative serving controller needs to be configured to access your container registry. Custom Certificates \u00b6 If you're using a registry that has a self-signed certificate, you'll need to convince the serving controller to trust that certificate. We respect the SSL_CERT_FILE and SSL_CERT_DIR environment variables, so you can trust them by mounting the certificates into the controller's deployment and setting the environment variable appropriately, assuming you have a custom-certs secret containing your CA certs: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller volumeMounts : - name : custom-certs mountPath : /path/to/custom/certs env : - name : SSL_CERT_DIR value : /path/to/custom/certs volumes : - name : custom-certs secret : secretName : custom-certs Corporate Proxy \u00b6 If you're behind a corporate proxy, you'll need to proxy the tag resolution requests between the controller and your registry. We respect the HTTP_PROXY and HTTPS_PROXY environment variables, so you can configure the controller's deployment via: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller env : - name : HTTP_PROXY value : http://proxy.example.com - name : HTTPS_PROXY value : https://proxy.example.com Skipping tag resolution \u00b6 If this all seems like too much trouble, you can configure serving to skip tag resolution via the registriesSkippingTagResolving configmap field: kubectl -n knative-serving edit configmap config-deployment E.g., to disable tag resolution for registry.example.com (note: This is not a complete configmap, it is a snippet showing registriesSkippingTagResolving): apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : # List of repositories for which tag to digest resolving should be skipped registriesSkippingTagResolving : registry.example.com","title":"Tag resolution"},{"location":"serving/tag-resolution/#enabling-tag-to-digest-resolution","text":"Knative serving resolves image tags to a digest when you create a revision. This gives knative revisions some very nice properties, e.g. your deployments will be consistent, you don't have to worry about \"immutable tags\", etc. For more info, see Why we resolve tags in Knative . Unfortunately, this means that the knative serving controller needs to be configured to access your container registry.","title":"Enabling tag to digest resolution"},{"location":"serving/tag-resolution/#custom-certificates","text":"If you're using a registry that has a self-signed certificate, you'll need to convince the serving controller to trust that certificate. We respect the SSL_CERT_FILE and SSL_CERT_DIR environment variables, so you can trust them by mounting the certificates into the controller's deployment and setting the environment variable appropriately, assuming you have a custom-certs secret containing your CA certs: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller volumeMounts : - name : custom-certs mountPath : /path/to/custom/certs env : - name : SSL_CERT_DIR value : /path/to/custom/certs volumes : - name : custom-certs secret : secretName : custom-certs","title":"Custom Certificates"},{"location":"serving/tag-resolution/#corporate-proxy","text":"If you're behind a corporate proxy, you'll need to proxy the tag resolution requests between the controller and your registry. We respect the HTTP_PROXY and HTTPS_PROXY environment variables, so you can configure the controller's deployment via: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller env : - name : HTTP_PROXY value : http://proxy.example.com - name : HTTPS_PROXY value : https://proxy.example.com","title":"Corporate Proxy"},{"location":"serving/tag-resolution/#skipping-tag-resolution","text":"If this all seems like too much trouble, you can configure serving to skip tag resolution via the registriesSkippingTagResolving configmap field: kubectl -n knative-serving edit configmap config-deployment E.g., to disable tag resolution for registry.example.com (note: This is not a complete configmap, it is a snippet showing registriesSkippingTagResolving): apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : # List of repositories for which tag to digest resolving should be skipped registriesSkippingTagResolving : registry.example.com","title":"Skipping tag resolution"},{"location":"serving/using-a-custom-domain/","text":"Setting up a custom domain \u00b6 By default, Knative Serving routes use example.com as the default domain. The fully qualified domain name for a route by default is {route}.{namespace}.{default-domain} . To change the {default-domain} value there are a few steps involved: Edit using kubectl \u00b6 Edit the domain configuration config-map to replace example.com with your own domain, for example mydomain.com : kubectl edit cm config-domain --namespace knative-serving This command opens your default text editor and allows you to edit the config map . apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... example.com: | kind : ConfigMap Edit the file to replace example.com with the domain you'd like to use, remove the _example key and save your changes. In this example, we configure mydomain.com for all routes: apiVersion : v1 data : mydomain.com : \"\" kind : ConfigMap [ ... ] Apply from a file \u00b6 You can also apply an updated domain configuration: Create a new file, config-domain.yaml and paste the following text, replacing the example.org and example.com values with the new domain you want to use: apiVersion : v1 kind : ConfigMap metadata : name : config-domain namespace : knative-serving data : # These are example settings of domain. # example.org will be used for routes having app=prod. example.org : | selector: app: prod # Default value for domain, for routes that does not have app=prod labels. # Although it will match all routes, it is the least-specific rule so it # will only be used if no other domain matches. example.com : \"\" Apply updated domain configuration to your cluster: kubectl apply --filename config-domain.yaml Deploy an application \u00b6 If you have an existing deployment, Knative will reconcile the change made to the configuration map and automatically update the host name for all of the deployed services and routes. Deploy an app (for example, helloworld-go ), to your cluster as normal. You can retrieve the URL in Knative Route \"helloworld-go\" with the following command: kubectl get route helloworld-go --output jsonpath = \"{.status.url}\" You should see the full customized domain: helloworld-go.default.mydomain.com . And you can check the IP address of your Knative gateway by running: export INGRESSGATEWAY = istio-ingressgateway if kubectl get configmap config-istio -n knative-serving & > /dev/null ; then export INGRESSGATEWAY = istio-ingressgateway fi kubectl get svc $INGRESSGATEWAY --namespace istio-system --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\" Local DNS setup \u00b6 You can map the domain to the IP address of your Knative gateway in your local machine with: INGRESSGATEWAY = istio-ingressgateway export GATEWAY_IP = ` kubectl get svc $INGRESSGATEWAY --namespace istio-system --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\" ` # helloworld-go is the generated Knative Route of \"helloworld-go\" sample. # You need to replace it with your own Route in your project. export DOMAIN_NAME = ` kubectl get route helloworld-go --output jsonpath = \"{.status.url}\" | cut -d '/' -f 3 ` # Add the record of Gateway IP and domain name into file \"/etc/hosts\" echo -e \" $GATEWAY_IP \\t $DOMAIN_NAME \" | sudo tee -a /etc/hosts You can now access your domain from the browser in your machine and do some quick checks. Publish your Domain \u00b6 Follow these steps to make your domain publicly accessible: Set static IP for Knative Gateway \u00b6 You might want to set a static IP for your Knative gateway , so that the gateway IP does not change each time your cluster is restarted. Update your DNS records \u00b6 To publish your domain, you need to update your DNS provider to point to the IP address for your service ingress. Create a wildcard record for the namespace and custom domain to the ingress IP Address, which would enable hostnames for multiple services in the same namespace to work without creating additional DNS entries. *.default.mydomain.com 59 IN A 35.237.28.44 Create an A record to point from the fully qualified domain name to the IP address of your Knative gateway. This step needs to be done for each Knative Service or Route created. helloworld-go.default.mydomain.com 59 IN A 35.237.28.44 If you are using Google Cloud DNS, you can find step-by-step instructions in the Cloud DNS quickstart . Once the domain update has propagated, you can access your app using the fully qualified domain name of the deployed route, for example http://helloworld-go.default.mydomain.com","title":"Setting up a custom domain"},{"location":"serving/using-a-custom-domain/#setting-up-a-custom-domain","text":"By default, Knative Serving routes use example.com as the default domain. The fully qualified domain name for a route by default is {route}.{namespace}.{default-domain} . To change the {default-domain} value there are a few steps involved:","title":"Setting up a custom domain"},{"location":"serving/using-a-custom-domain/#edit-using-kubectl","text":"Edit the domain configuration config-map to replace example.com with your own domain, for example mydomain.com : kubectl edit cm config-domain --namespace knative-serving This command opens your default text editor and allows you to edit the config map . apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... example.com: | kind : ConfigMap Edit the file to replace example.com with the domain you'd like to use, remove the _example key and save your changes. In this example, we configure mydomain.com for all routes: apiVersion : v1 data : mydomain.com : \"\" kind : ConfigMap [ ... ]","title":"Edit using kubectl"},{"location":"serving/using-a-custom-domain/#apply-from-a-file","text":"You can also apply an updated domain configuration: Create a new file, config-domain.yaml and paste the following text, replacing the example.org and example.com values with the new domain you want to use: apiVersion : v1 kind : ConfigMap metadata : name : config-domain namespace : knative-serving data : # These are example settings of domain. # example.org will be used for routes having app=prod. example.org : | selector: app: prod # Default value for domain, for routes that does not have app=prod labels. # Although it will match all routes, it is the least-specific rule so it # will only be used if no other domain matches. example.com : \"\" Apply updated domain configuration to your cluster: kubectl apply --filename config-domain.yaml","title":"Apply from a file"},{"location":"serving/using-a-custom-domain/#deploy-an-application","text":"If you have an existing deployment, Knative will reconcile the change made to the configuration map and automatically update the host name for all of the deployed services and routes. Deploy an app (for example, helloworld-go ), to your cluster as normal. You can retrieve the URL in Knative Route \"helloworld-go\" with the following command: kubectl get route helloworld-go --output jsonpath = \"{.status.url}\" You should see the full customized domain: helloworld-go.default.mydomain.com . And you can check the IP address of your Knative gateway by running: export INGRESSGATEWAY = istio-ingressgateway if kubectl get configmap config-istio -n knative-serving & > /dev/null ; then export INGRESSGATEWAY = istio-ingressgateway fi kubectl get svc $INGRESSGATEWAY --namespace istio-system --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\"","title":"Deploy an application"},{"location":"serving/using-a-custom-domain/#local-dns-setup","text":"You can map the domain to the IP address of your Knative gateway in your local machine with: INGRESSGATEWAY = istio-ingressgateway export GATEWAY_IP = ` kubectl get svc $INGRESSGATEWAY --namespace istio-system --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\" ` # helloworld-go is the generated Knative Route of \"helloworld-go\" sample. # You need to replace it with your own Route in your project. export DOMAIN_NAME = ` kubectl get route helloworld-go --output jsonpath = \"{.status.url}\" | cut -d '/' -f 3 ` # Add the record of Gateway IP and domain name into file \"/etc/hosts\" echo -e \" $GATEWAY_IP \\t $DOMAIN_NAME \" | sudo tee -a /etc/hosts You can now access your domain from the browser in your machine and do some quick checks.","title":"Local DNS setup"},{"location":"serving/using-a-custom-domain/#publish-your-domain","text":"Follow these steps to make your domain publicly accessible:","title":"Publish your Domain"},{"location":"serving/using-a-custom-domain/#set-static-ip-for-knative-gateway","text":"You might want to set a static IP for your Knative gateway , so that the gateway IP does not change each time your cluster is restarted.","title":"Set static IP for Knative Gateway"},{"location":"serving/using-a-custom-domain/#update-your-dns-records","text":"To publish your domain, you need to update your DNS provider to point to the IP address for your service ingress. Create a wildcard record for the namespace and custom domain to the ingress IP Address, which would enable hostnames for multiple services in the same namespace to work without creating additional DNS entries. *.default.mydomain.com 59 IN A 35.237.28.44 Create an A record to point from the fully qualified domain name to the IP address of your Knative gateway. This step needs to be done for each Knative Service or Route created. helloworld-go.default.mydomain.com 59 IN A 35.237.28.44 If you are using Google Cloud DNS, you can find step-by-step instructions in the Cloud DNS quickstart . Once the domain update has propagated, you can access your app using the fully qualified domain name of the deployed route, for example http://helloworld-go.default.mydomain.com","title":"Update your DNS records"},{"location":"serving/using-a-tls-cert/","text":"Configuring HTTPS with TLS certificates \u00b6 Learn how to configure secure HTTPS connections in Knative using TLS certificates ( TLS replaces SSL ). Configure secure HTTPS connections to enable your Knative services and routes to terminate external TLS connections . You can configure Knative to handle certificates that you manually specify, or you can enable Knative to automatically obtain and renew certificates. You can use either Certbot or cert-manager to obtain certificates. Both tools support TLS certificates but if you want to enable Knative for automatic TLS certificate provisioning, you must install and configure the cert-manager tool: Manually obtain and renew certificates : Both the Certbot and cert-manager tools can be used to manually obtain TLS certificates. In general, after you obtain a certificate, you must create a Kubernetes secret to use that certificate in your cluster. See the complete set of steps below for details about manually obtaining and configuring certificates. Enable Knative to automatically obtain and renew TLS certificates : You can also use cert-manager to configure Knative to automatically obtain new TLS certificates and renew existing ones. If you want to enable Knative to automatically provision TLS certificates, instead see the Enabling automatic TLS certificate provisioning topic. By default, the Let's Encrypt Certificate Authority (CA) is used to demonstrate how to enable HTTPS connections, but you can configure Knative to use any certificate from a CA that supports the ACME protocol. However, you must use and configure your certificate issuer to use the DNS-01 challenge type . Important: Certificates issued by Let's Encrypt are valid for only 90 days . Therefore, if you choose to manually obtain and configure your certificates, you must ensure that you renew each certificate before it expires. Before you begin \u00b6 You must meet the following requirements to enable secure HTTPS connections: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Important: Istio only supports a single certificate per Kubernetes cluster. To serve multiple domains using your Knative cluster, you must ensure that your new or existing certificate is signed for each of the domains that you want to serve. Obtaining a TLS certificate \u00b6 If you already have a signed certificate for your domain, see Manually adding a TLS certificate for details about configuring your Knative cluster. If you need a new TLS certificate, you can choose to use one of the following tools to obtain a certificate from Let's Encrypt: Setup Certbot to manually obtain Let's Encrypt certificates Setup cert-manager to either manually obtain a certificate, or to automatically provision certificates This page covers details for both of the above options. For details about using other CA's, see the tool's reference documentation: Certbot supported providers cert-manager supported providers Using Certbot to manually obtain Let\u2019s Encrypt certificates \u00b6 Use the following steps to install Certbot and the use the tool to manually obtain a TLS certificate from Let's Encrypt. Install Certbot by following the certbot-auto wrapper script instructions. Run the following command to use Certbot to request a certificate using DNS challenge during authorization: ./certbot-auto certonly --manual --preferred-challenges dns -d '*.default.yourdomain.com' where -d specifies your domain. If you want to validate multiple domain's, you can include multiple flags: -d MY.EXAMPLEDOMAIN.1 -d MY.EXAMPLEDOMAIN.2 . For more information, see the Cerbot command-line reference. The Certbot tool walks you through the steps of validating that you own each domain that you specify by creating TXT records in those domains. Result: CertBot creates two files: Certificate: fullchain.pem Private key: privkey.pem What's next: Add the certificate and private key to your Knative cluster by creating a Kubernetes secret . Using cert-manager to obtain Let's Encrypt certificates \u00b6 You can install and use cert-manager to either manually obtain a certificate or to configure your Knative cluster for automatic certificate provisioning: Manual certificates : Install cert-manager and then use the tool to manually obtain a certificate. To use cert-manager to manually obtain certificates: Install and configure cert-manager . Continue to the steps below about manually adding a TLS certificate by creating and using a Kubernetes secret. Automatic certificates : Configure Knative to use cert-manager for automatically obtaining and renewing TLS certificate. The steps for installing and configuring cert-manager for this method are covered in full in the Enabling automatic TLS cert provisioning topic. Manually adding a TLS certificate \u00b6 If you have an existing certificate or have used one of the Certbot or cert-manager tool to manually obtain a new certificate, you can use the following steps to add that certificate to your Knative cluster. For instructions about enabling Knative for automatic certificate provisioning, see Enabling automatic TLS cert provisioning . Otherwise, continue below for instructions about manually adding a certificate. Contour To manually add a TLS certificate to your Knative cluster, you must create a Kubernetes secret and then configure the Knative Contour plugin Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace contour-external secret tls default-cert \\ --key key.pem \\ --cert cert.pem IMPORTANT Take note of the namespace and secret name. You will need these in future steps. Contour requires you to create a delegation to use this certificate and private key in different namespaces. This can be done by creating the following resource: apiVersion : projectcontour.io/v1 kind : TLSCertificateDelegation metadata : name : default-delegation namespace : contour-external spec : delegations : - secretName : default-cert targetNamespaces : - \"*\" Update the Knative Contour plugin to start using the certificate as a fallback when auto-TLS is disabled. This can be done with the following patch: kubectl patch cm config-contour -n knative-serving \\ -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}' Istio To manually add a TLS certificate to your Knative cluster, you create a Kubernetes secret and then configure the knative-ingress-gateway : Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace istio-system secret tls tls-cert \\ --key key.pem \\ --cert cert.pem Configure Knative to use the new secret that you created for HTTPS connections: Run the following command to open the Knative shared gateway in edit mode: kubectl edit gateway knative-ingress-gateway --namespace knative-serving Update the gateway to include the following tls: section and configuration: tls : mode : SIMPLE credentialName : tls-cert Example: # Please edit the object below. Lines beginning with a '#' will be ignored. # and an empty file will abort the edit. If an error occurs while saving this # file will be reopened with the relevant failures. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : # ... skipped ... spec : selector : istio : ingressgateway servers : - hosts : - \"*\" port : name : http number : 80 protocol : HTTP - hosts : - TLS_HOSTS port : name : https number : 443 protocol : HTTPS tls : mode : SIMPLE credentialName : tls-cert In the example above, TLS_HOSTS represents the hosts of your TLS certificate. It can be a single host, multiple hosts, or a wildcard host. For detailed instructions, please refer Istio documentation What's next: \u00b6 After your changes are running on your Knative cluster, you can begin using the HTTPS protocol for secure access your deployed Knative services.","title":"Configuring HTTPS connections"},{"location":"serving/using-a-tls-cert/#configuring-https-with-tls-certificates","text":"Learn how to configure secure HTTPS connections in Knative using TLS certificates ( TLS replaces SSL ). Configure secure HTTPS connections to enable your Knative services and routes to terminate external TLS connections . You can configure Knative to handle certificates that you manually specify, or you can enable Knative to automatically obtain and renew certificates. You can use either Certbot or cert-manager to obtain certificates. Both tools support TLS certificates but if you want to enable Knative for automatic TLS certificate provisioning, you must install and configure the cert-manager tool: Manually obtain and renew certificates : Both the Certbot and cert-manager tools can be used to manually obtain TLS certificates. In general, after you obtain a certificate, you must create a Kubernetes secret to use that certificate in your cluster. See the complete set of steps below for details about manually obtaining and configuring certificates. Enable Knative to automatically obtain and renew TLS certificates : You can also use cert-manager to configure Knative to automatically obtain new TLS certificates and renew existing ones. If you want to enable Knative to automatically provision TLS certificates, instead see the Enabling automatic TLS certificate provisioning topic. By default, the Let's Encrypt Certificate Authority (CA) is used to demonstrate how to enable HTTPS connections, but you can configure Knative to use any certificate from a CA that supports the ACME protocol. However, you must use and configure your certificate issuer to use the DNS-01 challenge type . Important: Certificates issued by Let's Encrypt are valid for only 90 days . Therefore, if you choose to manually obtain and configure your certificates, you must ensure that you renew each certificate before it expires.","title":"Configuring HTTPS with TLS certificates"},{"location":"serving/using-a-tls-cert/#before-you-begin","text":"You must meet the following requirements to enable secure HTTPS connections: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Important: Istio only supports a single certificate per Kubernetes cluster. To serve multiple domains using your Knative cluster, you must ensure that your new or existing certificate is signed for each of the domains that you want to serve.","title":"Before you begin"},{"location":"serving/using-a-tls-cert/#obtaining-a-tls-certificate","text":"If you already have a signed certificate for your domain, see Manually adding a TLS certificate for details about configuring your Knative cluster. If you need a new TLS certificate, you can choose to use one of the following tools to obtain a certificate from Let's Encrypt: Setup Certbot to manually obtain Let's Encrypt certificates Setup cert-manager to either manually obtain a certificate, or to automatically provision certificates This page covers details for both of the above options. For details about using other CA's, see the tool's reference documentation: Certbot supported providers cert-manager supported providers","title":"Obtaining a TLS certificate"},{"location":"serving/using-a-tls-cert/#using-certbot-to-manually-obtain-lets-encrypt-certificates","text":"Use the following steps to install Certbot and the use the tool to manually obtain a TLS certificate from Let's Encrypt. Install Certbot by following the certbot-auto wrapper script instructions. Run the following command to use Certbot to request a certificate using DNS challenge during authorization: ./certbot-auto certonly --manual --preferred-challenges dns -d '*.default.yourdomain.com' where -d specifies your domain. If you want to validate multiple domain's, you can include multiple flags: -d MY.EXAMPLEDOMAIN.1 -d MY.EXAMPLEDOMAIN.2 . For more information, see the Cerbot command-line reference. The Certbot tool walks you through the steps of validating that you own each domain that you specify by creating TXT records in those domains. Result: CertBot creates two files: Certificate: fullchain.pem Private key: privkey.pem What's next: Add the certificate and private key to your Knative cluster by creating a Kubernetes secret .","title":"Using Certbot to manually obtain Let\u2019s Encrypt certificates"},{"location":"serving/using-a-tls-cert/#using-cert-manager-to-obtain-lets-encrypt-certificates","text":"You can install and use cert-manager to either manually obtain a certificate or to configure your Knative cluster for automatic certificate provisioning: Manual certificates : Install cert-manager and then use the tool to manually obtain a certificate. To use cert-manager to manually obtain certificates: Install and configure cert-manager . Continue to the steps below about manually adding a TLS certificate by creating and using a Kubernetes secret. Automatic certificates : Configure Knative to use cert-manager for automatically obtaining and renewing TLS certificate. The steps for installing and configuring cert-manager for this method are covered in full in the Enabling automatic TLS cert provisioning topic.","title":"Using cert-manager to obtain Let's Encrypt certificates"},{"location":"serving/using-a-tls-cert/#manually-adding-a-tls-certificate","text":"If you have an existing certificate or have used one of the Certbot or cert-manager tool to manually obtain a new certificate, you can use the following steps to add that certificate to your Knative cluster. For instructions about enabling Knative for automatic certificate provisioning, see Enabling automatic TLS cert provisioning . Otherwise, continue below for instructions about manually adding a certificate. Contour To manually add a TLS certificate to your Knative cluster, you must create a Kubernetes secret and then configure the Knative Contour plugin Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace contour-external secret tls default-cert \\ --key key.pem \\ --cert cert.pem IMPORTANT Take note of the namespace and secret name. You will need these in future steps. Contour requires you to create a delegation to use this certificate and private key in different namespaces. This can be done by creating the following resource: apiVersion : projectcontour.io/v1 kind : TLSCertificateDelegation metadata : name : default-delegation namespace : contour-external spec : delegations : - secretName : default-cert targetNamespaces : - \"*\" Update the Knative Contour plugin to start using the certificate as a fallback when auto-TLS is disabled. This can be done with the following patch: kubectl patch cm config-contour -n knative-serving \\ -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}' Istio To manually add a TLS certificate to your Knative cluster, you create a Kubernetes secret and then configure the knative-ingress-gateway : Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace istio-system secret tls tls-cert \\ --key key.pem \\ --cert cert.pem Configure Knative to use the new secret that you created for HTTPS connections: Run the following command to open the Knative shared gateway in edit mode: kubectl edit gateway knative-ingress-gateway --namespace knative-serving Update the gateway to include the following tls: section and configuration: tls : mode : SIMPLE credentialName : tls-cert Example: # Please edit the object below. Lines beginning with a '#' will be ignored. # and an empty file will abort the edit. If an error occurs while saving this # file will be reopened with the relevant failures. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : # ... skipped ... spec : selector : istio : ingressgateway servers : - hosts : - \"*\" port : name : http number : 80 protocol : HTTP - hosts : - TLS_HOSTS port : name : https number : 443 protocol : HTTPS tls : mode : SIMPLE credentialName : tls-cert In the example above, TLS_HOSTS represents the hosts of your TLS certificate. It can be a single host, multiple hosts, or a wildcard host. For detailed instructions, please refer Istio documentation","title":"Manually adding a TLS certificate"},{"location":"serving/using-a-tls-cert/#whats-next","text":"After your changes are running on your Knative cluster, you can begin using the HTTPS protocol for secure access your deployed Knative services.","title":"What's next:"},{"location":"serving/using-auto-tls/","text":"Enabling automatic TLS certificate provisioning \u00b6 If you install and configure cert-manager, you can configure Knative to automatically obtain new TLS certificates and renew existing ones for Knative Services. To learn more about using secure connections in Knative, see Configuring HTTPS with TLS certificates . Automatic TLS provision mode \u00b6 Knative supports the following Auto TLS modes: Using DNS-01 challenge In this mode, your cluster needs to be able to talk to your DNS server to verify the ownership of your domain. - Provision Certificate per namespace is supported when using DNS-01 challenge mode. - This is the recommended mode for faster certificate provision. - In this mode, a single Certificate will be provisioned per namespace and is reused across the Knative Services within the same namespace. Provision Certificate per Knative Service is supported when using DNS-01 challenge mode. This is the recommended mode for better certificate islation between Knative Services. In this mode, a Certificate will be provisioned for each Knative Service. The TLS effective time is longer as it needs Certificate provision for each Knative Service creation. Using HTTP-01 challenge In this type, your cluster does not need to be able to talk to your DNS server. You must map your domain to the IP of the cluser ingress. When using HTTP-01 challenge, a certificate will be provisioned per Knative Service. HTTP-01 does not support provisioning a certificate per namespace. Before you begin \u00b6 You must meet the following prerequisites to enable Auto TLS: The following must be installed on your Knative cluster: Knative Serving . A Networking layer such as Kourier, Istio with SDS v1.3 or higher, Contour v1.1 or higher, or Gloo v0.18.16 or higher. See Install a networking layer or Istio with SDS, version 1.3 or higher . Note: Currently, Ambassador is unsupported for use with Auto TLS. cert-manager version 1.0.0 and higher . Your Knative cluster must be configured to use a custom domain . Your DNS provider must be setup and configured to your domain. If you want to use HTTP-01 challenge, you need to configure your custom domain to map to the IP of ingress. You can achieve this by adding a DNS A record to map the domain to the IP according to the instructions of your DNS provider. Enabling Auto TLS \u00b6 To enable support for Auto TLS in Knative: Create cert-manager ClusterIssuer \u00b6 Create and add the ClusterIssuer configuration file to your Knative cluster to define who issues the TLS certificates, how requests are validated, and which DNS provider validates those requests. ClusterIssuer for DNS-01 challenge \u00b6 Use the cert-manager reference to determine how to configure your ClusterIssuer file: - See the generic ClusterIssuer example - Also see the DNS01 example Example : Cloud DNS ClusterIssuer configuration file: The following letsencrypt-issuer named ClusterIssuer file is configured for the Let's Encrypt CA and Google Cloud DNS. Under spec , the Let's Encrypt account info, required DNS-01 challenge type, and Cloud DNS provider info defined. For the complete Google Cloud DNS example, see Configuring HTTPS with cert-manager and Google Cloud DNS . apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-dns-issuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email: myemail@gmail.com privateKeySecretRef: # Set privateKeySecretRef to any unused secret name. name: letsencrypt-dns-issuer solvers: - dns01: clouddns: # Set this to your GCP project-id project: $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef: name: cloud-dns-key key: key.json ClusterIssuer for HTTP-01 challenge \u00b6 Run the following command to apply the ClusterIssuer for HTT01 challenge: kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-http01-issuer spec: acme: privateKeySecretRef: name: letsencrypt server: https://acme-v02.api.letsencrypt.org/directory solvers: - http01: ingress: class: istio EOF Ensure that the ClusterIssuer is created successfully: kubectl get clusterissuer <cluster-issuer-name> --output yaml Result: The Status.Conditions should include Ready=True . DNS-01 challenge only: Configure your DNS provider \u00b6 If you choose to use DNS-01 challenge, configure which DNS provider is used to validate the DNS-01 challenge requests. Instructions about configuring cert-manager, for all the supported DNS providers, are provided in DNS01 challenge providers and configuration instructions . Example: See how the Google Cloud DNS is defined as the provider: Configuring HTTPS with cert-manager and Google Cloud DNS Install networking-certmanager deployment \u00b6 Determine if networking-certmanager is already installed by running the following command: kubectl get deployment networking-certmanager -n knative-serving If networking-certmanager is not found, run the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Install networking-ns-cert component \u00b6 If you choose to use the mode of provisioning certificate per namespace, you need to install networking-ns-cert components. IMPORTANT: Provisioning a certificate per namespace only works with DNS-01 challenge. This component cannot be used with HTTP-01 challenge. Determine if networking-ns-cert deployment is already installed by running the following command: kubectl get deployment networking-ns-cert -n knative-serving If networking-ns-cert deployment is not found, run the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/serving/latest/serving-nscert.yaml Configure config-certmanager ConfigMap \u00b6 Update your config-certmanager ConfigMap in the knative-serving namespace to reference your new ClusterIssuer . Run the following command to edit your config-certmanager ConfigMap: kubectl edit configmap config-certmanager --namespace knative-serving Add the issuerRef within the data section: ... data: ... issuerRef: | kind: ClusterIssuer name: letsencrypt-issuer Example: apiVersion: v1 kind: ConfigMap metadata: name: config-certmanager namespace: knative-serving labels: networking.knative.dev/certificate-provider: cert-manager data: issuerRef: | kind: ClusterIssuer name: letsencrypt-http01-issuer issueRef defines which ClusterIssuer will be used by Knative to issue certificates. Ensure that the file was updated successfully: kubectl get configmap config-certmanager --namespace knative-serving --output yaml Turn on Auto TLS \u00b6 Update the config-network ConfigMap in the knative-serving namespace to enable autoTLS and specify how HTTP requests are handled: Run the following command to edit your config-network ConfigMap: kubectl edit configmap config-network --namespace knative-serving Add the autoTLS: Enabled attribute under the data section: ... data: ... autoTLS: Enabled ... Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... autoTLS: Enabled ... Configure how HTTP and HTTPS requests are handled in the httpProtocol attribute. By default, Knative ingress is configured to serve HTTP traffic ( httpProtocol: Enabled ). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic, you can specify whether or not any HTTP traffic is allowed. Supported httpProtocol values: Enabled : Serve HTTP traffic. Disabled : Rejects all HTTP traffic. Redirected : Responds to HTTP request with a 302 redirect to ask the clients to use HTTPS. ... data: ... autoTLS: Enabled ... Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... autoTLS: Enabled ... httpProtocol: Redirected ... Note: When using HTTP-01 challenge, httpProtocol field has to be set to Enabled to make sure HTTP-01 challenge requests can be accepted by the cluster. Ensure that the file was updated successfully: kubectl get configmap config-network --namespace knative-serving --output yaml Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS certificate is active on your cluster, your Knative services will be able to handle HTTPS traffic. Verify Auto TLS \u00b6 Run the following comand to create a Knative Service: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/autoscaling/autoscale-go/service.yaml When the certificate is provisioned (which could take up to several minutes depending on the challenge type), you should see something like: NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go https://autoscale-go.default.{custom-domain} autoscale-go-6jf85 autoscale-go-6jf85 True Note that the URL will be https in this case. Disable Auto TLS per service or route \u00b6 If you have Auto TLS enabled in your cluster, you can choose to disable Auto TLS for individual services or routes by adding the annotation networking.knative.dev/disableAutoTLS: true . Using the previous autoscale-go example: Edit the service using kubectl edit service.serving.knative.dev/autoscale-go -n default and add the annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : ... networking.knative.dev/disableAutoTLS : \"true\" ... The service URL should now be http , indicating that AutoTLS is disabled: NAME URL LATEST AGE CONDITIONS READY REASON autoscale-go http://autoscale-go.default.1.arenault.dev autoscale-go-dd42t 8m17s 3 OK / 3 True","title":"Enabling auto TLS certs"},{"location":"serving/using-auto-tls/#enabling-automatic-tls-certificate-provisioning","text":"If you install and configure cert-manager, you can configure Knative to automatically obtain new TLS certificates and renew existing ones for Knative Services. To learn more about using secure connections in Knative, see Configuring HTTPS with TLS certificates .","title":"Enabling automatic TLS certificate provisioning"},{"location":"serving/using-auto-tls/#automatic-tls-provision-mode","text":"Knative supports the following Auto TLS modes: Using DNS-01 challenge In this mode, your cluster needs to be able to talk to your DNS server to verify the ownership of your domain. - Provision Certificate per namespace is supported when using DNS-01 challenge mode. - This is the recommended mode for faster certificate provision. - In this mode, a single Certificate will be provisioned per namespace and is reused across the Knative Services within the same namespace. Provision Certificate per Knative Service is supported when using DNS-01 challenge mode. This is the recommended mode for better certificate islation between Knative Services. In this mode, a Certificate will be provisioned for each Knative Service. The TLS effective time is longer as it needs Certificate provision for each Knative Service creation. Using HTTP-01 challenge In this type, your cluster does not need to be able to talk to your DNS server. You must map your domain to the IP of the cluser ingress. When using HTTP-01 challenge, a certificate will be provisioned per Knative Service. HTTP-01 does not support provisioning a certificate per namespace.","title":"Automatic TLS provision mode"},{"location":"serving/using-auto-tls/#before-you-begin","text":"You must meet the following prerequisites to enable Auto TLS: The following must be installed on your Knative cluster: Knative Serving . A Networking layer such as Kourier, Istio with SDS v1.3 or higher, Contour v1.1 or higher, or Gloo v0.18.16 or higher. See Install a networking layer or Istio with SDS, version 1.3 or higher . Note: Currently, Ambassador is unsupported for use with Auto TLS. cert-manager version 1.0.0 and higher . Your Knative cluster must be configured to use a custom domain . Your DNS provider must be setup and configured to your domain. If you want to use HTTP-01 challenge, you need to configure your custom domain to map to the IP of ingress. You can achieve this by adding a DNS A record to map the domain to the IP according to the instructions of your DNS provider.","title":"Before you begin"},{"location":"serving/using-auto-tls/#enabling-auto-tls","text":"To enable support for Auto TLS in Knative:","title":"Enabling Auto TLS"},{"location":"serving/using-auto-tls/#create-cert-manager-clusterissuer","text":"Create and add the ClusterIssuer configuration file to your Knative cluster to define who issues the TLS certificates, how requests are validated, and which DNS provider validates those requests.","title":"Create cert-manager ClusterIssuer"},{"location":"serving/using-auto-tls/#clusterissuer-for-dns-01-challenge","text":"Use the cert-manager reference to determine how to configure your ClusterIssuer file: - See the generic ClusterIssuer example - Also see the DNS01 example Example : Cloud DNS ClusterIssuer configuration file: The following letsencrypt-issuer named ClusterIssuer file is configured for the Let's Encrypt CA and Google Cloud DNS. Under spec , the Let's Encrypt account info, required DNS-01 challenge type, and Cloud DNS provider info defined. For the complete Google Cloud DNS example, see Configuring HTTPS with cert-manager and Google Cloud DNS . apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-dns-issuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email: myemail@gmail.com privateKeySecretRef: # Set privateKeySecretRef to any unused secret name. name: letsencrypt-dns-issuer solvers: - dns01: clouddns: # Set this to your GCP project-id project: $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef: name: cloud-dns-key key: key.json","title":"ClusterIssuer for DNS-01 challenge"},{"location":"serving/using-auto-tls/#clusterissuer-for-http-01-challenge","text":"Run the following command to apply the ClusterIssuer for HTT01 challenge: kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-http01-issuer spec: acme: privateKeySecretRef: name: letsencrypt server: https://acme-v02.api.letsencrypt.org/directory solvers: - http01: ingress: class: istio EOF Ensure that the ClusterIssuer is created successfully: kubectl get clusterissuer <cluster-issuer-name> --output yaml Result: The Status.Conditions should include Ready=True .","title":"ClusterIssuer for HTTP-01 challenge"},{"location":"serving/using-auto-tls/#dns-01-challenge-only-configure-your-dns-provider","text":"If you choose to use DNS-01 challenge, configure which DNS provider is used to validate the DNS-01 challenge requests. Instructions about configuring cert-manager, for all the supported DNS providers, are provided in DNS01 challenge providers and configuration instructions . Example: See how the Google Cloud DNS is defined as the provider: Configuring HTTPS with cert-manager and Google Cloud DNS","title":"DNS-01 challenge only: Configure your DNS provider"},{"location":"serving/using-auto-tls/#install-networking-certmanager-deployment","text":"Determine if networking-certmanager is already installed by running the following command: kubectl get deployment networking-certmanager -n knative-serving If networking-certmanager is not found, run the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml","title":"Install networking-certmanager deployment"},{"location":"serving/using-auto-tls/#install-networking-ns-cert-component","text":"If you choose to use the mode of provisioning certificate per namespace, you need to install networking-ns-cert components. IMPORTANT: Provisioning a certificate per namespace only works with DNS-01 challenge. This component cannot be used with HTTP-01 challenge. Determine if networking-ns-cert deployment is already installed by running the following command: kubectl get deployment networking-ns-cert -n knative-serving If networking-ns-cert deployment is not found, run the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/serving/latest/serving-nscert.yaml","title":"Install networking-ns-cert component"},{"location":"serving/using-auto-tls/#configure-config-certmanager-configmap","text":"Update your config-certmanager ConfigMap in the knative-serving namespace to reference your new ClusterIssuer . Run the following command to edit your config-certmanager ConfigMap: kubectl edit configmap config-certmanager --namespace knative-serving Add the issuerRef within the data section: ... data: ... issuerRef: | kind: ClusterIssuer name: letsencrypt-issuer Example: apiVersion: v1 kind: ConfigMap metadata: name: config-certmanager namespace: knative-serving labels: networking.knative.dev/certificate-provider: cert-manager data: issuerRef: | kind: ClusterIssuer name: letsencrypt-http01-issuer issueRef defines which ClusterIssuer will be used by Knative to issue certificates. Ensure that the file was updated successfully: kubectl get configmap config-certmanager --namespace knative-serving --output yaml","title":"Configure config-certmanager ConfigMap"},{"location":"serving/using-auto-tls/#turn-on-auto-tls","text":"Update the config-network ConfigMap in the knative-serving namespace to enable autoTLS and specify how HTTP requests are handled: Run the following command to edit your config-network ConfigMap: kubectl edit configmap config-network --namespace knative-serving Add the autoTLS: Enabled attribute under the data section: ... data: ... autoTLS: Enabled ... Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... autoTLS: Enabled ... Configure how HTTP and HTTPS requests are handled in the httpProtocol attribute. By default, Knative ingress is configured to serve HTTP traffic ( httpProtocol: Enabled ). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic, you can specify whether or not any HTTP traffic is allowed. Supported httpProtocol values: Enabled : Serve HTTP traffic. Disabled : Rejects all HTTP traffic. Redirected : Responds to HTTP request with a 302 redirect to ask the clients to use HTTPS. ... data: ... autoTLS: Enabled ... Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... autoTLS: Enabled ... httpProtocol: Redirected ... Note: When using HTTP-01 challenge, httpProtocol field has to be set to Enabled to make sure HTTP-01 challenge requests can be accepted by the cluster. Ensure that the file was updated successfully: kubectl get configmap config-network --namespace knative-serving --output yaml Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS certificate is active on your cluster, your Knative services will be able to handle HTTPS traffic.","title":"Turn on Auto TLS"},{"location":"serving/using-auto-tls/#verify-auto-tls","text":"Run the following comand to create a Knative Service: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/autoscaling/autoscale-go/service.yaml When the certificate is provisioned (which could take up to several minutes depending on the challenge type), you should see something like: NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go https://autoscale-go.default.{custom-domain} autoscale-go-6jf85 autoscale-go-6jf85 True Note that the URL will be https in this case.","title":"Verify Auto TLS"},{"location":"serving/using-auto-tls/#disable-auto-tls-per-service-or-route","text":"If you have Auto TLS enabled in your cluster, you can choose to disable Auto TLS for individual services or routes by adding the annotation networking.knative.dev/disableAutoTLS: true . Using the previous autoscale-go example: Edit the service using kubectl edit service.serving.knative.dev/autoscale-go -n default and add the annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : ... networking.knative.dev/disableAutoTLS : \"true\" ... The service URL should now be http , indicating that AutoTLS is disabled: NAME URL LATEST AGE CONDITIONS READY REASON autoscale-go http://autoscale-go.default.1.arenault.dev autoscale-go-dd42t 8m17s 3 OK / 3 True","title":"Disable Auto TLS per service or route"},{"location":"serving/using-cert-manager-on-gcp/","text":"Configuring HTTPS with cert-manager and Google Cloud DNS \u00b6 You can use cert-manager with Knative to automatically provision TLS certificates from Let's Encrypt and use Google Cloud DNS to handle HTTPS requests and validate DNS challenges. The following guide demonstrates how you can setup Knative to handle secure HTTPS requests on Google Cloud Platform, specifically using cert-manager for TLS certificates and Google Cloud DNS as the DNS provider. Learn more about using TLS certificates in Knative: Configuring HTTPS with TLS certificates Enabling automatic TLS certificate provisioning Before you begin \u00b6 You must meet the following prerequisites to configure Knative with cert-manager and Cloud DNS: You must have a GCP project ID with owner privileges . Google Cloud DNS must set up and configure for your domain. You must have a Knative cluster with the following requirements: Knative Serving running. The Knative cluster must be running on Google Cloud Platform. For details about installing the Serving component, see the Knative installation guides . Your Knative cluster must be configured to use a custom domain . cert-manager v0.6.1 or higher installed Your DNS provider must be setup and configured to your domain. Creating a service account and using a Kubernetes secret \u00b6 To allow cert-manager to access and update the DNS record, you must create a service account in GCP, add the key in a Kubernetes secret, and then add that secret to your Knative cluster. Note that several example names are used in the following commands, for example secret or file names, which can all be changed to your liking. Create a service account in GCP with dns.admin project role by running the following commands, where <your-project-id> is the ID of your GCP project: # Set this to your GCP project ID export PROJECT_ID = <your-project-id> # Name of the service account you want to create. export CLOUD_DNS_SA = cert-manager-cloud-dns-admin gcloud --project $PROJECT_ID iam service-accounts \\ create $CLOUD_DNS_SA \\ --display-name \"Service Account to support ACME DNS-01 challenge.\" # Fully-qualified service account name also has project-id information. export CLOUD_DNS_SA = $CLOUD_DNS_SA @ $PROJECT_ID .iam.gserviceaccount.com # Bind the role dns.admin to this service account, so it can be used to support # the ACME DNS01 challenge. gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member serviceAccount: $CLOUD_DNS_SA \\ --role roles/dns.admin Download the service account key by running the following commands: # Make a temporary directory to store key KEY_DIRECTORY = ` mktemp -d ` # Download the secret key file for your service account. gcloud iam service-accounts keys create $KEY_DIRECTORY /cloud-dns-key.json \\ --iam-account = $CLOUD_DNS_SA Create a Kubernetes secret and then add that secret to your Knative cluster by running the following commands: # Upload that as a secret in your Kubernetes cluster. kubectl create secret --namespace cert-manager generic cloud-dns-key \\ --from-file = key.json = $KEY_DIRECTORY /cloud-dns-key.json # Delete the local secret rm -rf $KEY_DIRECTORY Adding your service account to cert-manager \u00b6 Create a ClusterIssuer configuration file to define how cert-manager obtains TLS certificates and how the requests are validated with Cloud DNS. Run the following command to create the ClusterIssuer configuration. The following creates the letsencrypt-issuer ClusterIssuer , that includes your Let's Encrypt account info, DNS-01 challenge type, and Cloud DNS provider info, including your cert-manager-cloud-dns-admin service account. kubectl apply --filename - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-issuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email: myemail@gmail.com privateKeySecretRef: # Set privateKeySecretRef to any unused secret name. name: letsencrypt-issuer solvers: - dns01: clouddns: # Set this to your GCP project-id project: $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef: name: cloud-dns-key key: key.json EOF Ensure that letsencrypt-issuer is created successfully by running the following command: kubectl get clusterissuer --namespace cert-manager letsencrypt-issuer --output yaml Result: The Status.Conditions should include Ready=True . For example: status : acme : uri : https://acme-v02.api.letsencrypt.org/acme/acct/40759665 conditions : - lastTransitionTime : 2018-08-23T01:44:54Z message : The ACME account was registered with the ACME server reason : ACMEAccountRegistered status : \"True\" type : Ready Add letsencrypt-issuer to your ingress secret to configure your certificate \u00b6 To configure how Knative uses your TLS certificates, you create a Certificate to add letsencrypt-issuer to the istio-ingressgateway-certs secret. Note that istio-ingressgateway-certs will be overridden if the secret already exists. Run the following commands to create the my-certificate Certificate , where <your-domain.com> is your domain: # Change this value to the domain you want to use. export DOMAIN = <your-domain.com> kubectl apply --filename - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: my-certificate namespace: istio-system spec: secretName: istio-ingressgateway-certs issuerRef: name: letsencrypt-issuer kind: ClusterIssuer dnsNames: - \"*.default.$DOMAIN\" - \"*.other-namespace.$DOMAIN\" EOF Ensure that my-certificate is created successfully by running the following command: kubectl get certificate --namespace istio-system my-certificate --output yaml Result: The Status.Conditions should include Ready=True . For example: status : acme : order : url : https://acme-v02.api.letsencrypt.org/acme/order/40759665/45358362 conditions : - lastTransitionTime : 2018-08-23T02:28:44Z message : Certificate issued successfully reason : CertIssued status : \"True\" type : Ready Note: If Status.Conditions is Ready=False , that indicates a failure to obtain a certificate, which should be explained in the accompanying error message. Configuring the Knative ingress gateway \u00b6 To configure the knative-ingress-gateway to use the TLS certificate that you created, append the tls: section to the end of your HTTPS port configuration. Run the following commands to configure Knative to use HTTPS connections and send a 301 redirect response for all HTTP requests: kubectl apply --filename - <<EOF apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: knative-ingress-gateway namespace: knative-serving spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" tls: # Sends 301 redirect for all http requests. # Omit to allow http and https. httpsRedirect: true - port: number: 443 name: https protocol: HTTPS hosts: - \"*\" tls: mode: SIMPLE privateKey: /etc/istio/ingressgateway-certs/tls.key serverCertificate: /etc/istio/ingressgateway-certs/tls.crt EOF Congratulations, you can now access your Knative services with secure HTTPS connections. Your Knative cluster is configured to use cert-manager to manually obtain TLS certificates but see the following section about automating that process. Configure Knative for automatic certificate provisioning \u00b6 You can update your Knative configuration to automatically obtain and renew TLS certificates before they expire. To learn more about automatic certificates, see Enabling automatic TLS certificate provisioning .","title":"Configuring HTTPS with Cloud DNS"},{"location":"serving/using-cert-manager-on-gcp/#configuring-https-with-cert-manager-and-google-cloud-dns","text":"You can use cert-manager with Knative to automatically provision TLS certificates from Let's Encrypt and use Google Cloud DNS to handle HTTPS requests and validate DNS challenges. The following guide demonstrates how you can setup Knative to handle secure HTTPS requests on Google Cloud Platform, specifically using cert-manager for TLS certificates and Google Cloud DNS as the DNS provider. Learn more about using TLS certificates in Knative: Configuring HTTPS with TLS certificates Enabling automatic TLS certificate provisioning","title":"Configuring HTTPS with cert-manager and Google Cloud DNS"},{"location":"serving/using-cert-manager-on-gcp/#before-you-begin","text":"You must meet the following prerequisites to configure Knative with cert-manager and Cloud DNS: You must have a GCP project ID with owner privileges . Google Cloud DNS must set up and configure for your domain. You must have a Knative cluster with the following requirements: Knative Serving running. The Knative cluster must be running on Google Cloud Platform. For details about installing the Serving component, see the Knative installation guides . Your Knative cluster must be configured to use a custom domain . cert-manager v0.6.1 or higher installed Your DNS provider must be setup and configured to your domain.","title":"Before you begin"},{"location":"serving/using-cert-manager-on-gcp/#creating-a-service-account-and-using-a-kubernetes-secret","text":"To allow cert-manager to access and update the DNS record, you must create a service account in GCP, add the key in a Kubernetes secret, and then add that secret to your Knative cluster. Note that several example names are used in the following commands, for example secret or file names, which can all be changed to your liking. Create a service account in GCP with dns.admin project role by running the following commands, where <your-project-id> is the ID of your GCP project: # Set this to your GCP project ID export PROJECT_ID = <your-project-id> # Name of the service account you want to create. export CLOUD_DNS_SA = cert-manager-cloud-dns-admin gcloud --project $PROJECT_ID iam service-accounts \\ create $CLOUD_DNS_SA \\ --display-name \"Service Account to support ACME DNS-01 challenge.\" # Fully-qualified service account name also has project-id information. export CLOUD_DNS_SA = $CLOUD_DNS_SA @ $PROJECT_ID .iam.gserviceaccount.com # Bind the role dns.admin to this service account, so it can be used to support # the ACME DNS01 challenge. gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member serviceAccount: $CLOUD_DNS_SA \\ --role roles/dns.admin Download the service account key by running the following commands: # Make a temporary directory to store key KEY_DIRECTORY = ` mktemp -d ` # Download the secret key file for your service account. gcloud iam service-accounts keys create $KEY_DIRECTORY /cloud-dns-key.json \\ --iam-account = $CLOUD_DNS_SA Create a Kubernetes secret and then add that secret to your Knative cluster by running the following commands: # Upload that as a secret in your Kubernetes cluster. kubectl create secret --namespace cert-manager generic cloud-dns-key \\ --from-file = key.json = $KEY_DIRECTORY /cloud-dns-key.json # Delete the local secret rm -rf $KEY_DIRECTORY","title":"Creating a service account and using a Kubernetes secret"},{"location":"serving/using-cert-manager-on-gcp/#adding-your-service-account-to-cert-manager","text":"Create a ClusterIssuer configuration file to define how cert-manager obtains TLS certificates and how the requests are validated with Cloud DNS. Run the following command to create the ClusterIssuer configuration. The following creates the letsencrypt-issuer ClusterIssuer , that includes your Let's Encrypt account info, DNS-01 challenge type, and Cloud DNS provider info, including your cert-manager-cloud-dns-admin service account. kubectl apply --filename - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-issuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email: myemail@gmail.com privateKeySecretRef: # Set privateKeySecretRef to any unused secret name. name: letsencrypt-issuer solvers: - dns01: clouddns: # Set this to your GCP project-id project: $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef: name: cloud-dns-key key: key.json EOF Ensure that letsencrypt-issuer is created successfully by running the following command: kubectl get clusterissuer --namespace cert-manager letsencrypt-issuer --output yaml Result: The Status.Conditions should include Ready=True . For example: status : acme : uri : https://acme-v02.api.letsencrypt.org/acme/acct/40759665 conditions : - lastTransitionTime : 2018-08-23T01:44:54Z message : The ACME account was registered with the ACME server reason : ACMEAccountRegistered status : \"True\" type : Ready","title":"Adding your service account to cert-manager"},{"location":"serving/using-cert-manager-on-gcp/#add-letsencrypt-issuer-to-your-ingress-secret-to-configure-your-certificate","text":"To configure how Knative uses your TLS certificates, you create a Certificate to add letsencrypt-issuer to the istio-ingressgateway-certs secret. Note that istio-ingressgateway-certs will be overridden if the secret already exists. Run the following commands to create the my-certificate Certificate , where <your-domain.com> is your domain: # Change this value to the domain you want to use. export DOMAIN = <your-domain.com> kubectl apply --filename - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: my-certificate namespace: istio-system spec: secretName: istio-ingressgateway-certs issuerRef: name: letsencrypt-issuer kind: ClusterIssuer dnsNames: - \"*.default.$DOMAIN\" - \"*.other-namespace.$DOMAIN\" EOF Ensure that my-certificate is created successfully by running the following command: kubectl get certificate --namespace istio-system my-certificate --output yaml Result: The Status.Conditions should include Ready=True . For example: status : acme : order : url : https://acme-v02.api.letsencrypt.org/acme/order/40759665/45358362 conditions : - lastTransitionTime : 2018-08-23T02:28:44Z message : Certificate issued successfully reason : CertIssued status : \"True\" type : Ready Note: If Status.Conditions is Ready=False , that indicates a failure to obtain a certificate, which should be explained in the accompanying error message.","title":"Add letsencrypt-issuer to your ingress secret to configure your certificate"},{"location":"serving/using-cert-manager-on-gcp/#configuring-the-knative-ingress-gateway","text":"To configure the knative-ingress-gateway to use the TLS certificate that you created, append the tls: section to the end of your HTTPS port configuration. Run the following commands to configure Knative to use HTTPS connections and send a 301 redirect response for all HTTP requests: kubectl apply --filename - <<EOF apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: knative-ingress-gateway namespace: knative-serving spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" tls: # Sends 301 redirect for all http requests. # Omit to allow http and https. httpsRedirect: true - port: number: 443 name: https protocol: HTTPS hosts: - \"*\" tls: mode: SIMPLE privateKey: /etc/istio/ingressgateway-certs/tls.key serverCertificate: /etc/istio/ingressgateway-certs/tls.crt EOF Congratulations, you can now access your Knative services with secure HTTPS connections. Your Knative cluster is configured to use cert-manager to manually obtain TLS certificates but see the following section about automating that process.","title":"Configuring the Knative ingress gateway"},{"location":"serving/using-cert-manager-on-gcp/#configure-knative-for-automatic-certificate-provisioning","text":"You can update your Knative configuration to automatically obtain and renew TLS certificates before they expire. To learn more about automatic certificates, see Enabling automatic TLS certificate provisioning .","title":"Configure Knative for automatic certificate provisioning"},{"location":"serving/using-external-dns-on-gcp/","text":"Using ExternalDNS on Google Cloud Platform to automate DNS setup \u00b6 ExternalDNS is a tool that synchronizes exposed Kubernetes Services and Ingresses with DNS providers. This doc explains how to set up ExternalDNS within a Knative cluster using Google Cloud DNS to automate the process of publishing the Knative domain. Set up environtment variables \u00b6 Run the following command to configure the environment variables export PROJECT_NAME = <your-google-cloud-project-name> export CUSTOM_DOMAIN = <your-custom-domain-used-in-knative> export CLUSTER_NAME = <knative-cluster-name> export CLUSTER_ZONE = <knative-cluster-zone> Set up Kubernetes Engine cluster with CloudDNS read/write permissions \u00b6 There are two ways to set up a Kubernetes Engine cluster with CloudDNS read/write permissions. Cluster with Cloud DNS scope \u00b6 You can create a GKE cluster with Cloud DNS scope by entering the following command: gcloud container clusters create $CLUSTER_NAME \\ --zone = $CLUSTER_ZONE \\ --cluster-version = latest \\ --machine-type = n1-standard-4 \\ --enable-autoscaling --min-nodes = 1 --max-nodes = 10 \\ --enable-autorepair \\ --scopes = service-control,service-management,compute-rw,storage-ro,cloud-platform,logging-write,monitoring-write,pubsub,datastore, \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" \\ --num-nodes = 3 Note that by using this way, any pod within the cluster will have permissions to read/write CloudDNS. Cluster with Cloud DNS Admin Service Account credential \u00b6 Create a GKE cluster without Cloud DNS scope by entering the following command: gcloud container clusters create $CLUSTER_NAME \\ --zone = $CLUSTER_ZONE \\ --cluster-version = latest \\ --machine-type = n1-standard-4 \\ --enable-autoscaling --min-nodes = 1 --max-nodes = 10 \\ --enable-autorepair \\ --scopes = service-control,service-management,compute-rw,storage-ro,cloud-platform,logging-write,monitoring-write,pubsub,datastore \\ --num-nodes = 3 Create a new service account for Cloud DNS admin role. # Name of the service account you want to create. export CLOUD_DNS_SA = cloud-dns-admin gcloud --project $PROJECT_NAME iam service-accounts \\ create $CLOUD_DNS_SA \\ --display-name \"Service Account to support ACME DNS-01 challenge.\" Bind the role dns.admin to the newly created service account. # Fully-qualified service account name also has project-id information. export CLOUD_DNS_SA = $CLOUD_DNS_SA @ $PROJECT_NAME .iam.gserviceaccount.com gcloud projects add-iam-policy-binding $PROJECT_NAME \\ --member serviceAccount: $CLOUD_DNS_SA \\ --role roles/dns.admin Download the secret key file for your service account. gcloud iam service-accounts keys create ~/key.json \\ --iam-account = $CLOUD_DNS_SA Upload the service account credential to your cluster. This command uses the secret name cloud-dns-key , but you can choose a different name. kubectl create secret generic cloud-dns-key \\ --from-file = key.json = $HOME /key.json Delete the local secret rm ~/key.json Now your cluster has the credential of your CloudDNS admin service account. And it can be used to access your Cloud DNS. You can enforce the access of the credentail secret within your cluster, so that only the pods that have the permission to get the credential secret can access your Cloud DNS. Set up Knative \u00b6 Follow the instruction to install Knative on your cluster. Configure Knative to use your custom domain. kubectl edit cm config-domain --namespace knative-serving This command opens your default text editor and allows you to edit the config map. apiVersion: v1 data: example.com: \"\" kind: ConfigMap [...] Edit the file to replace example.com with your custom domain (the value of $CUSTOM_DOMAIN ) and save your changes. In this example, we use domain external-dns-test.my-org.do for all routes: apiVersion: v1 data: external-dns-test.my-org.do: \"\" kind: ConfigMap [...] Set up ExternalDNS \u00b6 This guide uses Google Cloud Platform as an example to show how to set up ExternalDNS. You can find detailed instructions for other cloud providers in the ExternalDNS documentation . Create a DNS zone for managing DNS records \u00b6 Skip this step if you already have a zone for managing the DNS records of your custom domain. A DNS zone which will contain the managed DNS records needs to be created. Use the following command to create a DNS zone with Google Cloud DNS : export DNS_ZONE_NAME = <dns-zone-name> gcloud dns managed-zones create $DNS_ZONE_NAME \\ --dns-name $CUSTOM_DOMAIN \\ --description \"Automatically managed zone by kubernetes.io/external-dns\" Make a note of the nameservers that were assigned to your new zone. gcloud dns record-sets list \\ --zone $DNS_ZONE_NAME \\ --name $CUSTOM_DOMAIN \\ --type NS You should see output similar to the following assuming your custom domain is external-dns-test.my-org.do : NAME TYPE TTL DATA external-dns-test.my-org.do. NS 21600 ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com. In this case, the DNS nameservers are ns-cloud-{e1-e4}.googledomains.com . Yours could differ slightly, e.g. {a1-a4}, {b1-b4} etc. If this zone has the parent zone, you need to add NS records of this zone into the parent zone so that this zone can be found from the parent. Assuming the parent zone is my-org-do and the parent domain is my-org.do , and the parent zone is also hosted at Google Cloud DNS, you can follow these steps to add the NS records of this zone into the parent zone: gcloud dns record-sets transaction start --zone \"my-org-do\" gcloud dns record-sets transaction add ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name \"external-dns-test.my-org.do.\" --ttl 300 --type NS --zone \"my-org-do\" gcloud dns record-sets transaction execute --zone \"my-org-do\" Deploy ExternalDNS \u00b6 Firstly, choose the manifest of ExternalDNS. Use below manifest if you set up your cluster with CloudDNS scope . apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest args : - --source=service - --domain-filter=$CUSTOM_DOMAIN # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=google - --google-project=$PROJECT_NAME # Use this to specify a project different from the one external-dns is running inside - --policy=sync # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --registry=txt - --txt-owner-id=my-identifier Or use below manifest if you set up your cluster with CloudDNS service account credential . apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods,secrets\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate template : metadata : labels : app : external-dns spec : volumes : - name : google-cloud-key secret : secretName : cloud-dns-key serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest volumeMounts : - name : google-cloud-key mountPath : /var/secrets/google env : - name : GOOGLE_APPLICATION_CREDENTIALS value : /var/secrets/google/key.json args : - --source=service - --domain-filter=$CUSTOM_DOMAIN # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=google - --google-project=$PROJECT_NAME # Use this to specify a project different from the one external-dns is running inside - --policy=sync # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --registry=txt - --txt-owner-id=my-identifier Then use the following command to apply the manifest you chose to install ExternalDNS cat <<EOF | kubectl apply --filename - <your-chosen-manifest> EOF You should see ExternalDNS is installed by running: kubectl get deployment external-dns Configuring Knative Gateway service \u00b6 In order to publish the Knative Gateway service, the annotation external-dns.alpha.kubernetes.io/hostname: '*.$CUSTOM_DOMAIN needs to be added into Knative gateway service: INGRESSGATEWAY = istio-ingressgateway kubectl edit svc $INGRESSGATEWAY --namespace istio-system This command opens your default text editor and allows you to add the annotation to istio-ingressgateway service. After you've added your annotation, your file may look similar to this (assuming your custom domain is external-dns-test.my-org.do ): apiVersion: v1 kind: Service metadata: annotations: external-dns.alpha.kubernetes.io/hostname: '*.external-dns-test.my-org.do' ... Verify ExternalDNS works \u00b6 After roughly two minutes, check that a corresponding DNS record for your service was created. gcloud dns record-sets list --zone $DNS_ZONE_NAME --name \"*. $CUSTOM_DOMAIN .\" You should see output similar to: NAME TYPE TTL DATA *.external-dns-test.my-org.do. A 300 35.231.248.30 *.external-dns-test.my-org.do. TXT 300 \"heritage=external-dns,external-dns/owner=my-identifier,external-dns/resource=service/istio-system/istio-ingressgateway\" Verify domain has been published \u00b6 You can check if the domain has been published to the Internet be entering the following command: host test.external-dns-test.my-org.do You should see the below result after the domain is published: test.external-dns-test.my-org.do has address 35.231.248.30 Note: The process of publishing the domain to the Internet can take several minutes.","title":"Using ExternalDNS on Google Cloud Platform to automate DNS setup"},{"location":"serving/using-external-dns-on-gcp/#using-externaldns-on-google-cloud-platform-to-automate-dns-setup","text":"ExternalDNS is a tool that synchronizes exposed Kubernetes Services and Ingresses with DNS providers. This doc explains how to set up ExternalDNS within a Knative cluster using Google Cloud DNS to automate the process of publishing the Knative domain.","title":"Using ExternalDNS on Google Cloud Platform to automate DNS setup"},{"location":"serving/using-external-dns-on-gcp/#set-up-environtment-variables","text":"Run the following command to configure the environment variables export PROJECT_NAME = <your-google-cloud-project-name> export CUSTOM_DOMAIN = <your-custom-domain-used-in-knative> export CLUSTER_NAME = <knative-cluster-name> export CLUSTER_ZONE = <knative-cluster-zone>","title":"Set up environtment variables"},{"location":"serving/using-external-dns-on-gcp/#set-up-kubernetes-engine-cluster-with-clouddns-readwrite-permissions","text":"There are two ways to set up a Kubernetes Engine cluster with CloudDNS read/write permissions.","title":"Set up Kubernetes Engine cluster with CloudDNS read/write permissions"},{"location":"serving/using-external-dns-on-gcp/#cluster-with-cloud-dns-scope","text":"You can create a GKE cluster with Cloud DNS scope by entering the following command: gcloud container clusters create $CLUSTER_NAME \\ --zone = $CLUSTER_ZONE \\ --cluster-version = latest \\ --machine-type = n1-standard-4 \\ --enable-autoscaling --min-nodes = 1 --max-nodes = 10 \\ --enable-autorepair \\ --scopes = service-control,service-management,compute-rw,storage-ro,cloud-platform,logging-write,monitoring-write,pubsub,datastore, \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" \\ --num-nodes = 3 Note that by using this way, any pod within the cluster will have permissions to read/write CloudDNS.","title":"Cluster with Cloud DNS scope"},{"location":"serving/using-external-dns-on-gcp/#cluster-with-cloud-dns-admin-service-account-credential","text":"Create a GKE cluster without Cloud DNS scope by entering the following command: gcloud container clusters create $CLUSTER_NAME \\ --zone = $CLUSTER_ZONE \\ --cluster-version = latest \\ --machine-type = n1-standard-4 \\ --enable-autoscaling --min-nodes = 1 --max-nodes = 10 \\ --enable-autorepair \\ --scopes = service-control,service-management,compute-rw,storage-ro,cloud-platform,logging-write,monitoring-write,pubsub,datastore \\ --num-nodes = 3 Create a new service account for Cloud DNS admin role. # Name of the service account you want to create. export CLOUD_DNS_SA = cloud-dns-admin gcloud --project $PROJECT_NAME iam service-accounts \\ create $CLOUD_DNS_SA \\ --display-name \"Service Account to support ACME DNS-01 challenge.\" Bind the role dns.admin to the newly created service account. # Fully-qualified service account name also has project-id information. export CLOUD_DNS_SA = $CLOUD_DNS_SA @ $PROJECT_NAME .iam.gserviceaccount.com gcloud projects add-iam-policy-binding $PROJECT_NAME \\ --member serviceAccount: $CLOUD_DNS_SA \\ --role roles/dns.admin Download the secret key file for your service account. gcloud iam service-accounts keys create ~/key.json \\ --iam-account = $CLOUD_DNS_SA Upload the service account credential to your cluster. This command uses the secret name cloud-dns-key , but you can choose a different name. kubectl create secret generic cloud-dns-key \\ --from-file = key.json = $HOME /key.json Delete the local secret rm ~/key.json Now your cluster has the credential of your CloudDNS admin service account. And it can be used to access your Cloud DNS. You can enforce the access of the credentail secret within your cluster, so that only the pods that have the permission to get the credential secret can access your Cloud DNS.","title":"Cluster with Cloud DNS Admin Service Account credential"},{"location":"serving/using-external-dns-on-gcp/#set-up-knative","text":"Follow the instruction to install Knative on your cluster. Configure Knative to use your custom domain. kubectl edit cm config-domain --namespace knative-serving This command opens your default text editor and allows you to edit the config map. apiVersion: v1 data: example.com: \"\" kind: ConfigMap [...] Edit the file to replace example.com with your custom domain (the value of $CUSTOM_DOMAIN ) and save your changes. In this example, we use domain external-dns-test.my-org.do for all routes: apiVersion: v1 data: external-dns-test.my-org.do: \"\" kind: ConfigMap [...]","title":"Set up Knative"},{"location":"serving/using-external-dns-on-gcp/#set-up-externaldns","text":"This guide uses Google Cloud Platform as an example to show how to set up ExternalDNS. You can find detailed instructions for other cloud providers in the ExternalDNS documentation .","title":"Set up ExternalDNS"},{"location":"serving/using-external-dns-on-gcp/#create-a-dns-zone-for-managing-dns-records","text":"Skip this step if you already have a zone for managing the DNS records of your custom domain. A DNS zone which will contain the managed DNS records needs to be created. Use the following command to create a DNS zone with Google Cloud DNS : export DNS_ZONE_NAME = <dns-zone-name> gcloud dns managed-zones create $DNS_ZONE_NAME \\ --dns-name $CUSTOM_DOMAIN \\ --description \"Automatically managed zone by kubernetes.io/external-dns\" Make a note of the nameservers that were assigned to your new zone. gcloud dns record-sets list \\ --zone $DNS_ZONE_NAME \\ --name $CUSTOM_DOMAIN \\ --type NS You should see output similar to the following assuming your custom domain is external-dns-test.my-org.do : NAME TYPE TTL DATA external-dns-test.my-org.do. NS 21600 ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com. In this case, the DNS nameservers are ns-cloud-{e1-e4}.googledomains.com . Yours could differ slightly, e.g. {a1-a4}, {b1-b4} etc. If this zone has the parent zone, you need to add NS records of this zone into the parent zone so that this zone can be found from the parent. Assuming the parent zone is my-org-do and the parent domain is my-org.do , and the parent zone is also hosted at Google Cloud DNS, you can follow these steps to add the NS records of this zone into the parent zone: gcloud dns record-sets transaction start --zone \"my-org-do\" gcloud dns record-sets transaction add ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name \"external-dns-test.my-org.do.\" --ttl 300 --type NS --zone \"my-org-do\" gcloud dns record-sets transaction execute --zone \"my-org-do\"","title":"Create a DNS zone for managing DNS records"},{"location":"serving/using-external-dns-on-gcp/#deploy-externaldns","text":"Firstly, choose the manifest of ExternalDNS. Use below manifest if you set up your cluster with CloudDNS scope . apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest args : - --source=service - --domain-filter=$CUSTOM_DOMAIN # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=google - --google-project=$PROJECT_NAME # Use this to specify a project different from the one external-dns is running inside - --policy=sync # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --registry=txt - --txt-owner-id=my-identifier Or use below manifest if you set up your cluster with CloudDNS service account credential . apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods,secrets\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate template : metadata : labels : app : external-dns spec : volumes : - name : google-cloud-key secret : secretName : cloud-dns-key serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest volumeMounts : - name : google-cloud-key mountPath : /var/secrets/google env : - name : GOOGLE_APPLICATION_CREDENTIALS value : /var/secrets/google/key.json args : - --source=service - --domain-filter=$CUSTOM_DOMAIN # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=google - --google-project=$PROJECT_NAME # Use this to specify a project different from the one external-dns is running inside - --policy=sync # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --registry=txt - --txt-owner-id=my-identifier Then use the following command to apply the manifest you chose to install ExternalDNS cat <<EOF | kubectl apply --filename - <your-chosen-manifest> EOF You should see ExternalDNS is installed by running: kubectl get deployment external-dns","title":"Deploy ExternalDNS"},{"location":"serving/using-external-dns-on-gcp/#configuring-knative-gateway-service","text":"In order to publish the Knative Gateway service, the annotation external-dns.alpha.kubernetes.io/hostname: '*.$CUSTOM_DOMAIN needs to be added into Knative gateway service: INGRESSGATEWAY = istio-ingressgateway kubectl edit svc $INGRESSGATEWAY --namespace istio-system This command opens your default text editor and allows you to add the annotation to istio-ingressgateway service. After you've added your annotation, your file may look similar to this (assuming your custom domain is external-dns-test.my-org.do ): apiVersion: v1 kind: Service metadata: annotations: external-dns.alpha.kubernetes.io/hostname: '*.external-dns-test.my-org.do' ...","title":"Configuring Knative Gateway service"},{"location":"serving/using-external-dns-on-gcp/#verify-externaldns-works","text":"After roughly two minutes, check that a corresponding DNS record for your service was created. gcloud dns record-sets list --zone $DNS_ZONE_NAME --name \"*. $CUSTOM_DOMAIN .\" You should see output similar to: NAME TYPE TTL DATA *.external-dns-test.my-org.do. A 300 35.231.248.30 *.external-dns-test.my-org.do. TXT 300 \"heritage=external-dns,external-dns/owner=my-identifier,external-dns/resource=service/istio-system/istio-ingressgateway\"","title":"Verify ExternalDNS works"},{"location":"serving/using-external-dns-on-gcp/#verify-domain-has-been-published","text":"You can check if the domain has been published to the Internet be entering the following command: host test.external-dns-test.my-org.do You should see the below result after the domain is published: test.external-dns-test.my-org.do has address 35.231.248.30 Note: The process of publishing the domain to the Internet can take several minutes.","title":"Verify domain has been published"},{"location":"serving/using-subroutes/","text":"Creating and using Subroutes \u00b6 Subroutes are most effective when used with multiple revisions. When defining a Knative service/route, the traffic section of the spec can split between the different revisions. For example: traffic : - percent : 0 revisionName : foo - percent : 40 revisionName : bar - percent : 60 revisionName : baz This allows anyone targeting the main route to have a 0% chance of hitting revision foo , 40% chance of hitting revision bar and 60% chance of hitting revision baz . Using tags to create target URLs \u00b6 The spec defines an attribute called tag . When a tag is applied to a route, an address for the specific traffic target is created. traffic : - percent : 0 revisionName : foo tag : staging - percent : 40 revisionName : bar - percent : 60 revisionName : baz In the above example, you can access the staging target by accessing staging-<route name>.<namespace>.<domain> . The targets for bar and baz can only be accessed using the main route, <route name>.<namespace>.<domain> . When a traffic target gets tagged, a new Kubernetes service is created for it so that other services can also access it within the cluster. From the above example, a new Kubernetes service called staging-<route name> will be created in the same namespace. This service has the ability to override the visibility of this specific route by applying the label networking.knative.dev/visibility with value cluster-local . See cluster local routes for more information about how to restrict visibility on the specific route.","title":"Creating and using Subroutes"},{"location":"serving/using-subroutes/#creating-and-using-subroutes","text":"Subroutes are most effective when used with multiple revisions. When defining a Knative service/route, the traffic section of the spec can split between the different revisions. For example: traffic : - percent : 0 revisionName : foo - percent : 40 revisionName : bar - percent : 60 revisionName : baz This allows anyone targeting the main route to have a 0% chance of hitting revision foo , 40% chance of hitting revision bar and 60% chance of hitting revision baz .","title":"Creating and using Subroutes"},{"location":"serving/using-subroutes/#using-tags-to-create-target-urls","text":"The spec defines an attribute called tag . When a tag is applied to a route, an address for the specific traffic target is created. traffic : - percent : 0 revisionName : foo tag : staging - percent : 40 revisionName : bar - percent : 60 revisionName : baz In the above example, you can access the staging target by accessing staging-<route name>.<namespace>.<domain> . The targets for bar and baz can only be accessed using the main route, <route name>.<namespace>.<domain> . When a traffic target gets tagged, a new Kubernetes service is created for it so that other services can also access it within the cluster. From the above example, a new Kubernetes service called staging-<route name> will be created in the same namespace. This service has the ability to override the visibility of this specific route by applying the label networking.knative.dev/visibility with value cluster-local . See cluster local routes for more information about how to restrict visibility on the specific route.","title":"Using tags to create target URLs"},{"location":"serving/webhook-customizations/","text":"Exclude namespaces from the Knative webhook \u00b6 The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades. You can configure the label webhooks.knative.dev/exclude to allow namespaces to bypass the Knative webhook. apiVersion : v1 kind : Namespace metadata : name : knative-dev labels : webhooks.knative.dev/exclude : \"true\"","title":"Exclude namespaces from the Knative webhook"},{"location":"serving/webhook-customizations/#exclude-namespaces-from-the-knative-webhook","text":"The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades. You can configure the label webhooks.knative.dev/exclude to allow namespaces to bypass the Knative webhook. apiVersion : v1 kind : Namespace metadata : name : knative-dev labels : webhooks.knative.dev/exclude : \"true\"","title":"Exclude namespaces from the Knative webhook"},{"location":"serving/autoscaling/","text":"Autoscaling \u00b6 One of the main features of Knative is automatic scaling of replicas for an application to closely match incoming demand, including scaling applications to zero if no traffic is being received. Knative Serving enables this by default, using the Knative Pod Autoscaler (KPA). The Autoscaler component watches traffic flow to the application, and scales replicas up or down based on configured metrics. Knative services default to using autoscaling settings that are suitable for the majority of use cases. However, some workloads may require a custom, more finely-tuned configuration. This guide provides information about configuration options that you can modify to fit the requirements of your workload. For more information about how autoscaling for Knative works, see the Autoscaling concepts documentation. For more information about which metrics can be used to control the Autoscaler, see the metrics documentation. Optional autoscaling configuration tasks \u00b6 Configure your Knative deployment to use the Kubernetes Horizontal Pod Autoscaler (HPA) instead of the default KPA. For how to install HPA, see Install optional Eventing extensions . Disable scale to zero functionality for your cluster ( global configuration only ). Configure the type of metrics your Autoscaler consumes. Configure concurrency limits for applications. Try out the Go Autoscale Sample App .","title":"Overview"},{"location":"serving/autoscaling/#autoscaling","text":"One of the main features of Knative is automatic scaling of replicas for an application to closely match incoming demand, including scaling applications to zero if no traffic is being received. Knative Serving enables this by default, using the Knative Pod Autoscaler (KPA). The Autoscaler component watches traffic flow to the application, and scales replicas up or down based on configured metrics. Knative services default to using autoscaling settings that are suitable for the majority of use cases. However, some workloads may require a custom, more finely-tuned configuration. This guide provides information about configuration options that you can modify to fit the requirements of your workload. For more information about how autoscaling for Knative works, see the Autoscaling concepts documentation. For more information about which metrics can be used to control the Autoscaler, see the metrics documentation.","title":"Autoscaling"},{"location":"serving/autoscaling/#optional-autoscaling-configuration-tasks","text":"Configure your Knative deployment to use the Kubernetes Horizontal Pod Autoscaler (HPA) instead of the default KPA. For how to install HPA, see Install optional Eventing extensions . Disable scale to zero functionality for your cluster ( global configuration only ). Configure the type of metrics your Autoscaler consumes. Configure concurrency limits for applications. Try out the Go Autoscale Sample App .","title":"Optional autoscaling configuration tasks"},{"location":"serving/autoscaling/autoscaling-concepts/","text":"Autoscaling concepts \u00b6 This section covers conceptual information about which Autoscaler types are supported, as well as fundamental information about how autoscaling is configured. Supported Autoscaler types \u00b6 Knative Serving supports the implementation of Knative Pod Autoscaler (KPA) and Kubernetes' Horizontal Pod Autoscaler (HPA). The features and limitations of each of these Autoscalers are listed below. IMPORTANT: If you want to use Kubernetes Horizontal Pod Autoscaler (HPA), you must install it after you install Knative Serving. For how to install HPA, see Install optional Eventing extensions . Knative Pod Autoscaler (KPA) \u00b6 Part of the Knative Serving core and enabled by default once Knative Serving is installed. Supports scale to zero functionality. Does not support CPU-based autoscaling. Horizontal Pod Autoscaler (HPA) \u00b6 Not part of the Knative Serving core, and you must install Knative Serving first. Does not support scale to zero functionality. Supports CPU-based autoscaling. Configuring the Autoscaler implementation \u00b6 The type of Autoscaler implementation (KPA or HPA) can be configured by using the class annotation. Global settings key: pod-autoscaler-class Per-revision annotation key: autoscaling.knative.dev/class Possible values: \"kpa.autoscaling.knative.dev\" or \"hpa.autoscaling.knative.dev\" Default: \"kpa.autoscaling.knative.dev\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"kpa.autoscaling.knative.dev\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" Global versus per-revision settings \u00b6 Configuring for autoscaling in Knative can be set using either global or per-revision settings. If no per-revision autoscaling settings are specified, the global settings will be used. If per-revision settings are specified, these will override the global settings when both types of settings exist. Global settings \u00b6 Global settings for autoscaling are configured using the config-autoscaler ConfigMap. If you installed Knative Serving using the Operator, you can set global configuration settings in the spec.config.autoscaler ConfigMap, located in the KnativeServing custom resource (CR). Example of the default autoscaling ConfigMap \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"100\" container-concurrency-target-percentage : \"0.7\" enable-scale-to-zero : \"true\" max-scale-up-rate : \"1000\" max-scale-down-rate : \"2\" panic-window-percentage : \"10\" panic-threshold-percentage : \"200\" scale-to-zero-grace-period : \"30s\" scale-to-zero-pod-retention-period : \"0s\" stable-window : \"60s\" target-burst-capacity : \"200\" requests-per-second-target-default : \"200\" Per-revision settings \u00b6 Per-revision settings for autoscaling are configured by adding annotations to a revision. Example \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"70\" IMPORTANT: If you are creating revisions by using a service or configuration, you must set the annotations in the revision template so that any modifications will be applied to each revision as they are created. Setting annotations in the top level metadata of a single revision will not propagate the changes to other revisions and will not apply changes to the autoscaling configuration for your application.","title":"Autoscaling concepts"},{"location":"serving/autoscaling/autoscaling-concepts/#autoscaling-concepts","text":"This section covers conceptual information about which Autoscaler types are supported, as well as fundamental information about how autoscaling is configured.","title":"Autoscaling concepts"},{"location":"serving/autoscaling/autoscaling-concepts/#supported-autoscaler-types","text":"Knative Serving supports the implementation of Knative Pod Autoscaler (KPA) and Kubernetes' Horizontal Pod Autoscaler (HPA). The features and limitations of each of these Autoscalers are listed below. IMPORTANT: If you want to use Kubernetes Horizontal Pod Autoscaler (HPA), you must install it after you install Knative Serving. For how to install HPA, see Install optional Eventing extensions .","title":"Supported Autoscaler types"},{"location":"serving/autoscaling/autoscaling-concepts/#knative-pod-autoscaler-kpa","text":"Part of the Knative Serving core and enabled by default once Knative Serving is installed. Supports scale to zero functionality. Does not support CPU-based autoscaling.","title":"Knative Pod Autoscaler (KPA)"},{"location":"serving/autoscaling/autoscaling-concepts/#horizontal-pod-autoscaler-hpa","text":"Not part of the Knative Serving core, and you must install Knative Serving first. Does not support scale to zero functionality. Supports CPU-based autoscaling.","title":"Horizontal Pod Autoscaler (HPA)"},{"location":"serving/autoscaling/autoscaling-concepts/#configuring-the-autoscaler-implementation","text":"The type of Autoscaler implementation (KPA or HPA) can be configured by using the class annotation. Global settings key: pod-autoscaler-class Per-revision annotation key: autoscaling.knative.dev/class Possible values: \"kpa.autoscaling.knative.dev\" or \"hpa.autoscaling.knative.dev\" Default: \"kpa.autoscaling.knative.dev\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"kpa.autoscaling.knative.dev\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\"","title":"Configuring the Autoscaler implementation"},{"location":"serving/autoscaling/autoscaling-concepts/#global-versus-per-revision-settings","text":"Configuring for autoscaling in Knative can be set using either global or per-revision settings. If no per-revision autoscaling settings are specified, the global settings will be used. If per-revision settings are specified, these will override the global settings when both types of settings exist.","title":"Global versus per-revision settings"},{"location":"serving/autoscaling/autoscaling-concepts/#global-settings","text":"Global settings for autoscaling are configured using the config-autoscaler ConfigMap. If you installed Knative Serving using the Operator, you can set global configuration settings in the spec.config.autoscaler ConfigMap, located in the KnativeServing custom resource (CR).","title":"Global settings"},{"location":"serving/autoscaling/autoscaling-concepts/#example-of-the-default-autoscaling-configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"100\" container-concurrency-target-percentage : \"0.7\" enable-scale-to-zero : \"true\" max-scale-up-rate : \"1000\" max-scale-down-rate : \"2\" panic-window-percentage : \"10\" panic-threshold-percentage : \"200\" scale-to-zero-grace-period : \"30s\" scale-to-zero-pod-retention-period : \"0s\" stable-window : \"60s\" target-burst-capacity : \"200\" requests-per-second-target-default : \"200\"","title":"Example of the default autoscaling ConfigMap"},{"location":"serving/autoscaling/autoscaling-concepts/#per-revision-settings","text":"Per-revision settings for autoscaling are configured by adding annotations to a revision.","title":"Per-revision settings"},{"location":"serving/autoscaling/autoscaling-concepts/#example","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"70\" IMPORTANT: If you are creating revisions by using a service or configuration, you must set the annotations in the revision template so that any modifications will be applied to each revision as they are created. Setting annotations in the top level metadata of a single revision will not propagate the changes to other revisions and will not apply changes to the autoscaling configuration for your application.","title":"Example"},{"location":"serving/autoscaling/autoscaling-metrics/","text":"Metrics \u00b6 The metric configuration defines which metric type is watched by the Autoscaler. Setting metrics per revision \u00b6 For per-revision configuration, this is determined using the autoscaling.knative.dev/metric annotation. The possible metric types that can be configured per revision depend on the type of Autoscaler implementation you are using: The default KPA Autoscaler supports the concurrency and rps metrics. The HPA Autoscaler supports the cpu metric. For more information about KPA and HPA, see the documentation on Supported Autoscaler types . Per-revision annotation key: autoscaling.knative.dev/metric Possible values: \"concurrency\" , \"rps\" or \"cpu\" , depending on your Autoscaler type. The cpu metric is only supported on revisions with the HPA class. Default: \"concurrency\" Per-revision concurrency configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"concurrency\" Per-revision rps configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"rps\" Per-revision cpu configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"cpu\" Next steps \u00b6 Configure concurrency targets for applications Configure requests per second targets for replicas of an application","title":"Metrics"},{"location":"serving/autoscaling/autoscaling-metrics/#metrics","text":"The metric configuration defines which metric type is watched by the Autoscaler.","title":"Metrics"},{"location":"serving/autoscaling/autoscaling-metrics/#setting-metrics-per-revision","text":"For per-revision configuration, this is determined using the autoscaling.knative.dev/metric annotation. The possible metric types that can be configured per revision depend on the type of Autoscaler implementation you are using: The default KPA Autoscaler supports the concurrency and rps metrics. The HPA Autoscaler supports the cpu metric. For more information about KPA and HPA, see the documentation on Supported Autoscaler types . Per-revision annotation key: autoscaling.knative.dev/metric Possible values: \"concurrency\" , \"rps\" or \"cpu\" , depending on your Autoscaler type. The cpu metric is only supported on revisions with the HPA class. Default: \"concurrency\" Per-revision concurrency configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"concurrency\" Per-revision rps configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"rps\" Per-revision cpu configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"cpu\"","title":"Setting metrics per revision"},{"location":"serving/autoscaling/autoscaling-metrics/#next-steps","text":"Configure concurrency targets for applications Configure requests per second targets for replicas of an application","title":"Next steps"},{"location":"serving/autoscaling/autoscaling-targets/","text":"Targets \u00b6 Configuring a target provide the Autoscaler with a value that it tries to maintain for the configured metric for a revision. See the metrics documentation for more information about configurable metric types. The target annotation, used to configure per-revision targets, is metric agnostic . This means the target is simply an integer value, which can be applied for any metric type. Configuring targets \u00b6 Global settings key: container-concurrency-target-default . For more information, see the documentation on metrics . Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer (metric agnostic). Default: \"100\" for container-concurrency-target-default . There is no default value set for the target annotation. Target annotation - Per-revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"50\" Concurrency target - Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" Concurrency target - Container Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"Targets"},{"location":"serving/autoscaling/autoscaling-targets/#targets","text":"Configuring a target provide the Autoscaler with a value that it tries to maintain for the configured metric for a revision. See the metrics documentation for more information about configurable metric types. The target annotation, used to configure per-revision targets, is metric agnostic . This means the target is simply an integer value, which can be applied for any metric type.","title":"Targets"},{"location":"serving/autoscaling/autoscaling-targets/#configuring-targets","text":"Global settings key: container-concurrency-target-default . For more information, see the documentation on metrics . Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer (metric agnostic). Default: \"100\" for container-concurrency-target-default . There is no default value set for the target annotation. Target annotation - Per-revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"50\" Concurrency target - Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" Concurrency target - Container Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"Configuring targets"},{"location":"serving/autoscaling/concurrency/","text":"Configuring concurrency \u00b6 Concurrency determines the number of simultaneous requests that can be processed by each replica of an application at any given time. For per-revision concurrency, you must configure both autoscaling.knative.dev/metric and autoscaling.knative.dev/target for a soft limit , or containerConcurrency for a hard limit . For global concurrency, you can set the container-concurrency-target-default value. Soft versus hard concurrency limits \u00b6 It is possible to set either a soft or hard concurrency limit. NOTE: If both a soft and a hard limit are specified, the smaller of the two values will be used. This prevents the Autoscaler from having a target value that is not permitted by the hard limit value. The soft limit is a targeted limit rather than a strictly enforced bound. In some situations, particularly if there is a sudden burst of requests, this value can be exceeded. The hard limit is an enforced upper bound. If concurrency reaches the hard limit, surplus requests will be buffered and must wait until enough capacity is free to execute the requests. IMPORTANT: Using a hard limit configuration is only recommended if there is a clear use case for it with your application. Having a low hard limit specified may have a negative impact on the throughput and latency of an application, and may cause additional cold starts. Soft limit \u00b6 Global key: container-concurrency-target-default Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer. Default: \"100\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"200\" Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\" Hard limit \u00b6 The hard limit is specified per Revision using the containerConcurrency field on the Revision spec. This setting is not an annotation. There is no global setting for the hard limit in the autoscaling ConfigMap, because containerConcurrency has implications outside of autoscaling, such as on buffering and queuing of requests. However, a default value can be set for the Revision's containerConcurrency field in config-defaults.yaml . The default value is 0 , meaning that there is no limit on the number of requests that are allowed to flow into the revision. A value greater than 0 specifies the exact number of requests that are allowed to flow to the replica at any one time. Global key: container-concurrency (in config-defaults.yaml ) Per-revision spec key: containerConcurrency Possible values: integer Default: 0 , meaning no limit Per Revision Example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 50 Global (Defaults ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"50\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : defaults : container-concurrency : \"50\" Target utilization \u00b6 In addition to the literal settings explained previously, concurrency values can be further adjusted by using a target utilization value . This value specifies what percentage of the previously specified target should actually be targeted by the Autoscaler. This is also known as specifying the hotness at which a replica runs, which causes the Autoscaler to scale up before the defined hard limit is reached. For example, if containerConcurrency is set to 10, and the target utilization value is set to 70 (percent), the Autoscaler will create a new replica when the average number of concurrent requests across all existing replicas reaches 7. Requests numbered 7 to 10 will still be sent to the existing replicas, but this allows for additional replicas to be started in anticipation of being needed when the containerConcurrency limit is reached. Global key: container-concurrency-target-percentage Per-revision annotation key: autoscaling.knative.dev/targetUtilizationPercentage Possible values: float Default: 70 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/targetUtilizationPercentage : \"80\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-percentage : \"80\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-percentage : \"80\"","title":"Configuring concurrency"},{"location":"serving/autoscaling/concurrency/#configuring-concurrency","text":"Concurrency determines the number of simultaneous requests that can be processed by each replica of an application at any given time. For per-revision concurrency, you must configure both autoscaling.knative.dev/metric and autoscaling.knative.dev/target for a soft limit , or containerConcurrency for a hard limit . For global concurrency, you can set the container-concurrency-target-default value.","title":"Configuring concurrency"},{"location":"serving/autoscaling/concurrency/#soft-versus-hard-concurrency-limits","text":"It is possible to set either a soft or hard concurrency limit. NOTE: If both a soft and a hard limit are specified, the smaller of the two values will be used. This prevents the Autoscaler from having a target value that is not permitted by the hard limit value. The soft limit is a targeted limit rather than a strictly enforced bound. In some situations, particularly if there is a sudden burst of requests, this value can be exceeded. The hard limit is an enforced upper bound. If concurrency reaches the hard limit, surplus requests will be buffered and must wait until enough capacity is free to execute the requests. IMPORTANT: Using a hard limit configuration is only recommended if there is a clear use case for it with your application. Having a low hard limit specified may have a negative impact on the throughput and latency of an application, and may cause additional cold starts.","title":"Soft versus hard concurrency limits"},{"location":"serving/autoscaling/concurrency/#soft-limit","text":"Global key: container-concurrency-target-default Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer. Default: \"100\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"200\" Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"Soft limit"},{"location":"serving/autoscaling/concurrency/#hard-limit","text":"The hard limit is specified per Revision using the containerConcurrency field on the Revision spec. This setting is not an annotation. There is no global setting for the hard limit in the autoscaling ConfigMap, because containerConcurrency has implications outside of autoscaling, such as on buffering and queuing of requests. However, a default value can be set for the Revision's containerConcurrency field in config-defaults.yaml . The default value is 0 , meaning that there is no limit on the number of requests that are allowed to flow into the revision. A value greater than 0 specifies the exact number of requests that are allowed to flow to the replica at any one time. Global key: container-concurrency (in config-defaults.yaml ) Per-revision spec key: containerConcurrency Possible values: integer Default: 0 , meaning no limit Per Revision Example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 50 Global (Defaults ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"50\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : defaults : container-concurrency : \"50\"","title":"Hard limit"},{"location":"serving/autoscaling/concurrency/#target-utilization","text":"In addition to the literal settings explained previously, concurrency values can be further adjusted by using a target utilization value . This value specifies what percentage of the previously specified target should actually be targeted by the Autoscaler. This is also known as specifying the hotness at which a replica runs, which causes the Autoscaler to scale up before the defined hard limit is reached. For example, if containerConcurrency is set to 10, and the target utilization value is set to 70 (percent), the Autoscaler will create a new replica when the average number of concurrent requests across all existing replicas reaches 7. Requests numbered 7 to 10 will still be sent to the existing replicas, but this allows for additional replicas to be started in anticipation of being needed when the containerConcurrency limit is reached. Global key: container-concurrency-target-percentage Per-revision annotation key: autoscaling.knative.dev/targetUtilizationPercentage Possible values: float Default: 70 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/targetUtilizationPercentage : \"80\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-percentage : \"80\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-percentage : \"80\"","title":"Target utilization"},{"location":"serving/autoscaling/kpa-specific/","text":"Additional autoscaling configuration for Knative Pod Autoscaler \u00b6 The following settings are specific to the Knative Pod Autoscaler (KPA). Modes \u00b6 The KPA acts on metrics ( concurrency or rps ) aggregated over time-based windows. These windows define the amount of historical data that the Autoscaler takes into account, and are used to smooth the data over the specified amount of time. The shorter these windows are, the more quickly the Autoscaler will react. The KPA's implementation has two modes: stable and panic . Stable mode is used for general operation, while panic mode by default has a much shorter window, and will be used to quickly scale a revision up if a burst of traffic arrives. NOTE: When using panic mode, the revision will not scale down to avoid churn. The Autoscaler will leave panic mode if there has been no reason to react quickly during the stable window's timeframe. Stable window \u00b6 Global key: stable-window Per-revision annotation key: autoscaling.knative.dev/window Possible values: Duration, 6s <= value <= 1h Default: 60s NOTE: During scale down, the last replica will only be removed after there has not been any traffic to the revision for the entire duration of the stable window. Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\" Panic window \u00b6 The panic window is defined as a percentage of the stable window to assure that both are relative to each other in a working way. This value indicates how the window over which historical data is evaluated will shrink upon entering panic mode. For example, a value of 10.0 means that in panic mode the window will be 10% of the stable window size. Global key: panic-window-percentage Per-revision annotation key: autoscaling.knative.dev/panicWindowPercentage Possible values: float, 1.0 <= value <= 100.0 Default: 10.0 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panicWindowPercentage : \"20.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-window-percentage : \"20.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-window-percentage : \"20.0\" Panic mode threshold \u00b6 This threshold defines when the Autoscaler will move from stable mode into panic mode. This value is a percentage of the traffic that the current amount of replicas can handle. NOTE: A value of 100.0 (100 percent) means that the Autoscaler is always in panic mode, therefore the minimum value should be higher than 100.0 . The default setting of 200.0 means that panic mode will be start if traffic is twice as high as the current replica population can handle. Global key: panic-threshold-percentage Per-revision annotation key: autoscaling.knative.dev/panicThresholdPercentage Possible values: float, 110.0 <= value <= 1000.0 Default: 200.0 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panicThresholdPercentage : \"150.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-threshold-percentage : \"150.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-threshold-percentage : \"150.0\" Scale rates \u00b6 These settings control by how much the replica population can scale up or down in a single evaluation cycle. A minimal change of one replica in each direction is always permitted, so the Autoscaler can scale to +/- 1 replica at any time, regardless of the scale rates set. Scale up rate \u00b6 This setting determines the maximum ratio of desired to existing pods. For example, with a value of 2.0 , the revision can only scale from N to 2*N pods in one evaluation cycle. Global key: max-scale-up-rate Per-revision annotation key: n/a Possible values: float Default: 1000.0 Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-up-rate : \"500.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-up-rate : \"500.0\" Scale down rate \u00b6 This setting determines the maximum ratio of existing to desired pods. For example, with a value of 2.0 , the revision can only scale from N to N/2 pods in one evaluation cycle. Global key: max-scale-down-rate Per-revision annotation key: n/a Possible values: float Default: 2.0 Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-down-rate : \"4.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-down-rate : \"4.0\"","title":"Additional autoscaling configuration for Knative Pod Autoscaler"},{"location":"serving/autoscaling/kpa-specific/#additional-autoscaling-configuration-for-knative-pod-autoscaler","text":"The following settings are specific to the Knative Pod Autoscaler (KPA).","title":"Additional autoscaling configuration for Knative Pod Autoscaler"},{"location":"serving/autoscaling/kpa-specific/#modes","text":"The KPA acts on metrics ( concurrency or rps ) aggregated over time-based windows. These windows define the amount of historical data that the Autoscaler takes into account, and are used to smooth the data over the specified amount of time. The shorter these windows are, the more quickly the Autoscaler will react. The KPA's implementation has two modes: stable and panic . Stable mode is used for general operation, while panic mode by default has a much shorter window, and will be used to quickly scale a revision up if a burst of traffic arrives. NOTE: When using panic mode, the revision will not scale down to avoid churn. The Autoscaler will leave panic mode if there has been no reason to react quickly during the stable window's timeframe.","title":"Modes"},{"location":"serving/autoscaling/kpa-specific/#stable-window","text":"Global key: stable-window Per-revision annotation key: autoscaling.knative.dev/window Possible values: Duration, 6s <= value <= 1h Default: 60s NOTE: During scale down, the last replica will only be removed after there has not been any traffic to the revision for the entire duration of the stable window. Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"Stable window"},{"location":"serving/autoscaling/kpa-specific/#panic-window","text":"The panic window is defined as a percentage of the stable window to assure that both are relative to each other in a working way. This value indicates how the window over which historical data is evaluated will shrink upon entering panic mode. For example, a value of 10.0 means that in panic mode the window will be 10% of the stable window size. Global key: panic-window-percentage Per-revision annotation key: autoscaling.knative.dev/panicWindowPercentage Possible values: float, 1.0 <= value <= 100.0 Default: 10.0 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panicWindowPercentage : \"20.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-window-percentage : \"20.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-window-percentage : \"20.0\"","title":"Panic window"},{"location":"serving/autoscaling/kpa-specific/#panic-mode-threshold","text":"This threshold defines when the Autoscaler will move from stable mode into panic mode. This value is a percentage of the traffic that the current amount of replicas can handle. NOTE: A value of 100.0 (100 percent) means that the Autoscaler is always in panic mode, therefore the minimum value should be higher than 100.0 . The default setting of 200.0 means that panic mode will be start if traffic is twice as high as the current replica population can handle. Global key: panic-threshold-percentage Per-revision annotation key: autoscaling.knative.dev/panicThresholdPercentage Possible values: float, 110.0 <= value <= 1000.0 Default: 200.0 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panicThresholdPercentage : \"150.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-threshold-percentage : \"150.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-threshold-percentage : \"150.0\"","title":"Panic mode threshold"},{"location":"serving/autoscaling/kpa-specific/#scale-rates","text":"These settings control by how much the replica population can scale up or down in a single evaluation cycle. A minimal change of one replica in each direction is always permitted, so the Autoscaler can scale to +/- 1 replica at any time, regardless of the scale rates set.","title":"Scale rates"},{"location":"serving/autoscaling/kpa-specific/#scale-up-rate","text":"This setting determines the maximum ratio of desired to existing pods. For example, with a value of 2.0 , the revision can only scale from N to 2*N pods in one evaluation cycle. Global key: max-scale-up-rate Per-revision annotation key: n/a Possible values: float Default: 1000.0 Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-up-rate : \"500.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-up-rate : \"500.0\"","title":"Scale up rate"},{"location":"serving/autoscaling/kpa-specific/#scale-down-rate","text":"This setting determines the maximum ratio of existing to desired pods. For example, with a value of 2.0 , the revision can only scale from N to N/2 pods in one evaluation cycle. Global key: max-scale-down-rate Per-revision annotation key: n/a Possible values: float Default: 2.0 Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-down-rate : \"4.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-down-rate : \"4.0\"","title":"Scale down rate"},{"location":"serving/autoscaling/rps-target/","text":"Configuring the requests per second (RPS) target \u00b6 This setting specifies a target for requests-per-second per replica of an application. Global key: requests-per-second-target-default Per-revision annotation key: autoscaling.knative.dev/target (your revision must also be configured to use the rps metric annotation ) Possible values: An integer. Default: \"200\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"150\" autoscaling.knative.dev/metric : \"rps\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"Configuring the requests per second (RPS) target"},{"location":"serving/autoscaling/rps-target/#configuring-the-requests-per-second-rps-target","text":"This setting specifies a target for requests-per-second per replica of an application. Global key: requests-per-second-target-default Per-revision annotation key: autoscaling.knative.dev/target (your revision must also be configured to use the rps metric annotation ) Possible values: An integer. Default: \"200\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"150\" autoscaling.knative.dev/metric : \"rps\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"Configuring the requests per second (RPS) target"},{"location":"serving/autoscaling/scale-bounds/","text":"Configuring scale bounds \u00b6 You can configure upper and lower bounds to control autoscaling behavior. You can also specify the initial scale that a Revision is scaled to immediately after creation. This can be a default configuration for all Revisions, or for a specific Revision using an annotation. Lower bound \u00b6 This value controls the minimum number of replicas that each Revision should have. Knative will attempt to never have less than this number of replicas at any one point in time. Global key: n/a Per-revision annotation key: autoscaling.knative.dev/minScale Possible values: integer Default: 0 if scale-to-zero is enabled and class KPA is used, 1 otherwise NOTE: For more information about scale-to-zero configuration, see the documentation on Configuring scale to zero . Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/minScale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Upper bound \u00b6 This value controls the maximum number of replicas that each revision should have. Knative will attempt to never have more than this number of replicas running, or in the process of being created, at any one point in time. If the max-scale-limit global key is set, Knative ensures that neither the global max scale nor the per-revision max scale for new revisions exceed this value. When max-scale-limit is set to a positive value, a revision with a max scale above that value (including 0, which means unlimited) is disallowed. Global key: max-scale Per-revision annotation key: autoscaling.knative.dev/maxScale Possible values: integer Default: 0 which means unlimited Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/maxScale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale : \"3\" max-scale-limit : \"100\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale : \"3\" max-scale-limit : \"100\" Initial scale \u00b6 This value controls the initial target scale a Revision must reach immediately after it is created before it is marked as Ready . After the Revision has reached this scale one time, this value is ignored. This means that the Revision will scale down after the initial target scale is reached if the actual traffic received only needs a smaller scale. When the Revision is created, the larger of initial scale and lower bound is automatically chosen as the initial target scale. Global key: initial-scale in combination with allow-zero-initial-scale Per-revision annotation key: autoscaling.knative.dev/initialScale Possible values: integer Default: 1 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/initialScale : \"0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : initial-scale : \"0\" allow-zero-initial-scale : \"true\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : initial-scale : \"0\" allow-zero-initial-scale : \"true\" Scale Down Delay \u00b6 Scale Down Delay specifies a time window which must pass at reduced concurrency before a scale-down decision is applied. This can be useful, for example, to keep containers around for a configurable duration to avoid a cold start penalty if new requests come in. Unlike setting a lower bound, the revision will eventually be scaled down if reduced concurrency is maintained for the delay period. Global key: scale-down-delay Per-revision annotation key: autoscaling.knative.dev/scaleDownDelay Possible values: Duration, 0s <= value <= 1h Default: 0s (no delay) Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scaleDownDelay : \"15m\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-down-delay : \"15m\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-down-delay : \"15m\"","title":"Configuring scale bounds"},{"location":"serving/autoscaling/scale-bounds/#configuring-scale-bounds","text":"You can configure upper and lower bounds to control autoscaling behavior. You can also specify the initial scale that a Revision is scaled to immediately after creation. This can be a default configuration for all Revisions, or for a specific Revision using an annotation.","title":"Configuring scale bounds"},{"location":"serving/autoscaling/scale-bounds/#lower-bound","text":"This value controls the minimum number of replicas that each Revision should have. Knative will attempt to never have less than this number of replicas at any one point in time. Global key: n/a Per-revision annotation key: autoscaling.knative.dev/minScale Possible values: integer Default: 0 if scale-to-zero is enabled and class KPA is used, 1 otherwise NOTE: For more information about scale-to-zero configuration, see the documentation on Configuring scale to zero . Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/minScale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go","title":"Lower bound"},{"location":"serving/autoscaling/scale-bounds/#upper-bound","text":"This value controls the maximum number of replicas that each revision should have. Knative will attempt to never have more than this number of replicas running, or in the process of being created, at any one point in time. If the max-scale-limit global key is set, Knative ensures that neither the global max scale nor the per-revision max scale for new revisions exceed this value. When max-scale-limit is set to a positive value, a revision with a max scale above that value (including 0, which means unlimited) is disallowed. Global key: max-scale Per-revision annotation key: autoscaling.knative.dev/maxScale Possible values: integer Default: 0 which means unlimited Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/maxScale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale : \"3\" max-scale-limit : \"100\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale : \"3\" max-scale-limit : \"100\"","title":"Upper bound"},{"location":"serving/autoscaling/scale-bounds/#initial-scale","text":"This value controls the initial target scale a Revision must reach immediately after it is created before it is marked as Ready . After the Revision has reached this scale one time, this value is ignored. This means that the Revision will scale down after the initial target scale is reached if the actual traffic received only needs a smaller scale. When the Revision is created, the larger of initial scale and lower bound is automatically chosen as the initial target scale. Global key: initial-scale in combination with allow-zero-initial-scale Per-revision annotation key: autoscaling.knative.dev/initialScale Possible values: integer Default: 1 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/initialScale : \"0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : initial-scale : \"0\" allow-zero-initial-scale : \"true\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : initial-scale : \"0\" allow-zero-initial-scale : \"true\"","title":"Initial scale"},{"location":"serving/autoscaling/scale-bounds/#scale-down-delay","text":"Scale Down Delay specifies a time window which must pass at reduced concurrency before a scale-down decision is applied. This can be useful, for example, to keep containers around for a configurable duration to avoid a cold start penalty if new requests come in. Unlike setting a lower bound, the revision will eventually be scaled down if reduced concurrency is maintained for the delay period. Global key: scale-down-delay Per-revision annotation key: autoscaling.knative.dev/scaleDownDelay Possible values: Duration, 0s <= value <= 1h Default: 0s (no delay) Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scaleDownDelay : \"15m\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-down-delay : \"15m\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-down-delay : \"15m\"","title":"Scale Down Delay"},{"location":"serving/autoscaling/scale-to-zero/","text":"Configuring scale to zero \u00b6 IMPORTANT: Scale to zero can only be enabled if you are using the Knative Pod Autoscaler (KPA), and can only be configured globally. For more information about using KPA or global configuration, see the documentation on Autoscaling concepts . Enable scale to zero \u00b6 The scale to zero value controls whether Knative allows replicas to scale down to zero (if set to true ), or stop at 1 replica if set to false . NOTE: For more information about scale bounds configuration per revision, see the documentation on Configuring scale bounds . Global key: enable-scale-to-zero Per-revision annotation key: No per-revision setting. Possible values: boolean Default: true Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : enable-scale-to-zero : \"false\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : enable-scale-to-zero : \"false\" Scale to zero grace period \u00b6 This setting specifies an upper bound time limit that the system will wait internally for scale-from-zero machinery to be in place before the last replica is removed. IMPORTANT: This is a value that controls how long internal network programming is allowed to take, and should only be adjusted if you have experienced issues with requests being dropped while a revision was scaling to zero replicas. This setting does not adjust how long the last replica will be kept after traffic ends, and it does not guarantee that the replica will actually be kept for this entire duration. Global key: scale-to-zero-grace-period Per-revision annotation key: n/a Possible values: Duration Default: 30s Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-grace-period : \"40s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-grace-period : \"40s\" Scale to zero last pod retention period \u00b6 The scale-to-zero-pod-retention-period flag determines the minimum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. This contrasts with the scale-to-zero-grace-period flag, which determines the maximum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. Global key: scale-to-zero-pod-retention-period Per-revision annotation key: autoscaling.knative.dev/scaleToZeroPodRetentionPeriod Possible values: Non-negative duration string Default: 0s Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"1m5s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-pod-retention-period : \"42s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-pod-retention-period : \"42s\"","title":"Configuring scale to zero"},{"location":"serving/autoscaling/scale-to-zero/#configuring-scale-to-zero","text":"IMPORTANT: Scale to zero can only be enabled if you are using the Knative Pod Autoscaler (KPA), and can only be configured globally. For more information about using KPA or global configuration, see the documentation on Autoscaling concepts .","title":"Configuring scale to zero"},{"location":"serving/autoscaling/scale-to-zero/#enable-scale-to-zero","text":"The scale to zero value controls whether Knative allows replicas to scale down to zero (if set to true ), or stop at 1 replica if set to false . NOTE: For more information about scale bounds configuration per revision, see the documentation on Configuring scale bounds . Global key: enable-scale-to-zero Per-revision annotation key: No per-revision setting. Possible values: boolean Default: true Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : enable-scale-to-zero : \"false\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : enable-scale-to-zero : \"false\"","title":"Enable scale to zero"},{"location":"serving/autoscaling/scale-to-zero/#scale-to-zero-grace-period","text":"This setting specifies an upper bound time limit that the system will wait internally for scale-from-zero machinery to be in place before the last replica is removed. IMPORTANT: This is a value that controls how long internal network programming is allowed to take, and should only be adjusted if you have experienced issues with requests being dropped while a revision was scaling to zero replicas. This setting does not adjust how long the last replica will be kept after traffic ends, and it does not guarantee that the replica will actually be kept for this entire duration. Global key: scale-to-zero-grace-period Per-revision annotation key: n/a Possible values: Duration Default: 30s Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-grace-period : \"40s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-grace-period : \"40s\"","title":"Scale to zero grace period"},{"location":"serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period","text":"The scale-to-zero-pod-retention-period flag determines the minimum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. This contrasts with the scale-to-zero-grace-period flag, which determines the maximum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. Global key: scale-to-zero-pod-retention-period Per-revision annotation key: autoscaling.knative.dev/scaleToZeroPodRetentionPeriod Possible values: Non-negative duration string Default: 0s Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"1m5s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-pod-retention-period : \"42s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-pod-retention-period : \"42s\"","title":"Scale to zero last pod retention period"},{"location":"serving/autoscaling/autoscale-go/","text":"Autoscale Sample App - Go \u00b6 A demonstration of the autoscaling capabilities of a Knative Serving Revision. Prerequisites \u00b6 A Kubernetes cluster with Knative Serving installed. The hey load generator installed ( go get -u github.com/rakyll/hey ). Clone this repository, and move into the sample directory: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs Deploy the Service \u00b6 Deploy the sample Knative Service: kubectl apply --filename docs/serving/autoscaling/autoscale-go/service.yaml Obtain the URL of the service (once Ready ): $ kubectl get ksvc autoscale-go NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go http://autoscale-go.default.1.2.3.4.xip.io autoscale-go-96dtk autoscale-go-96dtk True Load the Service \u00b6 Make a request to the autoscale app to see it consume some resources. curl \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" Allocated 5 Mb of memory. The largest prime less than 10000 is 9973. Slept for 100.13 milliseconds. Send 30 seconds of traffic maintaining 50 in-flight requests. hey -z 30s -c 50 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" \\ && kubectl get pods Summary: Total: 30 .3379 secs Slowest: 0 .7433 secs Fastest: 0 .1672 secs Average: 0 .2778 secs Requests/sec: 178 .7861 Total data: 542038 bytes Size/request: 99 bytes Response time histogram: 0 .167 [ 1 ] | 0 .225 [ 1462 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .282 [ 1303 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .340 [ 1894 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .398 [ 471 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .455 [ 159 ] | \u25a0\u25a0\u25a0 0 .513 [ 68 ] | \u25a0 0 .570 [ 18 ] | 0 .628 [ 14 ] | 0 .686 [ 21 ] | 0 .743 [ 13 ] | Latency distribution: 10 % in 0 .1805 secs 25 % in 0 .2197 secs 50 % in 0 .2801 secs 75 % in 0 .3129 secs 90 % in 0 .3596 secs 95 % in 0 .4020 secs 99 % in 0 .5457 secs Details ( average, fastest, slowest ) : DNS+dialup: 0 .0007 secs, 0 .1672 secs, 0 .7433 secs DNS-lookup: 0 .0000 secs, 0 .0000 secs, 0 .0000 secs req write: 0 .0001 secs, 0 .0000 secs, 0 .0045 secs resp wait: 0 .2766 secs, 0 .1669 secs, 0 .6633 secs resp read: 0 .0002 secs, 0 .0000 secs, 0 .0065 secs Status code distribution: [ 200 ] 5424 responses NAME READY STATUS RESTARTS AGE autoscale-go-00001-deployment-78cdc67bf4-2w4sk 3 /3 Running 0 26s autoscale-go-00001-deployment-78cdc67bf4-dd2zb 3 /3 Running 0 24s autoscale-go-00001-deployment-78cdc67bf4-pg55p 3 /3 Running 0 18s autoscale-go-00001-deployment-78cdc67bf4-q8bf9 3 /3 Running 0 1m autoscale-go-00001-deployment-78cdc67bf4-thjbq 3 /3 Running 0 26s Analysis \u00b6 Algorithm \u00b6 Knative Serving autoscaling is based on the average number of in-flight requests per pod (concurrency). The system has a default target concurrency of 100 (Search for container-concurrency-target-default) but we used 10 for our service. We loaded the service with 50 concurrent requests so the autoscaler created 5 pods ( 50 concurrent requests / target of 10 = 5 pods ) Panic \u00b6 The autoscaler calculates average concurrency over a 60 second window so it takes a minute for the system to stablize at the desired level of concurrency. However the autoscaler also calculates a 6 second panic window and will enter panic mode if that window reached 2x the target concurrency. In panic mode the autoscaler operates on the shorter, more sensitive panic window. Once the panic conditions are no longer met for 60 seconds, the autoscaler will return to the initial 60 second stable window. | Panic Target---> +--| 20 | | | <------Panic Window | | Stable Target---> +-------------------------|--| 10 CONCURRENCY | | | | <-----------Stable Window | | | --------------------------+-------------------------+--+ 0 120 60 0 TIME Customization \u00b6 The autoscaler supports customization through annotations. There are two autoscaler classes built into Knative: kpa.autoscaling.knative.dev which is the concurrency-based autoscaler described above (the default), and hpa.autoscaling.knative.dev which delegates to the Kubernetes HPA which autoscales on CPU usage. Example of a Service scaled on CPU: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Standard Kubernetes CPU-based autoscaling. autoscaling.knative.dev/class : hpa.autoscaling.knative.dev autoscaling.knative.dev/metric : cpu spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Additionally the autoscaler targets and scaling bounds can be specified in annotations. Example of a Service with custom targets and scale bounds: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Knative concurrency-based autoscaling (default). autoscaling.knative.dev/class : kpa.autoscaling.knative.dev autoscaling.knative.dev/metric : concurrency # Target 10 requests in-flight per pod. autoscaling.knative.dev/target : \"10\" # Disable scale to zero with a minScale of 1. autoscaling.knative.dev/minScale : \"1\" # Limit scaling to 100 pods. autoscaling.knative.dev/maxScale : \"100\" spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Note: for an hpa.autoscaling.knative.dev class service, the autoscaling.knative.dev/target specifies the CPU percentage target (default \"80\" ). Demo \u00b6 View the Kubecon Demo of Knative autoscaler customization (32 minutes). Other Experiments \u00b6 Send 60 seconds of traffic maintaining 100 concurrent requests. hey -z 60s -c 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" Send 60 seconds of traffic maintaining 100 qps with short requests (10 ms). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=10\" Send 60 seconds of traffic maintaining 100 qps with long requests (1 sec). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=1000\" Send 60 seconds of traffic with heavy CPU usage (~1 cpu/sec/request, total 100 cpus). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?prime=40000000\" Send 60 seconds of traffic with heavy memory usage (1 gb/request, total 5 gb). hey -z 60s -c 5 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?bloat=1000\" Cleanup \u00b6 kubectl delete --filename docs/serving/autoscaling/autoscale-go/service.yaml Further reading \u00b6 Autoscaling Developer Documentation","title":"Autoscale Sample App - Go"},{"location":"serving/autoscaling/autoscale-go/#autoscale-sample-app-go","text":"A demonstration of the autoscaling capabilities of a Knative Serving Revision.","title":"Autoscale Sample App - Go"},{"location":"serving/autoscaling/autoscale-go/#prerequisites","text":"A Kubernetes cluster with Knative Serving installed. The hey load generator installed ( go get -u github.com/rakyll/hey ). Clone this repository, and move into the sample directory: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs","title":"Prerequisites"},{"location":"serving/autoscaling/autoscale-go/#deploy-the-service","text":"Deploy the sample Knative Service: kubectl apply --filename docs/serving/autoscaling/autoscale-go/service.yaml Obtain the URL of the service (once Ready ): $ kubectl get ksvc autoscale-go NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go http://autoscale-go.default.1.2.3.4.xip.io autoscale-go-96dtk autoscale-go-96dtk True","title":"Deploy the Service"},{"location":"serving/autoscaling/autoscale-go/#load-the-service","text":"Make a request to the autoscale app to see it consume some resources. curl \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" Allocated 5 Mb of memory. The largest prime less than 10000 is 9973. Slept for 100.13 milliseconds. Send 30 seconds of traffic maintaining 50 in-flight requests. hey -z 30s -c 50 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" \\ && kubectl get pods Summary: Total: 30 .3379 secs Slowest: 0 .7433 secs Fastest: 0 .1672 secs Average: 0 .2778 secs Requests/sec: 178 .7861 Total data: 542038 bytes Size/request: 99 bytes Response time histogram: 0 .167 [ 1 ] | 0 .225 [ 1462 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .282 [ 1303 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .340 [ 1894 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .398 [ 471 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .455 [ 159 ] | \u25a0\u25a0\u25a0 0 .513 [ 68 ] | \u25a0 0 .570 [ 18 ] | 0 .628 [ 14 ] | 0 .686 [ 21 ] | 0 .743 [ 13 ] | Latency distribution: 10 % in 0 .1805 secs 25 % in 0 .2197 secs 50 % in 0 .2801 secs 75 % in 0 .3129 secs 90 % in 0 .3596 secs 95 % in 0 .4020 secs 99 % in 0 .5457 secs Details ( average, fastest, slowest ) : DNS+dialup: 0 .0007 secs, 0 .1672 secs, 0 .7433 secs DNS-lookup: 0 .0000 secs, 0 .0000 secs, 0 .0000 secs req write: 0 .0001 secs, 0 .0000 secs, 0 .0045 secs resp wait: 0 .2766 secs, 0 .1669 secs, 0 .6633 secs resp read: 0 .0002 secs, 0 .0000 secs, 0 .0065 secs Status code distribution: [ 200 ] 5424 responses NAME READY STATUS RESTARTS AGE autoscale-go-00001-deployment-78cdc67bf4-2w4sk 3 /3 Running 0 26s autoscale-go-00001-deployment-78cdc67bf4-dd2zb 3 /3 Running 0 24s autoscale-go-00001-deployment-78cdc67bf4-pg55p 3 /3 Running 0 18s autoscale-go-00001-deployment-78cdc67bf4-q8bf9 3 /3 Running 0 1m autoscale-go-00001-deployment-78cdc67bf4-thjbq 3 /3 Running 0 26s","title":"Load the Service"},{"location":"serving/autoscaling/autoscale-go/#analysis","text":"","title":"Analysis"},{"location":"serving/autoscaling/autoscale-go/#algorithm","text":"Knative Serving autoscaling is based on the average number of in-flight requests per pod (concurrency). The system has a default target concurrency of 100 (Search for container-concurrency-target-default) but we used 10 for our service. We loaded the service with 50 concurrent requests so the autoscaler created 5 pods ( 50 concurrent requests / target of 10 = 5 pods )","title":"Algorithm"},{"location":"serving/autoscaling/autoscale-go/#panic","text":"The autoscaler calculates average concurrency over a 60 second window so it takes a minute for the system to stablize at the desired level of concurrency. However the autoscaler also calculates a 6 second panic window and will enter panic mode if that window reached 2x the target concurrency. In panic mode the autoscaler operates on the shorter, more sensitive panic window. Once the panic conditions are no longer met for 60 seconds, the autoscaler will return to the initial 60 second stable window. | Panic Target---> +--| 20 | | | <------Panic Window | | Stable Target---> +-------------------------|--| 10 CONCURRENCY | | | | <-----------Stable Window | | | --------------------------+-------------------------+--+ 0 120 60 0 TIME","title":"Panic"},{"location":"serving/autoscaling/autoscale-go/#customization","text":"The autoscaler supports customization through annotations. There are two autoscaler classes built into Knative: kpa.autoscaling.knative.dev which is the concurrency-based autoscaler described above (the default), and hpa.autoscaling.knative.dev which delegates to the Kubernetes HPA which autoscales on CPU usage. Example of a Service scaled on CPU: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Standard Kubernetes CPU-based autoscaling. autoscaling.knative.dev/class : hpa.autoscaling.knative.dev autoscaling.knative.dev/metric : cpu spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Additionally the autoscaler targets and scaling bounds can be specified in annotations. Example of a Service with custom targets and scale bounds: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Knative concurrency-based autoscaling (default). autoscaling.knative.dev/class : kpa.autoscaling.knative.dev autoscaling.knative.dev/metric : concurrency # Target 10 requests in-flight per pod. autoscaling.knative.dev/target : \"10\" # Disable scale to zero with a minScale of 1. autoscaling.knative.dev/minScale : \"1\" # Limit scaling to 100 pods. autoscaling.knative.dev/maxScale : \"100\" spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Note: for an hpa.autoscaling.knative.dev class service, the autoscaling.knative.dev/target specifies the CPU percentage target (default \"80\" ).","title":"Customization"},{"location":"serving/autoscaling/autoscale-go/#demo","text":"View the Kubecon Demo of Knative autoscaler customization (32 minutes).","title":"Demo"},{"location":"serving/autoscaling/autoscale-go/#other-experiments","text":"Send 60 seconds of traffic maintaining 100 concurrent requests. hey -z 60s -c 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" Send 60 seconds of traffic maintaining 100 qps with short requests (10 ms). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=10\" Send 60 seconds of traffic maintaining 100 qps with long requests (1 sec). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=1000\" Send 60 seconds of traffic with heavy CPU usage (~1 cpu/sec/request, total 100 cpus). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?prime=40000000\" Send 60 seconds of traffic with heavy memory usage (1 gb/request, total 5 gb). hey -z 60s -c 5 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?bloat=1000\"","title":"Other Experiments"},{"location":"serving/autoscaling/autoscale-go/#cleanup","text":"kubectl delete --filename docs/serving/autoscaling/autoscale-go/service.yaml","title":"Cleanup"},{"location":"serving/autoscaling/autoscale-go/#further-reading","text":"Autoscaling Developer Documentation","title":"Further reading"},{"location":"serving/deploying/","text":"Deploying images from a private container registry \u00b6 Learn how to configure your Knative cluster to deploy images from a private container registry. To share access to your private container images across multiple services and revisions, you create a list of Kubernetes secrets ( imagePullSecrets ) using your registry credentials, add that imagePullSecrets to your default service account , and then deploy those configurations to your Knative cluster. Before you begin \u00b6 You need: A Kubernetes cluster with Knative Serving installed . The credentials to the private container registry where your container images are stored. Configuring your credentials in Knative \u00b6 Create a imagePullSecrets that contains your credentials as a list of secrets: kubectl create secret docker-registry [ REGISTRY-CRED-SECRETS ] \\ --docker-server =[ PRIVATE_REGISTRY_SERVER_URL ] \\ --docker-email =[ PRIVATE_REGISTRY_EMAIL ] \\ --docker-username =[ PRIVATE_REGISTRY_USER ] \\ --docker-password =[ PRIVATE_REGISTRY_PASSWORD ] Where - [REGISTRY-CRED-SECRETS] is the name that you want for your secrets ( imagePullSecrets object). For example, container-registry . [PRIVATE_REGISTRY_SERVER_URL] is the URL to the private registry where your container images are stored. Examples: - Google Container Registry: https://gcr.io/ - DockerHub https://docker.io/ [PRIVATE_REGISTRY_EMAIL] is your email address that is associated with the private registry. [PRIVATE_REGISTRY_USER] is the username that you use to access the private container registry. [PRIVATE_REGISTRY_PASSWORD] is the password that you use to access the private container registry. Example: kubectl create secret ` container-registry ` \\ --docker-server = https://gcr.io/ \\ --docker-email = my-account-email@address.com \\ --docker-username = my-grc-username \\ --docker-password = my-gcr-password Tip: After creating the imagePullSecrets , you can view those secret's by running: kubectl get secret [ REGISTRY-CRED-SECRETS ] --output = yaml Add the imagePullSecrets to your default service account in the default namespace. Note: By default, the default service account in each of the namespaces of your Knative cluster are use by your revisions unless serviceAccountName is specified. Run the following command to modify your default service account, assuming you named your secrets container-registry : ```shell kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\" ``` Now, all the new pods that are created in the default namespace will include your credentials and have access to your container images in the private registry. What's next \u00b6 You can now create a service that uses your container images from the private registry. Learn how to create a Knative service .","title":"Deploying from private registries"},{"location":"serving/deploying/#deploying-images-from-a-private-container-registry","text":"Learn how to configure your Knative cluster to deploy images from a private container registry. To share access to your private container images across multiple services and revisions, you create a list of Kubernetes secrets ( imagePullSecrets ) using your registry credentials, add that imagePullSecrets to your default service account , and then deploy those configurations to your Knative cluster.","title":"Deploying images from a private container registry"},{"location":"serving/deploying/#before-you-begin","text":"You need: A Kubernetes cluster with Knative Serving installed . The credentials to the private container registry where your container images are stored.","title":"Before you begin"},{"location":"serving/deploying/#configuring-your-credentials-in-knative","text":"Create a imagePullSecrets that contains your credentials as a list of secrets: kubectl create secret docker-registry [ REGISTRY-CRED-SECRETS ] \\ --docker-server =[ PRIVATE_REGISTRY_SERVER_URL ] \\ --docker-email =[ PRIVATE_REGISTRY_EMAIL ] \\ --docker-username =[ PRIVATE_REGISTRY_USER ] \\ --docker-password =[ PRIVATE_REGISTRY_PASSWORD ] Where - [REGISTRY-CRED-SECRETS] is the name that you want for your secrets ( imagePullSecrets object). For example, container-registry . [PRIVATE_REGISTRY_SERVER_URL] is the URL to the private registry where your container images are stored. Examples: - Google Container Registry: https://gcr.io/ - DockerHub https://docker.io/ [PRIVATE_REGISTRY_EMAIL] is your email address that is associated with the private registry. [PRIVATE_REGISTRY_USER] is the username that you use to access the private container registry. [PRIVATE_REGISTRY_PASSWORD] is the password that you use to access the private container registry. Example: kubectl create secret ` container-registry ` \\ --docker-server = https://gcr.io/ \\ --docker-email = my-account-email@address.com \\ --docker-username = my-grc-username \\ --docker-password = my-gcr-password Tip: After creating the imagePullSecrets , you can view those secret's by running: kubectl get secret [ REGISTRY-CRED-SECRETS ] --output = yaml Add the imagePullSecrets to your default service account in the default namespace. Note: By default, the default service account in each of the namespaces of your Knative cluster are use by your revisions unless serviceAccountName is specified. Run the following command to modify your default service account, assuming you named your secrets container-registry : ```shell kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\" ``` Now, all the new pods that are created in the default namespace will include your credentials and have access to your container images in the private registry.","title":"Configuring your credentials in Knative"},{"location":"serving/deploying/#whats-next","text":"You can now create a service that uses your container images from the private registry. Learn how to create a Knative service .","title":"What's next"},{"location":"serving/load-balancing/","text":"Load balancing \u00b6 You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. NOTE: To do this, you must first ensure that individual pod addressability is enabled. Activator pod selection \u00b6 Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally. Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision. The Activator load balancing algorithm works as follows: If concurrency is unlimited, the request is sent to the better of two random choices. If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency. For more information, see the documentation on concurrency . Configuring target burst capacity \u00b6 Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the autoscaling.knative.dev/targetBurstCapacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity .","title":"Overview"},{"location":"serving/load-balancing/#load-balancing","text":"You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. NOTE: To do this, you must first ensure that individual pod addressability is enabled.","title":"Load balancing"},{"location":"serving/load-balancing/#activator-pod-selection","text":"Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally. Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision. The Activator load balancing algorithm works as follows: If concurrency is unlimited, the request is sent to the better of two random choices. If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency. For more information, see the documentation on concurrency .","title":"Activator pod selection"},{"location":"serving/load-balancing/#configuring-target-burst-capacity","text":"Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the autoscaling.knative.dev/targetBurstCapacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity .","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/target-burst-capacity/","text":"Configuring target burst capacity \u00b6 Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing. The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the autoscaling.knative.dev/targetBurstCapacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the target burst capacity \u00b6 Global key: target-burst-capacity Per-revision annotation key: autoscaling.knative.dev/targetBurstCapacity Possible values: float ( 0 means the Activator is only in path when scaled to 0, -1 means the Activator is always in path) Default: 200 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : name : <service_name> namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/targetBurstCapacity : \"200\" Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : target-burst-capacity : \"200\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : target-burst-capacity : \"200\" If autoscaling.knative.dev/targetBurstCapacity is set to 0 , the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied. NOTE: Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation. If autoscaling.knative.dev/targetBurstCapacity is set to -1 , the Activator is always in the request path, regardless of the revision size. If autoscaling.knative.dev/targetBurstCapacity is set to another integer, the Activator may be in the path, depending on the revision scale and load.","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/target-burst-capacity/#configuring-target-burst-capacity","text":"Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing. The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the autoscaling.knative.dev/targetBurstCapacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity .","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/target-burst-capacity/#setting-the-target-burst-capacity","text":"Global key: target-burst-capacity Per-revision annotation key: autoscaling.knative.dev/targetBurstCapacity Possible values: float ( 0 means the Activator is only in path when scaled to 0, -1 means the Activator is always in path) Default: 200 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : name : <service_name> namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/targetBurstCapacity : \"200\" Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : target-burst-capacity : \"200\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : target-burst-capacity : \"200\" If autoscaling.knative.dev/targetBurstCapacity is set to 0 , the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied. NOTE: Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation. If autoscaling.knative.dev/targetBurstCapacity is set to -1 , the Activator is always in the request path, regardless of the revision size. If autoscaling.knative.dev/targetBurstCapacity is set to another integer, the Activator may be in the path, depending on the revision scale and load.","title":"Setting the target burst capacity"},{"location":"serving/samples/","text":"Knative Serving code samples \u00b6 Use the following code samples to help you understand the various Knative Serving resources and how they can be applied across common use cases. Learn more about Knative Serving resources . See all Knative code samples Name Description Languages Hello World A quick introduction that highlights how to deploy an app using Knative Serving. C# , Go , Java (Spark) , Java (Spring) , Kotlin , Node.js , PHP , Python , Ruby , Scala , Shell Cloud Events A quick introduction that highlights how to send and receive Cloud Events. C# , Go , Node.js , Rust , Java (Vert.x) Advanced Deployment Simple blue/green-like application deployment pattern illustrating the process of updating a live application without dropping any traffic. YAML Autoscale A demonstration of the autoscaling capabilities of Knative. Go Github Webhook A simple webhook handler that demonstrates interacting with Github. Go gRPC A simple gRPC server. Go Knative Routing An example of mapping multiple Knative services to different paths under a single domain name using the Istio VirtualService concept. Go Knative Secrets A simple app that demonstrates how to use a Kubernetes secret as a Volume in Knative. Go REST API A simple Restful service that exposes an endpoint defined by an environment variable described in the Knative Configuration. Go Traffic Splitting This samples builds off the Creating a RESTful Service sample to illustrate applying a revision, then using that revision for manual traffic splitting. YAML Multi Container A quick introduction that highlights how to build and deploy an app using Knative Serving for multiple containers. Go","title":"Overview"},{"location":"serving/samples/#knative-serving-code-samples","text":"Use the following code samples to help you understand the various Knative Serving resources and how they can be applied across common use cases. Learn more about Knative Serving resources . See all Knative code samples Name Description Languages Hello World A quick introduction that highlights how to deploy an app using Knative Serving. C# , Go , Java (Spark) , Java (Spring) , Kotlin , Node.js , PHP , Python , Ruby , Scala , Shell Cloud Events A quick introduction that highlights how to send and receive Cloud Events. C# , Go , Node.js , Rust , Java (Vert.x) Advanced Deployment Simple blue/green-like application deployment pattern illustrating the process of updating a live application without dropping any traffic. YAML Autoscale A demonstration of the autoscaling capabilities of Knative. Go Github Webhook A simple webhook handler that demonstrates interacting with Github. Go gRPC A simple gRPC server. Go Knative Routing An example of mapping multiple Knative services to different paths under a single domain name using the Istio VirtualService concept. Go Knative Secrets A simple app that demonstrates how to use a Kubernetes secret as a Volume in Knative. Go REST API A simple Restful service that exposes an endpoint defined by an environment variable described in the Knative Configuration. Go Traffic Splitting This samples builds off the Creating a RESTful Service sample to illustrate applying a revision, then using that revision for manual traffic splitting. YAML Multi Container A quick introduction that highlights how to build and deploy an app using Knative Serving for multiple containers. Go","title":"Knative Serving code samples"},{"location":"serving/samples/blue-green-deployment/","text":"Routing and managing traffic with blue/green deployment \u00b6 This sample demonstrates updating an application to a new version using a blue/green traffic routing pattern. With Knative, you can safely reroute traffic from a live version of an application to a new version by changing the routing configuration. Before you begin \u00b6 You need: A Kubernetes cluster with Knative installed . (Optional) A custom domain configured for use with Knative. Note: The source code for the gcr.io/knative-samples/knative-route-demo image that is used in this sample, is located at https://github.com/mchmarny/knative-route-demo. Deploying Revision 1 (Blue) \u00b6 We'll be deploying an image of a sample application that displays the text \"App v1\" on a blue background. First, create a new file called blue-green-demo-config.yaml and copy this into it: apiVersion : serving.knative.dev/v1 kind : Configuration metadata : name : blue-green-demo namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue # The URL to the sample app docker image env : - name : T_VERSION value : \"blue\" Save the file, then deploy the configuration to your cluster: kubectl apply --filename blue-green-demo-config.yaml configuration \"blue-green-demo\" configured This will deploy the initial revision of the sample application. Before we can route traffic to this application we need to know the name of the initial revision which was just created. Using kubectl you can get it with the following command: kubectl get configurations blue-green-demo -o = jsonpath = '{.status.latestCreatedRevisionName}' The command above will return the name of the revision, it will be similar to blue-green-demo-lcfrd . In the rest of this document we will use this revision name, but yours will be different. To route inbound traffic to it, we need to define a route. Create a new file called blue-green-demo-route.yaml and copy the following YAML manifest into it (do not forget to edit the revision name): apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # The name of our route; appears in the URL to access the app namespace : default # The namespace we're working in; also appears in the URL to access the app spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 100 # All traffic goes to this revision Save the file, then apply the route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured You'll now be able to view the sample app at the URL shown by: kubectl get route blue-green-demo Deploying Revision 2 (Green) \u00b6 Revision 2 of the sample application will display the text \"App v2\" on a green background. To create the new revision, we'll edit our existing configuration in blue-green-demo-config.yaml with an updated image and environment variables: apiVersion : serving.knative.dev/v1 kind : Configuration metadata : name : blue-green-demo # Configuration name is unchanged, since we're updating an existing Configuration namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green # URL to the new version of the sample app docker image env : - name : T_VERSION value : \"green\" # Updated value for the T_VERSION environment variable Save the file, then apply the updated configuration to your cluster: kubectl apply --filename blue-green-demo-config.yaml configuration \"blue-green-demo\" configured Find the name of the second revision with the following command: kubectl get configurations blue-green-demo -o = jsonpath = '{.status.latestCreatedRevisionName}' In the rest of this document we will assume that the second revision is called blue-green-demo-m9548 , however yours will differ. Make sure to use the correct name of the second revision in the manifests that follow. At this point, the first revision ( blue-green-demo-lcfrd ) and the second revision ( blue-green-demo-m9548 ) will both be deployed and running. We can update our existing route to create a new (test) endpoint for the second revision while still sending all other traffic to the first revision. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Route name is unchanged, since we're updating an existing Route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 100 # All traffic still going to the first revision - revisionName : blue-green-demo-m9548 percent : 0 # 0% of traffic routed to the second revision tag : v2 # A named route Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Revision 2 of the app is staged at this point. That means: No traffic will be routed to revision 2 at the main URL, http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com Knative creates a new route named v2 for testing the newly deployed version. The URL of this can be seen in the status section of your Route. kubectl get route blue-green-demo --output jsonpath = \"{.status.traffic[*].url}\" This allows you to validate that the new version of the app is behaving as expected before switching any traffic over to it. Migrating traffic to the new revision \u00b6 We'll once again update our existing route to begin shifting traffic away from the first revision and toward the second. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Updating our existing route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 50 # Updating the percentage from 100 to 50 - revisionName : blue-green-demo-m9548 percent : 50 # Updating the percentage from 0 to 50 tag : v2 Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Refresh the original route ( http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com ) a few times to see that some traffic now goes to version 2 of the app. Note: This sample shows a 50/50 split to assure you don't have to refresh too much, but it's recommended to start with 1-2% of traffic in a production environment Rerouting all traffic to the new version \u00b6 Lastly, we'll update our existing route to finally shift all traffic to the second revision. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Updating our existing route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 0 tag : v1 # Adding a new named route for v1 - revisionName : blue-green-demo-m9548 percent : 100 # Named route for v2 has been removed, since we don't need it anymore Note: You can remove the first revision blue-green-demo-lcfrd instead of 0% of traffic when you will not roll back the revision anymore. Then the non-routeable revision object will be garbage collected. Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Refresh the original route ( http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com ) a few times to verify that no traffic is being routed to v1 of the app. We added a named route to v1 of the app, so you can now access it at the URL listed in the traffic block of the status section. To get the URL, enter the following command: kubectl get route blue-green-demo --output jsonpath = \"{.status.traffic[*].url}\" With all inbound traffic being directed to the second revision of the application, Knative will soon scale the first revision down to 0 running pods and the blue/green deployment can be considered complete. Using the named v1 route will reactivate a pod to serve any occasional requests intended specifically for the initial revision. Cleaning up \u00b6 To delete the sample app, enter the following commands: kubectl delete route blue-green-demo kubectl delete configuration blue-green-demo","title":"Routing and managing traffic"},{"location":"serving/samples/blue-green-deployment/#routing-and-managing-traffic-with-bluegreen-deployment","text":"This sample demonstrates updating an application to a new version using a blue/green traffic routing pattern. With Knative, you can safely reroute traffic from a live version of an application to a new version by changing the routing configuration.","title":"Routing and managing traffic with blue/green deployment"},{"location":"serving/samples/blue-green-deployment/#before-you-begin","text":"You need: A Kubernetes cluster with Knative installed . (Optional) A custom domain configured for use with Knative. Note: The source code for the gcr.io/knative-samples/knative-route-demo image that is used in this sample, is located at https://github.com/mchmarny/knative-route-demo.","title":"Before you begin"},{"location":"serving/samples/blue-green-deployment/#deploying-revision-1-blue","text":"We'll be deploying an image of a sample application that displays the text \"App v1\" on a blue background. First, create a new file called blue-green-demo-config.yaml and copy this into it: apiVersion : serving.knative.dev/v1 kind : Configuration metadata : name : blue-green-demo namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue # The URL to the sample app docker image env : - name : T_VERSION value : \"blue\" Save the file, then deploy the configuration to your cluster: kubectl apply --filename blue-green-demo-config.yaml configuration \"blue-green-demo\" configured This will deploy the initial revision of the sample application. Before we can route traffic to this application we need to know the name of the initial revision which was just created. Using kubectl you can get it with the following command: kubectl get configurations blue-green-demo -o = jsonpath = '{.status.latestCreatedRevisionName}' The command above will return the name of the revision, it will be similar to blue-green-demo-lcfrd . In the rest of this document we will use this revision name, but yours will be different. To route inbound traffic to it, we need to define a route. Create a new file called blue-green-demo-route.yaml and copy the following YAML manifest into it (do not forget to edit the revision name): apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # The name of our route; appears in the URL to access the app namespace : default # The namespace we're working in; also appears in the URL to access the app spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 100 # All traffic goes to this revision Save the file, then apply the route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured You'll now be able to view the sample app at the URL shown by: kubectl get route blue-green-demo","title":"Deploying Revision 1 (Blue)"},{"location":"serving/samples/blue-green-deployment/#deploying-revision-2-green","text":"Revision 2 of the sample application will display the text \"App v2\" on a green background. To create the new revision, we'll edit our existing configuration in blue-green-demo-config.yaml with an updated image and environment variables: apiVersion : serving.knative.dev/v1 kind : Configuration metadata : name : blue-green-demo # Configuration name is unchanged, since we're updating an existing Configuration namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green # URL to the new version of the sample app docker image env : - name : T_VERSION value : \"green\" # Updated value for the T_VERSION environment variable Save the file, then apply the updated configuration to your cluster: kubectl apply --filename blue-green-demo-config.yaml configuration \"blue-green-demo\" configured Find the name of the second revision with the following command: kubectl get configurations blue-green-demo -o = jsonpath = '{.status.latestCreatedRevisionName}' In the rest of this document we will assume that the second revision is called blue-green-demo-m9548 , however yours will differ. Make sure to use the correct name of the second revision in the manifests that follow. At this point, the first revision ( blue-green-demo-lcfrd ) and the second revision ( blue-green-demo-m9548 ) will both be deployed and running. We can update our existing route to create a new (test) endpoint for the second revision while still sending all other traffic to the first revision. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Route name is unchanged, since we're updating an existing Route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 100 # All traffic still going to the first revision - revisionName : blue-green-demo-m9548 percent : 0 # 0% of traffic routed to the second revision tag : v2 # A named route Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Revision 2 of the app is staged at this point. That means: No traffic will be routed to revision 2 at the main URL, http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com Knative creates a new route named v2 for testing the newly deployed version. The URL of this can be seen in the status section of your Route. kubectl get route blue-green-demo --output jsonpath = \"{.status.traffic[*].url}\" This allows you to validate that the new version of the app is behaving as expected before switching any traffic over to it.","title":"Deploying Revision 2 (Green)"},{"location":"serving/samples/blue-green-deployment/#migrating-traffic-to-the-new-revision","text":"We'll once again update our existing route to begin shifting traffic away from the first revision and toward the second. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Updating our existing route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 50 # Updating the percentage from 100 to 50 - revisionName : blue-green-demo-m9548 percent : 50 # Updating the percentage from 0 to 50 tag : v2 Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Refresh the original route ( http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com ) a few times to see that some traffic now goes to version 2 of the app. Note: This sample shows a 50/50 split to assure you don't have to refresh too much, but it's recommended to start with 1-2% of traffic in a production environment","title":"Migrating traffic to the new revision"},{"location":"serving/samples/blue-green-deployment/#rerouting-all-traffic-to-the-new-version","text":"Lastly, we'll update our existing route to finally shift all traffic to the second revision. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Updating our existing route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 0 tag : v1 # Adding a new named route for v1 - revisionName : blue-green-demo-m9548 percent : 100 # Named route for v2 has been removed, since we don't need it anymore Note: You can remove the first revision blue-green-demo-lcfrd instead of 0% of traffic when you will not roll back the revision anymore. Then the non-routeable revision object will be garbage collected. Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Refresh the original route ( http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com ) a few times to verify that no traffic is being routed to v1 of the app. We added a named route to v1 of the app, so you can now access it at the URL listed in the traffic block of the status section. To get the URL, enter the following command: kubectl get route blue-green-demo --output jsonpath = \"{.status.traffic[*].url}\" With all inbound traffic being directed to the second revision of the application, Knative will soon scale the first revision down to 0 running pods and the blue/green deployment can be considered complete. Using the named v1 route will reactivate a pod to serve any occasional requests intended specifically for the initial revision.","title":"Rerouting all traffic to the new version"},{"location":"serving/samples/blue-green-deployment/#cleaning-up","text":"To delete the sample app, enter the following commands: kubectl delete route blue-green-demo kubectl delete configuration blue-green-demo","title":"Cleaning up"},{"location":"serving/samples/cloudevents/","text":"","title":"Overview"},{"location":"serving/samples/cloudevents/cloudevents-dotnet/","text":"Cloud Events - .NET Core \u00b6 A simple web app written in ASP.NET and C# that can receive and send Cloud Events that you can use for testing. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source) The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-dotnet Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). The sample code \u00b6 If you look in controllers\\CloudEventsController.cs , you will see two key functions for the different modes of operation: private async Task < IActionResult > ReceiveAndSend ( CloudEvent receivedEvent ) { // This is called whenever an event is received if $K_SINK is set, and sends a new event // to the url in $K_SINK. } private IActionResult ReceiveAndReply ( CloudEvent receivedEvent ) { // This is called whenever an event is received if $K_SINK is NOT set, and it replies with // the new event instead. } If you look in Dockerfile , you will see a method for pulling in the dependencies and building an ASP.NET container based on Alpine. You can build and push this to your registry of choice via: docker build -t <image> . docker push <image> If you look in service.yaml , take the <image> name above and insert it into the image: field. kubectl apply -f service.yaml Testing the sample \u00b6 Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-dotnet http://cloudevents-dotnet... cloudevents-dotnet-ss5pj cloudevents-dotnet-ss5pj True Then send a cloud event to it with: $ curl -X POST \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ <URL from kubectl command above> You will get back: { \"specversion\" : \"1.0\" , \"type\" : \"dev.knative.docs.sample\" , \"source\" : \"https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-dotnet\" , \"id\" : \"d662b6f6-35ff-4b98-bffd-5ae9eee23dab\" , \"time\" : \"2020-05-19T01:26:23.3500138Z\" , \"datacontenttype\" : \"application/json\" , \"data\" : { \"message\" : \"Hello, Dave\" } } Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":".NET"},{"location":"serving/samples/cloudevents/cloudevents-dotnet/#cloud-events-net-core","text":"A simple web app written in ASP.NET and C# that can receive and send Cloud Events that you can use for testing. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source) The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-dotnet","title":"Cloud Events - .NET Core"},{"location":"serving/samples/cloudevents/cloudevents-dotnet/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"serving/samples/cloudevents/cloudevents-dotnet/#the-sample-code","text":"If you look in controllers\\CloudEventsController.cs , you will see two key functions for the different modes of operation: private async Task < IActionResult > ReceiveAndSend ( CloudEvent receivedEvent ) { // This is called whenever an event is received if $K_SINK is set, and sends a new event // to the url in $K_SINK. } private IActionResult ReceiveAndReply ( CloudEvent receivedEvent ) { // This is called whenever an event is received if $K_SINK is NOT set, and it replies with // the new event instead. } If you look in Dockerfile , you will see a method for pulling in the dependencies and building an ASP.NET container based on Alpine. You can build and push this to your registry of choice via: docker build -t <image> . docker push <image> If you look in service.yaml , take the <image> name above and insert it into the image: field. kubectl apply -f service.yaml","title":"The sample code"},{"location":"serving/samples/cloudevents/cloudevents-dotnet/#testing-the-sample","text":"Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-dotnet http://cloudevents-dotnet... cloudevents-dotnet-ss5pj cloudevents-dotnet-ss5pj True Then send a cloud event to it with: $ curl -X POST \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ <URL from kubectl command above> You will get back: { \"specversion\" : \"1.0\" , \"type\" : \"dev.knative.docs.sample\" , \"source\" : \"https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-dotnet\" , \"id\" : \"d662b6f6-35ff-4b98-bffd-5ae9eee23dab\" , \"time\" : \"2020-05-19T01:26:23.3500138Z\" , \"datacontenttype\" : \"application/json\" , \"data\" : { \"message\" : \"Hello, Dave\" } }","title":"Testing the sample"},{"location":"serving/samples/cloudevents/cloudevents-dotnet/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Removing the sample app deployment"},{"location":"serving/samples/cloudevents/cloudevents-go/","text":"Cloud Events - Go \u00b6 A simple web app written in Go that can receive and send Cloud Events that you can use for testing. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source) The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-go Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). The sample code. \u00b6 If you look in cloudevents.go , you will see two key functions for the different modes of operation: func ( recv * Receiver ) ReceiveAndSend ( ctx context . Context , event cloudevents . Event ) cloudevents . Result { // This is called whenever an event is received if $K_SINK is set, and sends a new event // to the url in $K_SINK. } func ( recv * Receiver ) ReceiveAndReply ( ctx context . Context , event cloudevents . Event ) ( * cloudevents . Event , cloudevents . Result ) { // This is called whenever an event is received if $K_SINK is NOT set, and it replies with // the new event instead. } Choose how you would like to build the application: Dockerfile If you look in Dockerfile , you will see a method for pulling in the dependencies and building a small Go container based on Alpine. You can build and push this to your registry of choice via: docker build -t <image> . docker push <image> ko You can use ko to build and push just the image with: ko publish github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-go However, if you use ko for the next step, this is not necessary. Choose how you would like to deploy the application: yaml (with Dockerfile) If you look in service.yaml , take the <image> name above and insert it into the image: field, then run: kubectl apply -f service.yaml yaml (with ko) If using ko to build and push: ko apply -f service.yaml kn (with Dockerfile) If using kn to deploy: kn service create cloudevents-go --image = <IMAGE> kn (with ko) You can compose kn and ko to build and deploy with a single step using: kn service create cloudevents-go --image = $( ko publish github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-go ) Testing the sample \u00b6 Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-go http://cloudevents-go.default.1.2.3.4.xip.io cloudevents-go-ss5pj cloudevents-go-ss5pj True Then send a cloud event to it with: $ curl -X POST \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-go.default.1.2.3.4.xip.io You will get back: { \"message\" : \"Hello, Dave\" } Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Go"},{"location":"serving/samples/cloudevents/cloudevents-go/#cloud-events-go","text":"A simple web app written in Go that can receive and send Cloud Events that you can use for testing. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source) The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-go","title":"Cloud Events - Go"},{"location":"serving/samples/cloudevents/cloudevents-go/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"serving/samples/cloudevents/cloudevents-go/#the-sample-code","text":"If you look in cloudevents.go , you will see two key functions for the different modes of operation: func ( recv * Receiver ) ReceiveAndSend ( ctx context . Context , event cloudevents . Event ) cloudevents . Result { // This is called whenever an event is received if $K_SINK is set, and sends a new event // to the url in $K_SINK. } func ( recv * Receiver ) ReceiveAndReply ( ctx context . Context , event cloudevents . Event ) ( * cloudevents . Event , cloudevents . Result ) { // This is called whenever an event is received if $K_SINK is NOT set, and it replies with // the new event instead. } Choose how you would like to build the application: Dockerfile If you look in Dockerfile , you will see a method for pulling in the dependencies and building a small Go container based on Alpine. You can build and push this to your registry of choice via: docker build -t <image> . docker push <image> ko You can use ko to build and push just the image with: ko publish github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-go However, if you use ko for the next step, this is not necessary. Choose how you would like to deploy the application: yaml (with Dockerfile) If you look in service.yaml , take the <image> name above and insert it into the image: field, then run: kubectl apply -f service.yaml yaml (with ko) If using ko to build and push: ko apply -f service.yaml kn (with Dockerfile) If using kn to deploy: kn service create cloudevents-go --image = <IMAGE> kn (with ko) You can compose kn and ko to build and deploy with a single step using: kn service create cloudevents-go --image = $( ko publish github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-go )","title":"The sample code."},{"location":"serving/samples/cloudevents/cloudevents-go/#testing-the-sample","text":"Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-go http://cloudevents-go.default.1.2.3.4.xip.io cloudevents-go-ss5pj cloudevents-go-ss5pj True Then send a cloud event to it with: $ curl -X POST \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-go.default.1.2.3.4.xip.io You will get back: { \"message\" : \"Hello, Dave\" }","title":"Testing the sample"},{"location":"serving/samples/cloudevents/cloudevents-go/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Removing the sample app deployment"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/","text":"Cloud Events - Node.js \u00b6 A simple web app written in Node.js that can receive and send Cloud Events that you can use for testing. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source) The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-nodejs Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). The Sample Code \u00b6 In the index.js file, you will see two key functions for the different modes of operation: const receiveAndSend = ( cloudEvent , res ) => { // This is called whenever an event is received if $K_SINK is set, and sends a new event // to the url in $K_SINK. } const receiveAndReply = ( cloudEvent , res ) => { // This is called whenever an event is received if $K_SINK is NOT set, and it replies with // the new event instead. } Build and Deploy the Application \u00b6 In the Dockerfile , you can see how the dependencies are installed using npm. You can build and push this to your registry of choice via: docker build -t <image> . docker push <image> yaml To deploy the Knative service, edit the service.yaml file and replace <registry/repository/image:tag> with the image you have just created. kubectl apply -f service.yaml kn To deploy using the kn CLI: kn service create cloudevents-nodejs --image = <image> Testing the sample \u00b6 Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-nodejs http://cloudevents-nodejs.default.1.2.3.4.xip.io cloudevents-nodejs-ss5pj cloudevents-nodejs-ss5pj True Then send a cloud event to it with: $ curl -X POST \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-nodejs.default.1.2.3.4.xip.io You will get back: { \"message\" : \"Hello, Dave\" } Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service. yaml Run: kubectl delete --filename service.yaml kn Run: kn service delete cloudevents-nodejs","title":"Node.js"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#cloud-events-nodejs","text":"A simple web app written in Node.js that can receive and send Cloud Events that you can use for testing. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source) The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-nodejs","title":"Cloud Events - Node.js"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#the-sample-code","text":"In the index.js file, you will see two key functions for the different modes of operation: const receiveAndSend = ( cloudEvent , res ) => { // This is called whenever an event is received if $K_SINK is set, and sends a new event // to the url in $K_SINK. } const receiveAndReply = ( cloudEvent , res ) => { // This is called whenever an event is received if $K_SINK is NOT set, and it replies with // the new event instead. }","title":"The Sample Code"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#build-and-deploy-the-application","text":"In the Dockerfile , you can see how the dependencies are installed using npm. You can build and push this to your registry of choice via: docker build -t <image> . docker push <image> yaml To deploy the Knative service, edit the service.yaml file and replace <registry/repository/image:tag> with the image you have just created. kubectl apply -f service.yaml kn To deploy using the kn CLI: kn service create cloudevents-nodejs --image = <image>","title":"Build and Deploy the Application"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#testing-the-sample","text":"Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-nodejs http://cloudevents-nodejs.default.1.2.3.4.xip.io cloudevents-nodejs-ss5pj cloudevents-nodejs-ss5pj True Then send a cloud event to it with: $ curl -X POST \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-nodejs.default.1.2.3.4.xip.io You will get back: { \"message\" : \"Hello, Dave\" }","title":"Testing the sample"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service. yaml Run: kubectl delete --filename service.yaml kn Run: kn service delete cloudevents-nodejs","title":"Removing the sample app deployment"},{"location":"serving/samples/cloudevents/cloudevents-rust/","text":"Cloud Events - Rust \u00b6 A simple web app written in Rust using Actix web that can receive CloudEvents. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. The input event is modified assigning a new source and type attribute. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source). The input event is modified assigning a new source and type attribute. The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-rust Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). This guide uses Rust musl toolchain to build the image in order to create really small docker images. To install the Rust toolchain: rustup . To install musl support: MUSL support for fully static binaries . Build and deploy the sample \u00b6 To build the binary, run: cargo build --target x86_64-unknown-linux-musl --release This will build a statically linked binary, in order to create an image from scratch. Now build the docker image: docker build -t <image> . yaml To deploy the Knative Service, look in the service.yaml and replace <image> with the deployed image name. Then run: kubectl apply -f service.yaml kn If using kn to deploy: kn service create cloudevents-rust --image = <image> Testing the sample \u00b6 Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-rust http://cloudevents-rust.xip.io cloudevents-rust-vl8fq cloudevents-rust-vl8fq True Then send a CloudEvent to it with: $ curl \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-rust.xip.io You can also send CloudEvents spawning a temporary curl pod in your cluster with: $ kubectl run curl \\ --image = curlimages/curl --rm = true --restart = Never -ti -- \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-rust.default.svc You'll get as result: > POST / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.69.1 > Accept: */* > content-type: application/json > ce-specversion: 1 .0 > ce-source: http://curl-command > ce-type: curl.demo > ce-id: 123 -abc > Content-Length: 15 > < HTTP/1.1 200 OK < content-length: 15 < content-type: application/json < ce-specversion: 1 .0 < ce-id: 123 -abc < ce-type: dev.knative.docs.sample < ce-source: https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-rust < date: Sat, 23 May 2020 09 :00:01 GMT < { \"name\" : \"Dave\" } Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service. yaml Run: kubectl delete --filename service.yaml kn Run: kn service delete cloudevents-rust","title":"Rust"},{"location":"serving/samples/cloudevents/cloudevents-rust/#cloud-events-rust","text":"A simple web app written in Rust using Actix web that can receive CloudEvents. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. The input event is modified assigning a new source and type attribute. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source). The input event is modified assigning a new source and type attribute. The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-rust","title":"Cloud Events - Rust"},{"location":"serving/samples/cloudevents/cloudevents-rust/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). This guide uses Rust musl toolchain to build the image in order to create really small docker images. To install the Rust toolchain: rustup . To install musl support: MUSL support for fully static binaries .","title":"Before you begin"},{"location":"serving/samples/cloudevents/cloudevents-rust/#build-and-deploy-the-sample","text":"To build the binary, run: cargo build --target x86_64-unknown-linux-musl --release This will build a statically linked binary, in order to create an image from scratch. Now build the docker image: docker build -t <image> . yaml To deploy the Knative Service, look in the service.yaml and replace <image> with the deployed image name. Then run: kubectl apply -f service.yaml kn If using kn to deploy: kn service create cloudevents-rust --image = <image>","title":"Build and deploy the sample"},{"location":"serving/samples/cloudevents/cloudevents-rust/#testing-the-sample","text":"Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-rust http://cloudevents-rust.xip.io cloudevents-rust-vl8fq cloudevents-rust-vl8fq True Then send a CloudEvent to it with: $ curl \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-rust.xip.io You can also send CloudEvents spawning a temporary curl pod in your cluster with: $ kubectl run curl \\ --image = curlimages/curl --rm = true --restart = Never -ti -- \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-rust.default.svc You'll get as result: > POST / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.69.1 > Accept: */* > content-type: application/json > ce-specversion: 1 .0 > ce-source: http://curl-command > ce-type: curl.demo > ce-id: 123 -abc > Content-Length: 15 > < HTTP/1.1 200 OK < content-length: 15 < content-type: application/json < ce-specversion: 1 .0 < ce-id: 123 -abc < ce-type: dev.knative.docs.sample < ce-source: https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-rust < date: Sat, 23 May 2020 09 :00:01 GMT < { \"name\" : \"Dave\" }","title":"Testing the sample"},{"location":"serving/samples/cloudevents/cloudevents-rust/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service. yaml Run: kubectl delete --filename service.yaml kn Run: kn service delete cloudevents-rust","title":"Removing the sample app deployment"},{"location":"serving/samples/cloudevents/cloudevents-spring/","text":"Cloud Events - Java and Spring \u00b6 A simple web app written in Java using Spring Cloud Function that can receive CloudEvents. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working with the Knative Eventing Broker concept. The input event is modified assigning a new source and type attribute. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source). The input event is modified assigning a new source and type attribute. The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-spring Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Build and deploy the sample \u00b6 To build the image, run: mvn compile jib:build -Dimage = <image_name> To deploy the Knative Service, edit the service.yaml file and replace <image> with the deployed image name. Then run: kubectl apply -f service.yaml Testing the sample \u00b6 Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-spring http://cloudevents-java.xip.io cloudevents-spring-86h28 cloudevents-spring-86h28 True Then send a CloudEvent to it with: $ curl \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.xip.io You can also send CloudEvents spawning a temporary curl pod in your cluster with: $ kubectl run curl \\ --image = curlimages/curl --rm = true --restart = Never -ti -- \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.default.svc You'll see on the console: > POST / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.69.1 > Accept: */* > content-type: application/json > ce-specversion: 1 .0 > ce-source: http://curl-command > ce-type: curl.demo > ce-id: 123 -abc > Content-Length: 15 > < HTTP/1.1 202 Accepted < ce-specversion: 1 .0 < ce-id: 123 -abc < ce-source: https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-spring < ce-type: curl.demo < content-type: application/json < content-length: 15 < { \"name\" : \"Dave\" } Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service: yaml Run: kubectl delete --filename service.yaml kn Run: kn service delete cloudevents-spring","title":"Java and Spring"},{"location":"serving/samples/cloudevents/cloudevents-spring/#cloud-events-java-and-spring","text":"A simple web app written in Java using Spring Cloud Function that can receive CloudEvents. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working with the Knative Eventing Broker concept. The input event is modified assigning a new source and type attribute. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source). The input event is modified assigning a new source and type attribute. The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-spring","title":"Cloud Events - Java and Spring"},{"location":"serving/samples/cloudevents/cloudevents-spring/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"serving/samples/cloudevents/cloudevents-spring/#build-and-deploy-the-sample","text":"To build the image, run: mvn compile jib:build -Dimage = <image_name> To deploy the Knative Service, edit the service.yaml file and replace <image> with the deployed image name. Then run: kubectl apply -f service.yaml","title":"Build and deploy the sample"},{"location":"serving/samples/cloudevents/cloudevents-spring/#testing-the-sample","text":"Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-spring http://cloudevents-java.xip.io cloudevents-spring-86h28 cloudevents-spring-86h28 True Then send a CloudEvent to it with: $ curl \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.xip.io You can also send CloudEvents spawning a temporary curl pod in your cluster with: $ kubectl run curl \\ --image = curlimages/curl --rm = true --restart = Never -ti -- \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.default.svc You'll see on the console: > POST / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.69.1 > Accept: */* > content-type: application/json > ce-specversion: 1 .0 > ce-source: http://curl-command > ce-type: curl.demo > ce-id: 123 -abc > Content-Length: 15 > < HTTP/1.1 202 Accepted < ce-specversion: 1 .0 < ce-id: 123 -abc < ce-source: https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-spring < ce-type: curl.demo < content-type: application/json < content-length: 15 < { \"name\" : \"Dave\" }","title":"Testing the sample"},{"location":"serving/samples/cloudevents/cloudevents-spring/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Removing the sample app deployment"},{"location":"serving/samples/cloudevents/cloudevents-spring/#removing-the-sample-app-deployment_1","text":"To remove the sample app from your cluster, delete the service: yaml Run: kubectl delete --filename service.yaml kn Run: kn service delete cloudevents-spring","title":"Removing the sample app deployment"},{"location":"serving/samples/cloudevents/cloudevents-vertx/","text":"Cloud Events - Java and Vert.x \u00b6 A simple web app written in Java using Vert.x that can receive CloudEvents. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. The input event is modified assigning a new source and type attribute. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source). The input event is modified assigning a new source and type attribute. The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-vertx Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Build and deploy the sample \u00b6 To build the image, run: mvn compile jib:build -Dimage = <image_name> yaml To deploy the Knative Service, look in the service.yaml and replace <image> with the deployed image name. Then run: kubectl apply -f service.yaml kn If using kn to deploy: kn service create cloudevents-vertx --image = <image> Testing the sample \u00b6 Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-vertx http://cloudevents-java.xip.io cloudevents-vertx-86h28 cloudevents-vertx-86h28 True Then send a CloudEvent to it with: $ curl \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.xip.io You can also send CloudEvents spawning a temporary curl pod in your cluster with: $ kubectl run curl \\ --image = curlimages/curl --rm = true --restart = Never -ti -- \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.default.svc You'll see on the console: > POST / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.69.1 > Accept: */* > content-type: application/json > ce-specversion: 1 .0 > ce-source: http://curl-command > ce-type: curl.demo > ce-id: 123 -abc > Content-Length: 15 > < HTTP/1.1 202 Accepted < ce-specversion: 1 .0 < ce-id: 123 -abc < ce-source: https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-vertx < ce-type: curl.demo < content-type: application/json < content-length: 15 < { \"name\" : \"Dave\" } Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service: yaml Run: kubectl delete --filename service.yaml kn Run: kn service delete cloudevents-vertx","title":"Java and Vert.x"},{"location":"serving/samples/cloudevents/cloudevents-vertx/#cloud-events-java-and-vertx","text":"A simple web app written in Java using Vert.x that can receive CloudEvents. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. The input event is modified assigning a new source and type attribute. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source). The input event is modified assigning a new source and type attribute. The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-vertx","title":"Cloud Events - Java and Vert.x"},{"location":"serving/samples/cloudevents/cloudevents-vertx/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"serving/samples/cloudevents/cloudevents-vertx/#build-and-deploy-the-sample","text":"To build the image, run: mvn compile jib:build -Dimage = <image_name> yaml To deploy the Knative Service, look in the service.yaml and replace <image> with the deployed image name. Then run: kubectl apply -f service.yaml kn If using kn to deploy: kn service create cloudevents-vertx --image = <image>","title":"Build and deploy the sample"},{"location":"serving/samples/cloudevents/cloudevents-vertx/#testing-the-sample","text":"Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-vertx http://cloudevents-java.xip.io cloudevents-vertx-86h28 cloudevents-vertx-86h28 True Then send a CloudEvent to it with: $ curl \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.xip.io You can also send CloudEvents spawning a temporary curl pod in your cluster with: $ kubectl run curl \\ --image = curlimages/curl --rm = true --restart = Never -ti -- \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.default.svc You'll see on the console: > POST / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.69.1 > Accept: */* > content-type: application/json > ce-specversion: 1 .0 > ce-source: http://curl-command > ce-type: curl.demo > ce-id: 123 -abc > Content-Length: 15 > < HTTP/1.1 202 Accepted < ce-specversion: 1 .0 < ce-id: 123 -abc < ce-source: https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-vertx < ce-type: curl.demo < content-type: application/json < content-length: 15 < { \"name\" : \"Dave\" }","title":"Testing the sample"},{"location":"serving/samples/cloudevents/cloudevents-vertx/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service: yaml Run: kubectl delete --filename service.yaml kn Run: kn service delete cloudevents-vertx","title":"Removing the sample app deployment"},{"location":"serving/samples/gitwebhook-go/","text":"GitHub webhook sample - Go \u00b6 A handler written in Go that demonstrates interacting with GitHub through a webhook. Before you begin \u00b6 You must meet the following requirements to run this sample: Own a public domain. For example, you can create a domain with Google Domains . A Kubernetes cluster running with the following: Knative Serving must be installed. For details about setting up a Knative cluster, see the installation guides . Your Knative cluster must be configured to use your custom domain . You must ensure that your Knative cluster uses a static IP address: For Google Kubernetes Engine, see assigning a static IP address . For other cloud providers, refer to your provider's documentation. An installed version of Docker . A Docker Hub account to which you are able to upload your sample's container image. Build the sample code \u00b6 Download a copy of the code: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/gitwebhook-go Use Docker to build a container image for this service. Replace {DOCKER_HUB_USERNAME} with your Docker Hub username in the following commands. export DOCKER_HUB_USERNAME = username # Build the container, run from the project folder docker build -t ${ DOCKER_HUB_USERNAME } /gitwebhook-go . # Push the container to the registry docker push ${ DOCKER_HUB_USERNAME } /gitwebhook-go Create a secret that holds two values from GitHub: A personal access token that you will use to make API requests to GitHub. Ensure that you grant read/write permission in the repo for that personal access token. Follow the GitHub instructions to A webhook secret that you will use to validate requests. Base64 encode the access token: $ echo -n \"45d382d4a9a93c453fb7c8adc109121e7c29fa3ca\" | base64 NDVkMzgyZDRhOWE5M2M0NTNmYjdjOGFkYzEwOTEyMWU3YzI5ZmEzY2E = Copy the encoded access token into github-secret.yaml next to personalAccessToken: . Create a webhook secret value unique to this sample, base64 encode it, and copy it into github-secret.yaml next to webhookSecret: : $ echo -n \"mygithubwebhooksecret\" | base64 bXlnaXRodWJ3ZWJob29rc2VjcmV0 Apply the secret to your cluster: kubectl apply --filename github-secret.yaml Next, update the service.yaml file in the project to reference the tagged image from step 1. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : gitwebhook namespace : default spec : template : spec : containers : - # Replace {DOCKER_HUB_USERNAME} with your actual docker hub username image : docker.io/{DOCKER_HUB_USERNAME}/gitwebhook-go:latest env : - name : GITHUB_PERSONAL_TOKEN valueFrom : secretKeyRef : name : githubsecret key : personalAccessToken - name : WEBHOOK_SECRET valueFrom : secretKeyRef : name : githubsecret key : webhookSecret Use kubectl to apply the service.yaml file. $ kubectl apply --filename service.yaml Response: service \"gitwebhook\" created Create a webhook in your GitHub repo using the URL for your gitwebhook service: Retrieve the hostname for this service, using the following command: $ kubectl get ksvc gitwebhook \\ --output = custom-columns = NAME:.metadata.name,DOMAIN:.status.domain Example response: NAME DOMAIN gitwebhook gitwebhook.default.MYCUSTOMDOMAIN.com where MYCUSTOMDOMAIN is the domain that you set as your custom domain . Go to the GitHub repository for which you have privileges to create a webhook. Click Settings > Webhooks > Add webhook to open the Webhooks page. Enter the Payload URL as http://{DOMAIN} , where {DOMAIN} is the address from the kubectl get ksvc gitwebhook command. For example: http://gitwebhook.default.MYCUSTOMDOMAIN.com Set the Content type to application/json . Enter your webhook secret in Secret using the original base value that you set in webhookSecret (not the base64 encoded value). For example: mygithubwebhooksecret If you did not enabled TLS certificates , click Disable under SSL Validation . Click Add webhook to create the webhook. Exploring \u00b6 Once deployed, you can inspect the created resources with kubectl commands: # This will show the Knative service that we created: kubectl get ksvc --output yaml # This will show the Route, created by the service: kubectl get route --output yaml # This will show the Configuration, created by the service: kubectl get configurations --output yaml # This will show the Revision, created by the Configuration: kubectl get revisions --output yaml Testing the service \u00b6 Now that you have the service running and the webhook created, send a Pull Request to the same GitHub repo where you added the webhook. If all is working right, you'll see the title of the PR will be modified, with the text (looks pretty legit) appended the end of the title. Cleaning up \u00b6 To clean up the sample service: kubectl delete --filename service.yaml","title":"GitHub Webhook - Go"},{"location":"serving/samples/gitwebhook-go/#github-webhook-sample-go","text":"A handler written in Go that demonstrates interacting with GitHub through a webhook.","title":"GitHub webhook sample - Go"},{"location":"serving/samples/gitwebhook-go/#before-you-begin","text":"You must meet the following requirements to run this sample: Own a public domain. For example, you can create a domain with Google Domains . A Kubernetes cluster running with the following: Knative Serving must be installed. For details about setting up a Knative cluster, see the installation guides . Your Knative cluster must be configured to use your custom domain . You must ensure that your Knative cluster uses a static IP address: For Google Kubernetes Engine, see assigning a static IP address . For other cloud providers, refer to your provider's documentation. An installed version of Docker . A Docker Hub account to which you are able to upload your sample's container image.","title":"Before you begin"},{"location":"serving/samples/gitwebhook-go/#build-the-sample-code","text":"Download a copy of the code: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/gitwebhook-go Use Docker to build a container image for this service. Replace {DOCKER_HUB_USERNAME} with your Docker Hub username in the following commands. export DOCKER_HUB_USERNAME = username # Build the container, run from the project folder docker build -t ${ DOCKER_HUB_USERNAME } /gitwebhook-go . # Push the container to the registry docker push ${ DOCKER_HUB_USERNAME } /gitwebhook-go Create a secret that holds two values from GitHub: A personal access token that you will use to make API requests to GitHub. Ensure that you grant read/write permission in the repo for that personal access token. Follow the GitHub instructions to A webhook secret that you will use to validate requests. Base64 encode the access token: $ echo -n \"45d382d4a9a93c453fb7c8adc109121e7c29fa3ca\" | base64 NDVkMzgyZDRhOWE5M2M0NTNmYjdjOGFkYzEwOTEyMWU3YzI5ZmEzY2E = Copy the encoded access token into github-secret.yaml next to personalAccessToken: . Create a webhook secret value unique to this sample, base64 encode it, and copy it into github-secret.yaml next to webhookSecret: : $ echo -n \"mygithubwebhooksecret\" | base64 bXlnaXRodWJ3ZWJob29rc2VjcmV0 Apply the secret to your cluster: kubectl apply --filename github-secret.yaml Next, update the service.yaml file in the project to reference the tagged image from step 1. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : gitwebhook namespace : default spec : template : spec : containers : - # Replace {DOCKER_HUB_USERNAME} with your actual docker hub username image : docker.io/{DOCKER_HUB_USERNAME}/gitwebhook-go:latest env : - name : GITHUB_PERSONAL_TOKEN valueFrom : secretKeyRef : name : githubsecret key : personalAccessToken - name : WEBHOOK_SECRET valueFrom : secretKeyRef : name : githubsecret key : webhookSecret Use kubectl to apply the service.yaml file. $ kubectl apply --filename service.yaml Response: service \"gitwebhook\" created Create a webhook in your GitHub repo using the URL for your gitwebhook service: Retrieve the hostname for this service, using the following command: $ kubectl get ksvc gitwebhook \\ --output = custom-columns = NAME:.metadata.name,DOMAIN:.status.domain Example response: NAME DOMAIN gitwebhook gitwebhook.default.MYCUSTOMDOMAIN.com where MYCUSTOMDOMAIN is the domain that you set as your custom domain . Go to the GitHub repository for which you have privileges to create a webhook. Click Settings > Webhooks > Add webhook to open the Webhooks page. Enter the Payload URL as http://{DOMAIN} , where {DOMAIN} is the address from the kubectl get ksvc gitwebhook command. For example: http://gitwebhook.default.MYCUSTOMDOMAIN.com Set the Content type to application/json . Enter your webhook secret in Secret using the original base value that you set in webhookSecret (not the base64 encoded value). For example: mygithubwebhooksecret If you did not enabled TLS certificates , click Disable under SSL Validation . Click Add webhook to create the webhook.","title":"Build the sample code"},{"location":"serving/samples/gitwebhook-go/#exploring","text":"Once deployed, you can inspect the created resources with kubectl commands: # This will show the Knative service that we created: kubectl get ksvc --output yaml # This will show the Route, created by the service: kubectl get route --output yaml # This will show the Configuration, created by the service: kubectl get configurations --output yaml # This will show the Revision, created by the Configuration: kubectl get revisions --output yaml","title":"Exploring"},{"location":"serving/samples/gitwebhook-go/#testing-the-service","text":"Now that you have the service running and the webhook created, send a Pull Request to the same GitHub repo where you added the webhook. If all is working right, you'll see the title of the PR will be modified, with the text (looks pretty legit) appended the end of the title.","title":"Testing the service"},{"location":"serving/samples/gitwebhook-go/#cleaning-up","text":"To clean up the sample service: kubectl delete --filename service.yaml","title":"Cleaning up"},{"location":"serving/samples/grpc-ping-go/","text":"gRPC Server - Go \u00b6 A gRPC server written in Go. This sample can be used to try out gRPC, HTTP/2, and custom port configuration in a knative service. The container image is built with two binaries: the server and the client. This is done for ease of testing and is not a recommended practice for production containers. Prerequisites \u00b6 Install the latest version of Knative Serving . Install docker . A Docker Hub account to which you can upload the sample's container image. Build and Deploy the sample code \u00b6 Download a copy of the code: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/grpc-ping-go Use Docker to build a container image for this service and push to Docker Hub. Replace {username} with your Docker Hub username then run the commands: # Build the container on your local machine. docker build --tag \"{username}/grpc-ping-go\" . # Push the container to docker registry. docker push \"{username}/grpc-ping-go\" Update the service.yaml file in the project to reference the published image from step 1. Replace {username} in service.yaml with your Docker Hub user name: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : grpc-ping namespace : default spec : template : spec : containers : - image : docker.io/{username}/grpc-ping-go ports : - name : h2c containerPort : 8080 Use kubectl to deploy the service. kubectl apply --filename service.yaml Response: service \"grpc-ping\" created Exploring \u00b6 Once deployed, you can inspect the created resources with kubectl commands: # This will show the Knative service that we created: kubectl get ksvc --output yaml # This will show the Route, created by the service: kubectl get route --output yaml # This will show the Configuration, created by the service: kubectl get configurations --output yaml # This will show the Revision, created by the Configuration: kubectl get revisions --output yaml Testing the service \u00b6 Testing the gRPC service requires using a gRPC client built from the same protobuf definition used by the server. The Dockerfile builds the client binary. To run the client you will use the same container image deployed for the server with an override to the entrypoint command to use the client binary instead of the server binary. Replace {username} with your Docker Hub user name and run the command: docker run --rm { username } /grpc-ping-go \\ /client \\ -server_addr = \"grpc-ping.default.1.2.3.4.xip.io:80\" \\ -insecure The arguments after the container tag {username}/grpc-ping-go are used instead of the entrypoint command defined in the Dockerfile CMD statement.","title":"gRPC Server - Go"},{"location":"serving/samples/grpc-ping-go/#grpc-server-go","text":"A gRPC server written in Go. This sample can be used to try out gRPC, HTTP/2, and custom port configuration in a knative service. The container image is built with two binaries: the server and the client. This is done for ease of testing and is not a recommended practice for production containers.","title":"gRPC Server - Go"},{"location":"serving/samples/grpc-ping-go/#prerequisites","text":"Install the latest version of Knative Serving . Install docker . A Docker Hub account to which you can upload the sample's container image.","title":"Prerequisites"},{"location":"serving/samples/grpc-ping-go/#build-and-deploy-the-sample-code","text":"Download a copy of the code: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/grpc-ping-go Use Docker to build a container image for this service and push to Docker Hub. Replace {username} with your Docker Hub username then run the commands: # Build the container on your local machine. docker build --tag \"{username}/grpc-ping-go\" . # Push the container to docker registry. docker push \"{username}/grpc-ping-go\" Update the service.yaml file in the project to reference the published image from step 1. Replace {username} in service.yaml with your Docker Hub user name: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : grpc-ping namespace : default spec : template : spec : containers : - image : docker.io/{username}/grpc-ping-go ports : - name : h2c containerPort : 8080 Use kubectl to deploy the service. kubectl apply --filename service.yaml Response: service \"grpc-ping\" created","title":"Build and Deploy the sample code"},{"location":"serving/samples/grpc-ping-go/#exploring","text":"Once deployed, you can inspect the created resources with kubectl commands: # This will show the Knative service that we created: kubectl get ksvc --output yaml # This will show the Route, created by the service: kubectl get route --output yaml # This will show the Configuration, created by the service: kubectl get configurations --output yaml # This will show the Revision, created by the Configuration: kubectl get revisions --output yaml","title":"Exploring"},{"location":"serving/samples/grpc-ping-go/#testing-the-service","text":"Testing the gRPC service requires using a gRPC client built from the same protobuf definition used by the server. The Dockerfile builds the client binary. To run the client you will use the same container image deployed for the server with an override to the entrypoint command to use the client binary instead of the server binary. Replace {username} with your Docker Hub user name and run the command: docker run --rm { username } /grpc-ping-go \\ /client \\ -server_addr = \"grpc-ping.default.1.2.3.4.xip.io:80\" \\ -insecure The arguments after the container tag {username}/grpc-ping-go are used instead of the entrypoint command defined in the Dockerfile CMD statement.","title":"Testing the service"},{"location":"serving/samples/hello-world/","text":"","title":"Overview"},{"location":"serving/samples/hello-world/helloworld-csharp/","text":"Hello world - .NET Core \u00b6 A simple web app written in C# using .NET Core 3.1 that you can use for testing. It reads in an env variable TARGET and prints \"Hello \\${TARGET}!\". If TARGET is not specified, it will use \"World\" as the TARGET. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-csharp Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). You have installed .NET Core SDK 3.1 . Recreating the sample code \u00b6 First, make sure you have .NET Core SDK 3.1 installed: dotnet --version 3 .1.100 From the console, create a new empty web project using the dotnet command: dotnet new web -o helloworld-csharp Update the CreateHostBuilder definition in Program.cs by adding .UseUrls() to define the serving port: public static IHostBuilder CreateHostBuilder ( string [] args ) { string port = Environment . GetEnvironmentVariable ( \"PORT\" ) ?? \"8080\" ; string url = String . Concat ( \"http://0.0.0.0:\" , port ); return Host . CreateDefaultBuilder ( args ) . ConfigureWebHostDefaults ( webBuilder => { webBuilder . UseStartup < Startup >(). UseUrls ( url ); }); } Update the app.UseEndpoints(...) statement in Startup.cs to read and return the TARGET environment variable: app . UseEndpoints ( endpoints => { endpoints . MapGet ( \"/\" , async context => { var target = Environment . GetEnvironmentVariable ( \"TARGET\" ) ?? \"World\" ; await context . Response . WriteAsync ( $ \"Hello {target}!\\n\" ); }); }); In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing an ASP.NET Core app, see Docker images for ASP.NET Core . # Use Microsoft's official build .NET image. FROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build WORKDIR /app # Install production dependencies. # Copy csproj and restore as distinct layers. COPY *.csproj ./ RUN dotnet restore # Copy local code to the container image. COPY . ./ WORKDIR /app # Build a release artifact. RUN dotnet publish -c Release -o out # Use Microsoft's official runtime .NET image. FROM mcr.microsoft.com/dotnet/core/aspnet:3.1 AS runtime WORKDIR /app COPY --from = build /app/out ./ # Run the web service on container startup. ENTRYPOINT [ \"dotnet\" , \"helloworld-csharp.dll\" ] Create a .dockerignore file to ensure that any files related to a local build do not affect the container that you build for deployment. Dockerfile README.md **/obj/ **/bin/ Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-csharp namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-csharp env : - name : TARGET value : \"C# Sample v1\" Building and deploying the sample \u00b6 Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-csharp . # Push the container to docker registry docker push { username } /helloworld-csharp After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). To find the URL for your service, use kubectl get ksvc helloworld-csharp --output=custom-columns=NAME:.metadata.name,URL:.status.url NAME URL helloworld-csharp http://helloworld-csharp.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-csharp.default.1.2.3.4.xip.io Hello C# Sample v1! Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":".NET"},{"location":"serving/samples/hello-world/helloworld-csharp/#hello-world-net-core","text":"A simple web app written in C# using .NET Core 3.1 that you can use for testing. It reads in an env variable TARGET and prints \"Hello \\${TARGET}!\". If TARGET is not specified, it will use \"World\" as the TARGET. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-csharp","title":"Hello world - .NET Core"},{"location":"serving/samples/hello-world/helloworld-csharp/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). You have installed .NET Core SDK 3.1 .","title":"Before you begin"},{"location":"serving/samples/hello-world/helloworld-csharp/#recreating-the-sample-code","text":"First, make sure you have .NET Core SDK 3.1 installed: dotnet --version 3 .1.100 From the console, create a new empty web project using the dotnet command: dotnet new web -o helloworld-csharp Update the CreateHostBuilder definition in Program.cs by adding .UseUrls() to define the serving port: public static IHostBuilder CreateHostBuilder ( string [] args ) { string port = Environment . GetEnvironmentVariable ( \"PORT\" ) ?? \"8080\" ; string url = String . Concat ( \"http://0.0.0.0:\" , port ); return Host . CreateDefaultBuilder ( args ) . ConfigureWebHostDefaults ( webBuilder => { webBuilder . UseStartup < Startup >(). UseUrls ( url ); }); } Update the app.UseEndpoints(...) statement in Startup.cs to read and return the TARGET environment variable: app . UseEndpoints ( endpoints => { endpoints . MapGet ( \"/\" , async context => { var target = Environment . GetEnvironmentVariable ( \"TARGET\" ) ?? \"World\" ; await context . Response . WriteAsync ( $ \"Hello {target}!\\n\" ); }); }); In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing an ASP.NET Core app, see Docker images for ASP.NET Core . # Use Microsoft's official build .NET image. FROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build WORKDIR /app # Install production dependencies. # Copy csproj and restore as distinct layers. COPY *.csproj ./ RUN dotnet restore # Copy local code to the container image. COPY . ./ WORKDIR /app # Build a release artifact. RUN dotnet publish -c Release -o out # Use Microsoft's official runtime .NET image. FROM mcr.microsoft.com/dotnet/core/aspnet:3.1 AS runtime WORKDIR /app COPY --from = build /app/out ./ # Run the web service on container startup. ENTRYPOINT [ \"dotnet\" , \"helloworld-csharp.dll\" ] Create a .dockerignore file to ensure that any files related to a local build do not affect the container that you build for deployment. Dockerfile README.md **/obj/ **/bin/ Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-csharp namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-csharp env : - name : TARGET value : \"C# Sample v1\"","title":"Recreating the sample code"},{"location":"serving/samples/hello-world/helloworld-csharp/#building-and-deploying-the-sample","text":"Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-csharp . # Push the container to docker registry docker push { username } /helloworld-csharp After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). To find the URL for your service, use kubectl get ksvc helloworld-csharp --output=custom-columns=NAME:.metadata.name,URL:.status.url NAME URL helloworld-csharp http://helloworld-csharp.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-csharp.default.1.2.3.4.xip.io Hello C# Sample v1!","title":"Building and deploying the sample"},{"location":"serving/samples/hello-world/helloworld-csharp/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Removing the sample app deployment"},{"location":"serving/samples/hello-world/helloworld-go/","text":"Hello World - Go \u00b6 This guide describes the steps required to to create the helloworld-go sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. Prerequisites \u00b6 You will need: - A Kubernetes cluster with Knative installed and DNS configured . - Docker installed and running on your local machine, and a Docker Hub account configured. - Optional: You can use the Knative CLI client kn to simplify resource creation and deployment. Alternatively, you can use kubectl to apply resource files directly. Building \u00b6 Create a basic web server which listens on port 8080, by copying the following code into a new file named helloworld.go : package main import ( \"fmt\" \"log\" \"net/http\" \"os\" ) func handler ( w http . ResponseWriter , r * http . Request ) { log . Print ( \"helloworld: received a request\" ) target := os . Getenv ( \"TARGET\" ) if target == \"\" { target = \"World\" } fmt . Fprintf ( w , \"Hello %s!\\n\" , target ) } func main () { log . Print ( \"helloworld: starting server...\" ) http . HandleFunc ( \"/\" , handler ) port := os . Getenv ( \"PORT\" ) if port == \"\" { port = \"8080\" } log . Printf ( \"helloworld: listening on port %s\" , port ) log . Fatal ( http . ListenAndServe ( fmt . Sprintf ( \":%s\" , port ), nil )) } You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-go Navigate to your project directory and copy the following code into a new file named Dockerfile : # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. FROM golang:1.13 as builder # Create and change to the app directory. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod=readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o server # Use the official Alpine image for a lean production container. # https://hub.docker.com/_/alpine # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/server /server # Run the web service on container startup. CMD [ \"/server\" ] Use the Go tool to create a go.mod manifest. go mod init github.com/knative/docs/docs/serving/samples/hello-world/helloworld-go Deploying \u00b6 To build the sample code into a container, and push using Docker Hub, enter the following commands and replace {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-go . # Push the container to docker registry docker push { username } /helloworld-go After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Choose one of the following methods: yaml Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-go env : - name : TARGET value : \"Go Sample v1\" Check that the container image value in the service.yaml file matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml After your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). Run the following command to find the domain URL for your service: kubectl get ksvc helloworld-go --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-go http://helloworld-go.default.1.2.3.4.xip.io kn Use kn to deploy the service: kn service create helloworld-go --image = docker.io/ { username } /helloworld-go --env TARGET = \"Go Sample v1\" You should see output like this: Creating service 'helloworld-go' in namespace 'default' : 0 .031s The Configuration is still working to reflect the latest desired specification. 0 .051s The Route is still working to reflect the latest desired specification. 0 .076s Configuration \"helloworld-go\" is waiting for a Revision to become ready. 15 .694s ... 15 .738s Ingress has not yet been reconciled. 15 .784s Waiting for Envoys to receive Endpoints data. 16 .066s Waiting for load balancer to be ready 16 .237s Ready to serve. Service 'helloworld-go' created to latest revision 'helloworld-go-jjzgd-1' is available at URL: http://helloworld-go.default.1.2.3.4.xip.io You can then access your service through the resulting URL. Verifying \u00b6 Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-go.default.1.2.3.4.xip.io Hello Go Sample v1! Note: Add -v option to get more detail if the curl command failed. Removing \u00b6 To remove the sample app from your cluster, delete the service record: kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-go","title":"Go"},{"location":"serving/samples/hello-world/helloworld-go/#hello-world-go","text":"This guide describes the steps required to to create the helloworld-go sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value.","title":"Hello World - Go"},{"location":"serving/samples/hello-world/helloworld-go/#prerequisites","text":"You will need: - A Kubernetes cluster with Knative installed and DNS configured . - Docker installed and running on your local machine, and a Docker Hub account configured. - Optional: You can use the Knative CLI client kn to simplify resource creation and deployment. Alternatively, you can use kubectl to apply resource files directly.","title":"Prerequisites"},{"location":"serving/samples/hello-world/helloworld-go/#building","text":"Create a basic web server which listens on port 8080, by copying the following code into a new file named helloworld.go : package main import ( \"fmt\" \"log\" \"net/http\" \"os\" ) func handler ( w http . ResponseWriter , r * http . Request ) { log . Print ( \"helloworld: received a request\" ) target := os . Getenv ( \"TARGET\" ) if target == \"\" { target = \"World\" } fmt . Fprintf ( w , \"Hello %s!\\n\" , target ) } func main () { log . Print ( \"helloworld: starting server...\" ) http . HandleFunc ( \"/\" , handler ) port := os . Getenv ( \"PORT\" ) if port == \"\" { port = \"8080\" } log . Printf ( \"helloworld: listening on port %s\" , port ) log . Fatal ( http . ListenAndServe ( fmt . Sprintf ( \":%s\" , port ), nil )) } You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-go Navigate to your project directory and copy the following code into a new file named Dockerfile : # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. FROM golang:1.13 as builder # Create and change to the app directory. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod=readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o server # Use the official Alpine image for a lean production container. # https://hub.docker.com/_/alpine # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/server /server # Run the web service on container startup. CMD [ \"/server\" ] Use the Go tool to create a go.mod manifest. go mod init github.com/knative/docs/docs/serving/samples/hello-world/helloworld-go","title":"Building"},{"location":"serving/samples/hello-world/helloworld-go/#deploying","text":"To build the sample code into a container, and push using Docker Hub, enter the following commands and replace {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-go . # Push the container to docker registry docker push { username } /helloworld-go After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Choose one of the following methods: yaml Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-go env : - name : TARGET value : \"Go Sample v1\" Check that the container image value in the service.yaml file matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml After your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). Run the following command to find the domain URL for your service: kubectl get ksvc helloworld-go --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-go http://helloworld-go.default.1.2.3.4.xip.io kn Use kn to deploy the service: kn service create helloworld-go --image = docker.io/ { username } /helloworld-go --env TARGET = \"Go Sample v1\" You should see output like this: Creating service 'helloworld-go' in namespace 'default' : 0 .031s The Configuration is still working to reflect the latest desired specification. 0 .051s The Route is still working to reflect the latest desired specification. 0 .076s Configuration \"helloworld-go\" is waiting for a Revision to become ready. 15 .694s ... 15 .738s Ingress has not yet been reconciled. 15 .784s Waiting for Envoys to receive Endpoints data. 16 .066s Waiting for load balancer to be ready 16 .237s Ready to serve. Service 'helloworld-go' created to latest revision 'helloworld-go-jjzgd-1' is available at URL: http://helloworld-go.default.1.2.3.4.xip.io You can then access your service through the resulting URL.","title":"Deploying"},{"location":"serving/samples/hello-world/helloworld-go/#verifying","text":"Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-go.default.1.2.3.4.xip.io Hello Go Sample v1! Note: Add -v option to get more detail if the curl command failed.","title":"Verifying"},{"location":"serving/samples/hello-world/helloworld-go/#removing","text":"To remove the sample app from your cluster, delete the service record: kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-go","title":"Removing"},{"location":"serving/samples/hello-world/helloworld-java-spark/","text":"Hello World - Spark Java Framework \u00b6 A simple web app written in Java using Spark Java Framework that you can use for testing. This guide describes the steps required to to create the helloworld-java sample app and deploy it to your cluster. Prerequisites \u00b6 You will need: - A Kubernetes cluster with Knative installed and DNS configured . - Docker installed and running on your local machine, and a Docker Hub account configured. - Java SE 8 or later JDK . Develop \u00b6 The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-java Run the application locally: ./mvnw package && java -jar target/helloworld-0.0.1-SNAPSHOT-jar-with-dependencies.jar Go to http://localhost:8080/ to see your Hello World! message. In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Spark Java app, see Spark with Docker . For additional information on multi-stage docker builds for Java see Creating Smaller Java Image using Docker Multi-stage Build . Navigate to your project directory and copy the following code into a new file named Dockerfile : FROM maven:3.5-jdk-8-alpine as builder # Copy local code to the container image. WORKDIR /app COPY pom.xml . COPY src ./src RUN mvn package -DskipTests FROM openjdk:8-jre-alpine # Copy the jar to the production image from the builder stage. COPY --from = builder /app/target/helloworld-0.0.1-SNAPSHOT-jar-with-dependencies.jar helloworld.jar ENV PORT 8080 EXPOSE 8080 # Run the web service on container startup. CMD [ \"java\" , \"-jar\" , \"helloworld.jar\" ] To build the sample code into a container, and push using Docker Hub, enter the following commands and replace {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-java . # Push the container to docker registry docker push { username } /helloworld-java Deploy \u00b6 After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. Choose one of the following methods: kn Use kn to deploy the service, make sure to replace {username} with your Docker Hub username: kn service create helloworld-java --image = docker.io/ { username } /helloworld-java --env TARGET = \"SparkJava Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. kubectl Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-java namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-java env : - name : TARGET value : \"SparkJava Sample v1\" Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml After your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). Verify \u00b6 Run one of the followings commands to find the domain URL for your service. kn kn service describe helloworld-java -o url Example: http://helloworld-java.default.1.2.3.4.xip.io kubectl kubectl get ksvc helloworld-java --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-java http://helloworld-java.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. Example: curl http://helloworld-java.default.1.2.3.4.xip.io Hello SparkJava Sample v1! # Even easier with kn: curl $( kn service describe helloworld-java -o url ) Note: Add -v option to get more detail if the curl command failed. Delete \u00b6 To remove the sample app from your cluster, delete the service record. kn kn service delete helloworld-java kubectl kubectl delete --filename service.yaml","title":"Java (Spark)"},{"location":"serving/samples/hello-world/helloworld-java-spark/#hello-world-spark-java-framework","text":"A simple web app written in Java using Spark Java Framework that you can use for testing. This guide describes the steps required to to create the helloworld-java sample app and deploy it to your cluster.","title":"Hello World - Spark Java Framework"},{"location":"serving/samples/hello-world/helloworld-java-spark/#prerequisites","text":"You will need: - A Kubernetes cluster with Knative installed and DNS configured . - Docker installed and running on your local machine, and a Docker Hub account configured. - Java SE 8 or later JDK .","title":"Prerequisites"},{"location":"serving/samples/hello-world/helloworld-java-spark/#develop","text":"The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-java Run the application locally: ./mvnw package && java -jar target/helloworld-0.0.1-SNAPSHOT-jar-with-dependencies.jar Go to http://localhost:8080/ to see your Hello World! message. In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Spark Java app, see Spark with Docker . For additional information on multi-stage docker builds for Java see Creating Smaller Java Image using Docker Multi-stage Build . Navigate to your project directory and copy the following code into a new file named Dockerfile : FROM maven:3.5-jdk-8-alpine as builder # Copy local code to the container image. WORKDIR /app COPY pom.xml . COPY src ./src RUN mvn package -DskipTests FROM openjdk:8-jre-alpine # Copy the jar to the production image from the builder stage. COPY --from = builder /app/target/helloworld-0.0.1-SNAPSHOT-jar-with-dependencies.jar helloworld.jar ENV PORT 8080 EXPOSE 8080 # Run the web service on container startup. CMD [ \"java\" , \"-jar\" , \"helloworld.jar\" ] To build the sample code into a container, and push using Docker Hub, enter the following commands and replace {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-java . # Push the container to docker registry docker push { username } /helloworld-java","title":"Develop"},{"location":"serving/samples/hello-world/helloworld-java-spark/#deploy","text":"After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. Choose one of the following methods: kn Use kn to deploy the service, make sure to replace {username} with your Docker Hub username: kn service create helloworld-java --image = docker.io/ { username } /helloworld-java --env TARGET = \"SparkJava Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. kubectl Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-java namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-java env : - name : TARGET value : \"SparkJava Sample v1\" Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml After your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods).","title":"Deploy"},{"location":"serving/samples/hello-world/helloworld-java-spark/#verify","text":"Run one of the followings commands to find the domain URL for your service. kn kn service describe helloworld-java -o url Example: http://helloworld-java.default.1.2.3.4.xip.io kubectl kubectl get ksvc helloworld-java --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-java http://helloworld-java.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. Example: curl http://helloworld-java.default.1.2.3.4.xip.io Hello SparkJava Sample v1! # Even easier with kn: curl $( kn service describe helloworld-java -o url ) Note: Add -v option to get more detail if the curl command failed.","title":"Verify"},{"location":"serving/samples/hello-world/helloworld-java-spark/#delete","text":"To remove the sample app from your cluster, delete the service record. kn kn service delete helloworld-java kubectl kubectl delete --filename service.yaml","title":"Delete"},{"location":"serving/samples/hello-world/helloworld-java-spring/","text":"Hello World - Spring Boot Java \u00b6 This guide describes the steps required to create the helloworld-java-spring sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-java-spring Prerequisites \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions . Docker installed and running on your local machine, and a Docker Hub account configured. (optional) The Knative CLI client kn can be used to simplify the deployment. Alternatively, you can use kubectl , and apply resource files directly. Building the sample app \u00b6 From the console, create a new, empty web project by using the curl and unzip commands: curl https://start.spring.io/starter.zip \\ -d dependencies = web \\ -d name = helloworld \\ -d artifactId = helloworld \\ -o helloworld.zip unzip helloworld.zip If you don't have curl installed, you can accomplish the same by visiting the Spring Initializr page. Specify Artifact as helloworld and add the Web dependency. Then click Generate Project , download and unzip the sample archive. Update the SpringBootApplication class in src/main/java/com/example/helloworld/HelloworldApplication.java by adding a @RestController to handle the \"/\" mapping and also add a @Value field to provide the TARGET environment variable: package com.example.helloworld ; import org.springframework.beans.factory.annotation.Value ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.springframework.web.bind.annotation.GetMapping ; import org.springframework.web.bind.annotation.RestController ; @SpringBootApplication public class HelloworldApplication { @Value ( \"${TARGET:World}\" ) String target ; @RestController class HelloworldController { @GetMapping ( \"/\" ) String hello () { return \"Hello \" + target + \"!\" ; } } public static void main ( String [] args ) { SpringApplication . run ( HelloworldApplication . class , args ); } } Run the application locally: ./mvnw package && java -jar target/helloworld-0.0.1-SNAPSHOT.jar Go to http://localhost:8080/ to see your Hello World! message. In your project directory, create a file named Dockerfile and copy the code block below into it: # Use the official maven/Java 8 image to create a build artifact. # https://hub.docker.com/_/maven FROM maven:3.5-jdk-8-alpine as builder # Copy local code to the container image. WORKDIR /app COPY pom.xml . COPY src ./src # Build a release artifact. RUN mvn package -DskipTests # Use AdoptOpenJDK for base image. # It's important to use OpenJDK 8u191 or above that has container support enabled. # https://hub.docker.com/r/adoptopenjdk/openjdk8 # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM adoptopenjdk/openjdk8:jdk8u202-b08-alpine-slim # Copy the jar to the production image from the builder stage. COPY --from = builder /app/target/helloworld-*.jar /helloworld.jar # Run the web service on container startup. CMD [ \"java\" , \"-Djava.security.egd=file:/dev/./urandom\" , \"-jar\" , \"/helloworld.jar\" ] For detailed instructions on dockerizing a Spring Boot app, see Spring Boot with Docker . For additional information on multi-stage docker builds for Java see Creating Smaller Java Image using Docker Multi-stage Build . NOTE: Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username. Use Docker to build the sample code into a container, then push the container to the Docker registry: # Build the container on your local machine docker build -t { username } /helloworld-java-spring . # Push the container to docker registry docker push { username } /helloworld-java-spring Deploying the app \u00b6 After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. yaml Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-java-spring namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-java-spring env : - name : TARGET value : \"Java Spring Sample v1\" Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml kn With kn you can deploy the service with kn service create helloworld-java-spring --image = docker.io/ { username } /helloworld-java-spring --env TARGET = \"Java Spring Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. During the creation of your service, Knative performs the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down, including scaling down to zero active pods. Verification \u00b6 Find the domain URL for your service: kubectl kubectl get ksvc helloworld-java-spring --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-java-spring http://helloworld-java-spring.default.1.2.3.4.xip.io kn kn service describe helloworld-java-spring -o url Example: http://helloworld-java-spring.default.1.2.3.4.xip.io Make a request to your app and observe the result. Replace the URL below with the URL returned in the previous command. Example: curl http://helloworld-java-spring.default.1.2.3.4.xip.io Hello Java Spring Sample v1! # Even easier with kn: curl $( kn service describe helloworld-java-spring -o url ) Note: Add -v option to get more detail if the curl command failed. Deleting the app \u00b6 To remove the sample app from your cluster, delete the service. kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-java-spring","title":"Java (Spring)"},{"location":"serving/samples/hello-world/helloworld-java-spring/#hello-world-spring-boot-java","text":"This guide describes the steps required to create the helloworld-java-spring sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-java-spring","title":"Hello World - Spring Boot Java"},{"location":"serving/samples/hello-world/helloworld-java-spring/#prerequisites","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions . Docker installed and running on your local machine, and a Docker Hub account configured. (optional) The Knative CLI client kn can be used to simplify the deployment. Alternatively, you can use kubectl , and apply resource files directly.","title":"Prerequisites"},{"location":"serving/samples/hello-world/helloworld-java-spring/#building-the-sample-app","text":"From the console, create a new, empty web project by using the curl and unzip commands: curl https://start.spring.io/starter.zip \\ -d dependencies = web \\ -d name = helloworld \\ -d artifactId = helloworld \\ -o helloworld.zip unzip helloworld.zip If you don't have curl installed, you can accomplish the same by visiting the Spring Initializr page. Specify Artifact as helloworld and add the Web dependency. Then click Generate Project , download and unzip the sample archive. Update the SpringBootApplication class in src/main/java/com/example/helloworld/HelloworldApplication.java by adding a @RestController to handle the \"/\" mapping and also add a @Value field to provide the TARGET environment variable: package com.example.helloworld ; import org.springframework.beans.factory.annotation.Value ; import org.springframework.boot.SpringApplication ; import org.springframework.boot.autoconfigure.SpringBootApplication ; import org.springframework.web.bind.annotation.GetMapping ; import org.springframework.web.bind.annotation.RestController ; @SpringBootApplication public class HelloworldApplication { @Value ( \"${TARGET:World}\" ) String target ; @RestController class HelloworldController { @GetMapping ( \"/\" ) String hello () { return \"Hello \" + target + \"!\" ; } } public static void main ( String [] args ) { SpringApplication . run ( HelloworldApplication . class , args ); } } Run the application locally: ./mvnw package && java -jar target/helloworld-0.0.1-SNAPSHOT.jar Go to http://localhost:8080/ to see your Hello World! message. In your project directory, create a file named Dockerfile and copy the code block below into it: # Use the official maven/Java 8 image to create a build artifact. # https://hub.docker.com/_/maven FROM maven:3.5-jdk-8-alpine as builder # Copy local code to the container image. WORKDIR /app COPY pom.xml . COPY src ./src # Build a release artifact. RUN mvn package -DskipTests # Use AdoptOpenJDK for base image. # It's important to use OpenJDK 8u191 or above that has container support enabled. # https://hub.docker.com/r/adoptopenjdk/openjdk8 # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM adoptopenjdk/openjdk8:jdk8u202-b08-alpine-slim # Copy the jar to the production image from the builder stage. COPY --from = builder /app/target/helloworld-*.jar /helloworld.jar # Run the web service on container startup. CMD [ \"java\" , \"-Djava.security.egd=file:/dev/./urandom\" , \"-jar\" , \"/helloworld.jar\" ] For detailed instructions on dockerizing a Spring Boot app, see Spring Boot with Docker . For additional information on multi-stage docker builds for Java see Creating Smaller Java Image using Docker Multi-stage Build . NOTE: Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username. Use Docker to build the sample code into a container, then push the container to the Docker registry: # Build the container on your local machine docker build -t { username } /helloworld-java-spring . # Push the container to docker registry docker push { username } /helloworld-java-spring","title":"Building the sample app"},{"location":"serving/samples/hello-world/helloworld-java-spring/#deploying-the-app","text":"After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. yaml Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-java-spring namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-java-spring env : - name : TARGET value : \"Java Spring Sample v1\" Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml kn With kn you can deploy the service with kn service create helloworld-java-spring --image = docker.io/ { username } /helloworld-java-spring --env TARGET = \"Java Spring Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. During the creation of your service, Knative performs the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down, including scaling down to zero active pods.","title":"Deploying the app"},{"location":"serving/samples/hello-world/helloworld-java-spring/#verification","text":"Find the domain URL for your service: kubectl kubectl get ksvc helloworld-java-spring --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-java-spring http://helloworld-java-spring.default.1.2.3.4.xip.io kn kn service describe helloworld-java-spring -o url Example: http://helloworld-java-spring.default.1.2.3.4.xip.io Make a request to your app and observe the result. Replace the URL below with the URL returned in the previous command. Example: curl http://helloworld-java-spring.default.1.2.3.4.xip.io Hello Java Spring Sample v1! # Even easier with kn: curl $( kn service describe helloworld-java-spring -o url ) Note: Add -v option to get more detail if the curl command failed.","title":"Verification"},{"location":"serving/samples/hello-world/helloworld-java-spring/#deleting-the-app","text":"To remove the sample app from your cluster, delete the service. kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-java-spring","title":"Deleting the app"},{"location":"serving/samples/hello-world/helloworld-kotlin/","text":"Hello World - Kotlin \u00b6 A simple web app written in Kotlin using Ktor that you can use for testing. It reads in an env variable TARGET and prints \"Hello \\${TARGET}\". If TARGET is not specified, it will use \"World\" as the TARGET. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-kotlin Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Recreating the sample code \u00b6 Create a new directory and cd into it: mkdir hello cd hello Create a file named Main.kt at src/main/kotlin/com/example/hello and copy the code block below into it: mkdir -p src/main/kotlin/com/example/hello package com.example.hello import io.ktor.application.* import io.ktor.http.* import io.ktor.response.* import io.ktor.routing.* import io.ktor.server.engine.* import io.ktor.server.netty.* fun main ( args : Array < String > ) { val target = System . getenv ( \"TARGET\" ) ?: \"World\" val port = System . getenv ( \"PORT\" ) ?: \"8080\" embeddedServer ( Netty , port . toInt ()) { routing { get ( \"/\" ) { call . respondText ( \"Hello $ target !\\n\" , ContentType . Text . Html ) } } }. start ( wait = true ) } Switch back to hello directory Create a new file, build.gradle and copy the following setting plugins { id \"org.jetbrains.kotlin.jvm\" version \"1.4.10\" } apply plugin: 'application' mainClassName = 'com.example.hello.MainKt' jar { manifest { attributes 'Main-Class' : mainClassName } from { configurations . compile . collect { it . isDirectory () ? it : zipTree ( it ) } } } repositories { mavenCentral () } dependencies { compile \"io.ktor:ktor-server-netty:1.3.1\" } Create a file named Dockerfile and copy the code block below into it. # Use the official gradle image to create a build artifact. # https://hub.docker.com/_/gradle FROM gradle:6.7 as builder # Copy local code to the container image. COPY build.gradle . COPY src ./src # Build a release artifact. RUN gradle clean build --no-daemon # Use the Official OpenJDK image for a lean production stage of our multi-stage build. # https://hub.docker.com/_/openjdk # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM openjdk:8-jre-alpine # Copy the jar to the production image from the builder stage. COPY --from = builder /home/gradle/build/libs/gradle.jar /helloworld.jar # Run the web service on container startup. CMD [ \"java\" , \"-jar\" , \"-Djava.security.egd=file:/dev/./urandom\" , \"/helloworld.jar\" ] Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-kotlin namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-kotlin env : - name : TARGET value : \"Kotlin Sample v1\" Build and deploy this sample \u00b6 Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-kotlin . # Push the container to docker registry docker push { username } /helloworld-kotlin After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). To find the URL for your service, use kubectl get ksvc helloworld-kotlin --output = custom-columns = NAME:.metadata.name,URL:.status.url NAME URL helloworld-kotlin http://helloworld-kotlin.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-kotlin.default.1.2.3.4.xip.io Hello Kotlin Sample v1! Remove the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Kotlin"},{"location":"serving/samples/hello-world/helloworld-kotlin/#hello-world-kotlin","text":"A simple web app written in Kotlin using Ktor that you can use for testing. It reads in an env variable TARGET and prints \"Hello \\${TARGET}\". If TARGET is not specified, it will use \"World\" as the TARGET. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-kotlin","title":"Hello World - Kotlin"},{"location":"serving/samples/hello-world/helloworld-kotlin/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"serving/samples/hello-world/helloworld-kotlin/#recreating-the-sample-code","text":"Create a new directory and cd into it: mkdir hello cd hello Create a file named Main.kt at src/main/kotlin/com/example/hello and copy the code block below into it: mkdir -p src/main/kotlin/com/example/hello package com.example.hello import io.ktor.application.* import io.ktor.http.* import io.ktor.response.* import io.ktor.routing.* import io.ktor.server.engine.* import io.ktor.server.netty.* fun main ( args : Array < String > ) { val target = System . getenv ( \"TARGET\" ) ?: \"World\" val port = System . getenv ( \"PORT\" ) ?: \"8080\" embeddedServer ( Netty , port . toInt ()) { routing { get ( \"/\" ) { call . respondText ( \"Hello $ target !\\n\" , ContentType . Text . Html ) } } }. start ( wait = true ) } Switch back to hello directory Create a new file, build.gradle and copy the following setting plugins { id \"org.jetbrains.kotlin.jvm\" version \"1.4.10\" } apply plugin: 'application' mainClassName = 'com.example.hello.MainKt' jar { manifest { attributes 'Main-Class' : mainClassName } from { configurations . compile . collect { it . isDirectory () ? it : zipTree ( it ) } } } repositories { mavenCentral () } dependencies { compile \"io.ktor:ktor-server-netty:1.3.1\" } Create a file named Dockerfile and copy the code block below into it. # Use the official gradle image to create a build artifact. # https://hub.docker.com/_/gradle FROM gradle:6.7 as builder # Copy local code to the container image. COPY build.gradle . COPY src ./src # Build a release artifact. RUN gradle clean build --no-daemon # Use the Official OpenJDK image for a lean production stage of our multi-stage build. # https://hub.docker.com/_/openjdk # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM openjdk:8-jre-alpine # Copy the jar to the production image from the builder stage. COPY --from = builder /home/gradle/build/libs/gradle.jar /helloworld.jar # Run the web service on container startup. CMD [ \"java\" , \"-jar\" , \"-Djava.security.egd=file:/dev/./urandom\" , \"/helloworld.jar\" ] Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-kotlin namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-kotlin env : - name : TARGET value : \"Kotlin Sample v1\"","title":"Recreating the sample code"},{"location":"serving/samples/hello-world/helloworld-kotlin/#build-and-deploy-this-sample","text":"Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-kotlin . # Push the container to docker registry docker push { username } /helloworld-kotlin After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). To find the URL for your service, use kubectl get ksvc helloworld-kotlin --output = custom-columns = NAME:.metadata.name,URL:.status.url NAME URL helloworld-kotlin http://helloworld-kotlin.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-kotlin.default.1.2.3.4.xip.io Hello Kotlin Sample v1!","title":"Build and deploy this sample"},{"location":"serving/samples/hello-world/helloworld-kotlin/#remove-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Remove the sample app deployment"},{"location":"serving/samples/hello-world/helloworld-nodejs/","text":"Hello World - Node.js \u00b6 A simple web app written in Node.js that you can use for testing. It reads in an env variable TARGET and prints \"Hello \\${TARGET}!\". If TARGET is not specified, it will use \"World\" as the TARGET. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-nodejs Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Node.js installed and configured. Recreating the sample code \u00b6 Create a new directory and initialize npm : npm init package name: ( helloworld-nodejs ) version: ( 1 .0.0 ) description: entry point: ( index.js ) test command: git repository: keywords: author: license: ( ISC ) Apache-2.0 Install the express package: npm install express Create a new file named index.js and paste the following code: const express = require ( 'express' ); const app = express (); app . get ( '/' , ( req , res ) => { console . log ( 'Hello world received a request.' ); const target = process . env . TARGET || 'World' ; res . send ( `Hello ${ target } !\\n` ); }); const port = process . env . PORT || 8080 ; app . listen ( port , () => { console . log ( 'Hello world listening on port' , port ); }); Modify the package.json file to add a start command to the scripts section: { \"name\" : \"knative-serving-helloworld\" , \"version\" : \"1.0.0\" , \"description\" : \"Simple hello world sample in Node\" , \"main\" : \"index.js\" , \"scripts\" : { \"start\" : \"node index.js\" }, \"author\" : \"\" , \"license\" : \"Apache-2.0\" , \"dependencies\" : { \"express\" : \"^4.16.4\" } } In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Node.js app, see Dockerizing a Node.js web app . # Use the official lightweight Node.js 12 image. # https://hub.docker.com/_/node FROM node:12-slim # Create and change to the app directory. WORKDIR /usr/src/app # Copy application dependency manifests to the container image. # A wildcard is used to ensure both package.json AND package-lock.json are copied. # Copying this separately prevents re-running npm install on every code change. COPY package*.json ./ # Install production dependencies. RUN npm install --only = production # Copy local code to the container image. COPY . ./ # Run the web service on container startup. CMD [ \"npm\" , \"start\" ] Create a .dockerignore file to ensure that any files related to a local build do not affect the container that you build for deployment. Dockerfile README.md node_modules npm-debug.log Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-nodejs namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-nodejs env : - name : TARGET value : \"Node.js Sample v1\" Building and deploying the sample \u00b6 Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-nodejs . # Push the container to docker registry docker push { username } /helloworld-nodejs After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). To find the URL for your service, use kubectl get ksvc helloworld-nodejs --output=custom-columns=NAME:.metadata.name,URL:.status.url NAME URL helloworld-nodejs http://helloworld-nodejs.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-nodejs.default.1.2.3.4.xip.io Hello Node.js Sample v1! Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Node.js"},{"location":"serving/samples/hello-world/helloworld-nodejs/#hello-world-nodejs","text":"A simple web app written in Node.js that you can use for testing. It reads in an env variable TARGET and prints \"Hello \\${TARGET}!\". If TARGET is not specified, it will use \"World\" as the TARGET. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-nodejs","title":"Hello World - Node.js"},{"location":"serving/samples/hello-world/helloworld-nodejs/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Node.js installed and configured.","title":"Before you begin"},{"location":"serving/samples/hello-world/helloworld-nodejs/#recreating-the-sample-code","text":"Create a new directory and initialize npm : npm init package name: ( helloworld-nodejs ) version: ( 1 .0.0 ) description: entry point: ( index.js ) test command: git repository: keywords: author: license: ( ISC ) Apache-2.0 Install the express package: npm install express Create a new file named index.js and paste the following code: const express = require ( 'express' ); const app = express (); app . get ( '/' , ( req , res ) => { console . log ( 'Hello world received a request.' ); const target = process . env . TARGET || 'World' ; res . send ( `Hello ${ target } !\\n` ); }); const port = process . env . PORT || 8080 ; app . listen ( port , () => { console . log ( 'Hello world listening on port' , port ); }); Modify the package.json file to add a start command to the scripts section: { \"name\" : \"knative-serving-helloworld\" , \"version\" : \"1.0.0\" , \"description\" : \"Simple hello world sample in Node\" , \"main\" : \"index.js\" , \"scripts\" : { \"start\" : \"node index.js\" }, \"author\" : \"\" , \"license\" : \"Apache-2.0\" , \"dependencies\" : { \"express\" : \"^4.16.4\" } } In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Node.js app, see Dockerizing a Node.js web app . # Use the official lightweight Node.js 12 image. # https://hub.docker.com/_/node FROM node:12-slim # Create and change to the app directory. WORKDIR /usr/src/app # Copy application dependency manifests to the container image. # A wildcard is used to ensure both package.json AND package-lock.json are copied. # Copying this separately prevents re-running npm install on every code change. COPY package*.json ./ # Install production dependencies. RUN npm install --only = production # Copy local code to the container image. COPY . ./ # Run the web service on container startup. CMD [ \"npm\" , \"start\" ] Create a .dockerignore file to ensure that any files related to a local build do not affect the container that you build for deployment. Dockerfile README.md node_modules npm-debug.log Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-nodejs namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-nodejs env : - name : TARGET value : \"Node.js Sample v1\"","title":"Recreating the sample code"},{"location":"serving/samples/hello-world/helloworld-nodejs/#building-and-deploying-the-sample","text":"Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-nodejs . # Push the container to docker registry docker push { username } /helloworld-nodejs After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). To find the URL for your service, use kubectl get ksvc helloworld-nodejs --output=custom-columns=NAME:.metadata.name,URL:.status.url NAME URL helloworld-nodejs http://helloworld-nodejs.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-nodejs.default.1.2.3.4.xip.io Hello Node.js Sample v1!","title":"Building and deploying the sample"},{"location":"serving/samples/hello-world/helloworld-nodejs/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Removing the sample app deployment"},{"location":"serving/samples/hello-world/helloworld-php/","text":"Hello World - PHP \u00b6 A simple web app written in PHP that you can use for testing. It reads in an env variable TARGET and prints Hello ${TARGET}! . If TARGET is not specified, it will use World as the TARGET . Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-php Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Recreating the sample code \u00b6 Create a new directory and cd into it: mkdir app cd app Create a file named index.php and copy the code block below into it: <?php $target = getenv ( 'TARGET' , true ) ?: 'World' ; echo sprintf ( \"Hello %s! \\n \" , $target ); ?> Create a file named Dockerfile and copy the code block below into it. See official PHP docker image for more details. # Use the official PHP 7.3 image. # https://hub.docker.com/_/php FROM php:7.3-apache # Copy local code to the container image. COPY index.php /var/www/html/ # Use the PORT environment variable in Apache configuration files. RUN sed -i 's/80/${PORT}/g' /etc/apache2/sites-available/000-default.conf /etc/apache2/ports.conf # Configure PHP for development. # Switch to the production php.ini for production operations. # RUN mv \"$PHP_INI_DIR/php.ini-production\" \"$PHP_INI_DIR/php.ini\" # https://hub.docker.com/_/php#configuration RUN mv \" $PHP_INI_DIR /php.ini-development\" \" $PHP_INI_DIR /php.ini\" Create a .dockerignore file to ensure that any files related to a local build do not affect the container that you build for deployment. README.md Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-php namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-php env : - name : TARGET value : \"PHP Sample v1\" Building and deploying the sample \u00b6 Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. yaml Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-php . # Push the container to docker registry docker push { username } /helloworld-php After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml kn With kn you can deploy the service with kn service create helloworld-php --image = docker.io/ { username } /helloworld-php --env TARGET = \"Ruby Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. The output will look like: ``` Creating service 'helloworld-php' in namespace 'default': 0.035s The Configuration is still working to reflect the latest desired specification. 0.139s The Route is still working to reflect the latest desired specification. 0.250s Configuration \"helloworld-php\" is waiting for a Revision to become ready. 8.040s ... 8.136s Ingress has not yet been reconciled. 8.277s unsuccessfully observed a new generation 8.398s Ready to serve. Service 'helloworld-php' created to latest revision 'helloworld-php-akhft-1' is available at URL: http://helloworld-php.default.1.2.3.4.xip.io ``` Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and a load balancer for your app. Automatically scale your pods up and down (including to zero active pods). To find the URL for your service, use kubectl kubectl get ksvc helloworld-php --output=custom-columns=NAME:.metadata.name,URL:.status.url NAME URL helloworld-php http://helloworld-php.default.1.2.3.4.xip.io kn kn service describe helloworld-php -o url Example: http://helloworld-php.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-php.default.1.2.3.4.xip.io Hello PHP Sample v1! Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record. kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-php","title":"PHP"},{"location":"serving/samples/hello-world/helloworld-php/#hello-world-php","text":"A simple web app written in PHP that you can use for testing. It reads in an env variable TARGET and prints Hello ${TARGET}! . If TARGET is not specified, it will use World as the TARGET . Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-php","title":"Hello World - PHP"},{"location":"serving/samples/hello-world/helloworld-php/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"serving/samples/hello-world/helloworld-php/#recreating-the-sample-code","text":"Create a new directory and cd into it: mkdir app cd app Create a file named index.php and copy the code block below into it: <?php $target = getenv ( 'TARGET' , true ) ?: 'World' ; echo sprintf ( \"Hello %s! \\n \" , $target ); ?> Create a file named Dockerfile and copy the code block below into it. See official PHP docker image for more details. # Use the official PHP 7.3 image. # https://hub.docker.com/_/php FROM php:7.3-apache # Copy local code to the container image. COPY index.php /var/www/html/ # Use the PORT environment variable in Apache configuration files. RUN sed -i 's/80/${PORT}/g' /etc/apache2/sites-available/000-default.conf /etc/apache2/ports.conf # Configure PHP for development. # Switch to the production php.ini for production operations. # RUN mv \"$PHP_INI_DIR/php.ini-production\" \"$PHP_INI_DIR/php.ini\" # https://hub.docker.com/_/php#configuration RUN mv \" $PHP_INI_DIR /php.ini-development\" \" $PHP_INI_DIR /php.ini\" Create a .dockerignore file to ensure that any files related to a local build do not affect the container that you build for deployment. README.md Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-php namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-php env : - name : TARGET value : \"PHP Sample v1\"","title":"Recreating the sample code"},{"location":"serving/samples/hello-world/helloworld-php/#building-and-deploying-the-sample","text":"Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. yaml Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-php . # Push the container to docker registry docker push { username } /helloworld-php After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml kn With kn you can deploy the service with kn service create helloworld-php --image = docker.io/ { username } /helloworld-php --env TARGET = \"Ruby Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. The output will look like: ``` Creating service 'helloworld-php' in namespace 'default': 0.035s The Configuration is still working to reflect the latest desired specification. 0.139s The Route is still working to reflect the latest desired specification. 0.250s Configuration \"helloworld-php\" is waiting for a Revision to become ready. 8.040s ... 8.136s Ingress has not yet been reconciled. 8.277s unsuccessfully observed a new generation 8.398s Ready to serve. Service 'helloworld-php' created to latest revision 'helloworld-php-akhft-1' is available at URL: http://helloworld-php.default.1.2.3.4.xip.io ``` Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and a load balancer for your app. Automatically scale your pods up and down (including to zero active pods). To find the URL for your service, use kubectl kubectl get ksvc helloworld-php --output=custom-columns=NAME:.metadata.name,URL:.status.url NAME URL helloworld-php http://helloworld-php.default.1.2.3.4.xip.io kn kn service describe helloworld-php -o url Example: http://helloworld-php.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-php.default.1.2.3.4.xip.io Hello PHP Sample v1!","title":"Building and deploying the sample"},{"location":"serving/samples/hello-world/helloworld-php/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record. kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-php","title":"Removing the sample app deployment"},{"location":"serving/samples/hello-world/helloworld-python/","text":"Hello World - Python \u00b6 This guide describes the steps required to create the helloworld-python sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-python Prerequisites \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions . Docker installed and running on your local machine, and a Docker Hub account configured. (optional) The Knative CLI client kn can be used to simplify the deployment. Alternatively, you can use kubectl , and apply resource files directly. Build \u00b6 Create a new directory and cd into it: mkdir app cd app Create a file named app.py and copy the code block below into it: import os from flask import Flask app = Flask ( __name__ ) @app . route ( '/' ) def hello_world (): target = os . environ . get ( 'TARGET' , 'World' ) return 'Hello {} ! \\n ' . format ( target ) if __name__ == \"__main__\" : app . run ( debug = True , host = '0.0.0.0' , port = int ( os . environ . get ( 'PORT' , 8080 ))) In your project directory, create a file named Dockerfile and copy the code block below into it. See official Python docker image for more details. # Use the official lightweight Python image. # https://hub.docker.com/_/python FROM python:3.7-slim # Allow statements and log messages to immediately appear in the Knative logs ENV PYTHONUNBUFFERED True # Copy local code to the container image. ENV APP_HOME /app WORKDIR $APP_HOME COPY . ./ # Install production dependencies. RUN pip install Flask gunicorn # Run the web service on container startup. Here we use the gunicorn # webserver, with one worker process and 8 threads. # For environments with multiple CPU cores, increase the number of workers # to be equal to the cores available. CMD exec gunicorn --bind : $PORT --workers 1 --threads 8 --timeout 0 app:app Create a .dockerignore file to ensure that any files related to a local build do not affect the container that you build for deployment. Dockerfile README.md *.pyc *.pyo *.pyd __pycache__ NOTE: Use Docker to build the sample code into a container. To build and push to Docker Hub or container registry of your choice, run these commands replacing {username} with your Docker Hub username or the URL of the container registry. Use Docker to build the sample code into a container, then push the container to the Docker registry: # Build the container on your local machine docker build -t { username } /helloworld-python . # Push the container to docker registry docker push { username } /helloworld-python Deploying the app \u00b6 After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. === \"yaml\" 1. Create a new file, `service.yaml` and copy the following service definition into the file. Make sure to replace `{username}` with your Docker Hub username or with the URL provided by your container registry ```yaml apiVersion: serving.knative.dev/v1 kind: Service metadata: name: helloworld-python namespace: default spec: template: spec: containers: - image: docker.io/{username}/helloworld-python env: - name: TARGET value: \"Python Sample v1\" ``` Ensure that the container image value in `service.yaml` matches the container you built in the previous step. Apply the configuration using `kubectl`: ```shell kubectl apply --filename service.yaml ``` kn With kn you can deploy the service with kn service create helloworld-python --image = docker.io/ { username } /helloworld-python --env TARGET = \"Python Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. During the creation of your service, Knative performs the following steps: Creates a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scales your pods up and down, including scaling down to zero active pods. Verification \u00b6 Run one of the followings commands to find the domain URL for your service. Note: If your URL includes example.com then consult the setup instructions for configuring DNS (e.g. with xip.io ), or using a Custom Domain . === \"kubectl\" ```shell kubectl get ksvc helloworld-python --output=custom-columns=NAME:.metadata.name,URL:.status.url ``` Example: ```shell NAME URL helloworld-python http://helloworld-python.default.1.2.3.4.xip.io ``` kn kn service describe helloworld-python -o url Example: http://helloworld-python.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. Example: curl http://helloworld-python.default.1.2.3.4.xip.io Hello Python Sample v1! # Even easier with kn: curl $( kn service describe helloworld-python -o url ) Note: Add -v option to get more detail if the curl command failed. Removing \u00b6 To remove the sample app from your cluster, delete the service record. kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-python","title":"Python"},{"location":"serving/samples/hello-world/helloworld-python/#hello-world-python","text":"This guide describes the steps required to create the helloworld-python sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-python","title":"Hello World - Python"},{"location":"serving/samples/hello-world/helloworld-python/#prerequisites","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions . Docker installed and running on your local machine, and a Docker Hub account configured. (optional) The Knative CLI client kn can be used to simplify the deployment. Alternatively, you can use kubectl , and apply resource files directly.","title":"Prerequisites"},{"location":"serving/samples/hello-world/helloworld-python/#build","text":"Create a new directory and cd into it: mkdir app cd app Create a file named app.py and copy the code block below into it: import os from flask import Flask app = Flask ( __name__ ) @app . route ( '/' ) def hello_world (): target = os . environ . get ( 'TARGET' , 'World' ) return 'Hello {} ! \\n ' . format ( target ) if __name__ == \"__main__\" : app . run ( debug = True , host = '0.0.0.0' , port = int ( os . environ . get ( 'PORT' , 8080 ))) In your project directory, create a file named Dockerfile and copy the code block below into it. See official Python docker image for more details. # Use the official lightweight Python image. # https://hub.docker.com/_/python FROM python:3.7-slim # Allow statements and log messages to immediately appear in the Knative logs ENV PYTHONUNBUFFERED True # Copy local code to the container image. ENV APP_HOME /app WORKDIR $APP_HOME COPY . ./ # Install production dependencies. RUN pip install Flask gunicorn # Run the web service on container startup. Here we use the gunicorn # webserver, with one worker process and 8 threads. # For environments with multiple CPU cores, increase the number of workers # to be equal to the cores available. CMD exec gunicorn --bind : $PORT --workers 1 --threads 8 --timeout 0 app:app Create a .dockerignore file to ensure that any files related to a local build do not affect the container that you build for deployment. Dockerfile README.md *.pyc *.pyo *.pyd __pycache__ NOTE: Use Docker to build the sample code into a container. To build and push to Docker Hub or container registry of your choice, run these commands replacing {username} with your Docker Hub username or the URL of the container registry. Use Docker to build the sample code into a container, then push the container to the Docker registry: # Build the container on your local machine docker build -t { username } /helloworld-python . # Push the container to docker registry docker push { username } /helloworld-python","title":"Build"},{"location":"serving/samples/hello-world/helloworld-python/#deploying-the-app","text":"After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. === \"yaml\" 1. Create a new file, `service.yaml` and copy the following service definition into the file. Make sure to replace `{username}` with your Docker Hub username or with the URL provided by your container registry ```yaml apiVersion: serving.knative.dev/v1 kind: Service metadata: name: helloworld-python namespace: default spec: template: spec: containers: - image: docker.io/{username}/helloworld-python env: - name: TARGET value: \"Python Sample v1\" ``` Ensure that the container image value in `service.yaml` matches the container you built in the previous step. Apply the configuration using `kubectl`: ```shell kubectl apply --filename service.yaml ``` kn With kn you can deploy the service with kn service create helloworld-python --image = docker.io/ { username } /helloworld-python --env TARGET = \"Python Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. During the creation of your service, Knative performs the following steps: Creates a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scales your pods up and down, including scaling down to zero active pods.","title":"Deploying the app"},{"location":"serving/samples/hello-world/helloworld-python/#verification","text":"Run one of the followings commands to find the domain URL for your service. Note: If your URL includes example.com then consult the setup instructions for configuring DNS (e.g. with xip.io ), or using a Custom Domain . === \"kubectl\" ```shell kubectl get ksvc helloworld-python --output=custom-columns=NAME:.metadata.name,URL:.status.url ``` Example: ```shell NAME URL helloworld-python http://helloworld-python.default.1.2.3.4.xip.io ``` kn kn service describe helloworld-python -o url Example: http://helloworld-python.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. Example: curl http://helloworld-python.default.1.2.3.4.xip.io Hello Python Sample v1! # Even easier with kn: curl $( kn service describe helloworld-python -o url ) Note: Add -v option to get more detail if the curl command failed.","title":"Verification"},{"location":"serving/samples/hello-world/helloworld-python/#removing","text":"To remove the sample app from your cluster, delete the service record. kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-python","title":"Removing"},{"location":"serving/samples/hello-world/helloworld-ruby/","text":"Hello World - Ruby \u00b6 This guide describes the steps required to create the helloworld-ruby sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-ruby Prerequisites \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions . Docker installed and running on your local machine, and a Docker Hub account configured. (optional) The Knative CLI client kn that simplifies the deployment. Alternative you can also use kubectl and apply resource files directly. Recreating the sample code \u00b6 Create a new directory and cd into it: mkdir app cd app Create a file named app.rb and copy the code block below into it: require 'sinatra' set :bind , '0.0.0.0' get '/' do target = ENV [ 'TARGET' ] || 'World' \"Hello #{ target } ! \\n \" end Create a file named Dockerfile and copy the code block below into it. See official Ruby docker image for more details. # Use the official lightweight Ruby image. # https://hub.docker.com/_/ruby FROM ruby:2.6-slim # Install production dependencies. WORKDIR /usr/src/app COPY Gemfile Gemfile.lock ./ ENV BUNDLE_FROZEN = true RUN gem install bundler && bundle install # Copy local code to the container image. COPY . ./ # Run the web service on container startup. CMD [ \"ruby\" , \"./app.rb\" ] Create a file named Gemfile and copy the text block below into it. source 'https://rubygems.org' gem 'sinatra' gem 'rack', '>= 2.0.6' Run bundle. If you don't have bundler installed, copy the Gemfile.lock to your working directory. bundle install Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-ruby namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-ruby env : - name : TARGET value : \"Ruby Sample v1\" Deploying \u00b6 After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. yaml Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. ```yaml apiVersion: serving.knative.dev/v1 kind: Service metadata: name: helloworld-ruby namespace: default spec: template: spec: containers: - image: docker.io/{username}/helloworld-ruby env: - name: TARGET value: \"Ruby Sample v1\" ``` Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml kn With kn you can deploy the service with kn service create helloworld-ruby --image = docker.io/ { username } /helloworld-ruby --env TARGET = \"Ruby Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. The output will look like: ``` Creating service 'helloworld-ruby' in namespace 'default': 0.035s The Configuration is still working to reflect the latest desired specification. 0.139s The Route is still working to reflect the latest desired specification. 0.250s Configuration \"helloworld-ruby\" is waiting for a Revision to become ready. 8.040s ... 8.136s Ingress has not yet been reconciled. 8.277s unsuccessfully observed a new generation 8.398s Ready to serve. Service 'helloworld-ruby' created to latest revision 'helloworld-ruby-akhft-1' is available at URL: http://helloworld-ruby.default.1.2.3.4.xip.io ``` During the creation of your service, Knative performs the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). Verification \u00b6 Run one of the followings commands to find the domain URL for your service. kubectl kubectl get ksvc helloworld-ruby --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-ruby http://helloworld-ruby.default.1.2.3.4.xip.io kn kn service describe helloworld-ruby -o url Example: http://helloworld-ruby.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. Example: curl http://helloworld-ruby.default.1.2.3.4.xip.io Hello Ruby Sample v1! # Even easier with kn: curl $( kn service describe helloworld-ruby -o url ) Note: Add -v option to get more detail if the curl command failed. Removing \u00b6 To remove the sample app from your cluster, delete the service record. kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-ruby","title":"Ruby"},{"location":"serving/samples/hello-world/helloworld-ruby/#hello-world-ruby","text":"This guide describes the steps required to create the helloworld-ruby sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-ruby","title":"Hello World - Ruby"},{"location":"serving/samples/hello-world/helloworld-ruby/#prerequisites","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions . Docker installed and running on your local machine, and a Docker Hub account configured. (optional) The Knative CLI client kn that simplifies the deployment. Alternative you can also use kubectl and apply resource files directly.","title":"Prerequisites"},{"location":"serving/samples/hello-world/helloworld-ruby/#recreating-the-sample-code","text":"Create a new directory and cd into it: mkdir app cd app Create a file named app.rb and copy the code block below into it: require 'sinatra' set :bind , '0.0.0.0' get '/' do target = ENV [ 'TARGET' ] || 'World' \"Hello #{ target } ! \\n \" end Create a file named Dockerfile and copy the code block below into it. See official Ruby docker image for more details. # Use the official lightweight Ruby image. # https://hub.docker.com/_/ruby FROM ruby:2.6-slim # Install production dependencies. WORKDIR /usr/src/app COPY Gemfile Gemfile.lock ./ ENV BUNDLE_FROZEN = true RUN gem install bundler && bundle install # Copy local code to the container image. COPY . ./ # Run the web service on container startup. CMD [ \"ruby\" , \"./app.rb\" ] Create a file named Gemfile and copy the text block below into it. source 'https://rubygems.org' gem 'sinatra' gem 'rack', '>= 2.0.6' Run bundle. If you don't have bundler installed, copy the Gemfile.lock to your working directory. bundle install Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-ruby namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-ruby env : - name : TARGET value : \"Ruby Sample v1\"","title":"Recreating the sample code"},{"location":"serving/samples/hello-world/helloworld-ruby/#deploying","text":"After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. yaml Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. ```yaml apiVersion: serving.knative.dev/v1 kind: Service metadata: name: helloworld-ruby namespace: default spec: template: spec: containers: - image: docker.io/{username}/helloworld-ruby env: - name: TARGET value: \"Ruby Sample v1\" ``` Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml kn With kn you can deploy the service with kn service create helloworld-ruby --image = docker.io/ { username } /helloworld-ruby --env TARGET = \"Ruby Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. The output will look like: ``` Creating service 'helloworld-ruby' in namespace 'default': 0.035s The Configuration is still working to reflect the latest desired specification. 0.139s The Route is still working to reflect the latest desired specification. 0.250s Configuration \"helloworld-ruby\" is waiting for a Revision to become ready. 8.040s ... 8.136s Ingress has not yet been reconciled. 8.277s unsuccessfully observed a new generation 8.398s Ready to serve. Service 'helloworld-ruby' created to latest revision 'helloworld-ruby-akhft-1' is available at URL: http://helloworld-ruby.default.1.2.3.4.xip.io ``` During the creation of your service, Knative performs the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods).","title":"Deploying"},{"location":"serving/samples/hello-world/helloworld-ruby/#verification","text":"Run one of the followings commands to find the domain URL for your service. kubectl kubectl get ksvc helloworld-ruby --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-ruby http://helloworld-ruby.default.1.2.3.4.xip.io kn kn service describe helloworld-ruby -o url Example: http://helloworld-ruby.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. Example: curl http://helloworld-ruby.default.1.2.3.4.xip.io Hello Ruby Sample v1! # Even easier with kn: curl $( kn service describe helloworld-ruby -o url ) Note: Add -v option to get more detail if the curl command failed.","title":"Verification"},{"location":"serving/samples/hello-world/helloworld-ruby/#removing","text":"To remove the sample app from your cluster, delete the service record. kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-ruby","title":"Removing"},{"location":"serving/samples/hello-world/helloworld-scala/","text":"Hello World - Scala using Akka HTTP \u00b6 A microservice which demonstrates how to get set up and running with Knative Serving when using Scala and Akka HTTP . It will respond to a HTTP request with a text specified as an ENV variable named MESSAGE , defaulting to \"Hello World!\" . Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-scala Before you begin \u00b6 A Kubernetes cluster installation with Knative Serving up and running. Docker installed locally, and running, optionally a Docker Hub account configured or some other Docker Repository installed locally. Java JDK8 or later installed locally. Scala's standard build tool sbt installed locally. Configuring the sbt build \u00b6 If you want to use your Docker Hub repository, set the repository to \"docker.io/yourusername/yourreponame\". If you use Minikube, you first need to run: eval $( minikube docker-env ) If want to use the Docker Repository inside Minikube, either set this to \"dev.local\" or if you want to use another repository name, then you need to run the following command after docker:publishLocal : docker tag yourreponame/helloworld-scala:<version> dev.local/helloworld-scala:<version> Otherwise Knative Serving won't be able to resolve this image from the Minikube Docker Repository. You specify the repository in build.sbt : dockerRepository := Some ( \"your_repository_name\" ) You can learn more about the build configuration syntax here . Configuring the Service descriptor \u00b6 Importantly, in helloworld-scala.yaml change the image reference to match up with the repository , name, and version specified in the build.sbt in the previous section. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-scala namespace : default spec : template : spec : containers : - image : \"your_repository_name/helloworld-scala:0.0.1\" env : - name : MESSAGE value : \"Scala & Akka on Knative says hello!\" - name : HOST value : \"localhost\" Publishing to Docker \u00b6 In order to build the project and create and push the Docker image, run either: sbt docker:publishLocal or sbt docker:publish Which of them to use is depending on whether you are publishing to a remote or a local Docker Repository. Deploying to Knative Serving \u00b6 yaml Apply the Service yaml definition : kubectl apply --filename helloworld-scala.yaml kn With kn you can deploy the service with kn service create helloworld-scala --image = docker.io/ { username } /helloworld-scala --env TARGET = \"Scala Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. The output will look like: ``` Creating service 'helloworld-scala' in namespace 'default': 0.035s The Configuration is still working to reflect the latest desired specification. 0.139s The Route is still working to reflect the latest desired specification. 0.250s Configuration \"helloworld-scala\" is waiting for a Revision to become ready. 8.040s ... 8.136s Ingress has not yet been reconciled. 8.277s unsuccessfully observed a new generation 8.398s Ready to serve. Service 'helloworld-scala' created to latest revision 'helloworld-scala-abcd-1' is available at URL: http://helloworld-scala.default.1.2.3.4.xip.io ``` kubectl Then find the service host: kubectl get ksvc helloworld-scala \\ --output = custom-columns = NAME:.metadata.name,URL:.status.url # It will print something like this, the URL is what you're looking for. # NAME URL # helloworld-scala http://helloworld-scala.default.1.2.3.4.xip.io Finally, to try your service, use the obtained URL: curl -v http://helloworld-scala.default.1.2.3.4.xip.io kn kn service describe helloworld-scala -o url Example: http://helloworld-scala.default.1.2.3.4.xip.io Finally, to try your service, use the obtained URL: curl -v http://helloworld-scala.default.1.2.3.4.xip.io Cleanup \u00b6 kubectl kubectl delete --filename helloworld-scala.yaml kubetl delete --filename helloworld-scala.yaml kn kn service delete helloworld-scala","title":"Scala"},{"location":"serving/samples/hello-world/helloworld-scala/#hello-world-scala-using-akka-http","text":"A microservice which demonstrates how to get set up and running with Knative Serving when using Scala and Akka HTTP . It will respond to a HTTP request with a text specified as an ENV variable named MESSAGE , defaulting to \"Hello World!\" . Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-scala","title":"Hello World - Scala using Akka HTTP"},{"location":"serving/samples/hello-world/helloworld-scala/#before-you-begin","text":"A Kubernetes cluster installation with Knative Serving up and running. Docker installed locally, and running, optionally a Docker Hub account configured or some other Docker Repository installed locally. Java JDK8 or later installed locally. Scala's standard build tool sbt installed locally.","title":"Before you begin"},{"location":"serving/samples/hello-world/helloworld-scala/#configuring-the-sbt-build","text":"If you want to use your Docker Hub repository, set the repository to \"docker.io/yourusername/yourreponame\". If you use Minikube, you first need to run: eval $( minikube docker-env ) If want to use the Docker Repository inside Minikube, either set this to \"dev.local\" or if you want to use another repository name, then you need to run the following command after docker:publishLocal : docker tag yourreponame/helloworld-scala:<version> dev.local/helloworld-scala:<version> Otherwise Knative Serving won't be able to resolve this image from the Minikube Docker Repository. You specify the repository in build.sbt : dockerRepository := Some ( \"your_repository_name\" ) You can learn more about the build configuration syntax here .","title":"Configuring the sbt build"},{"location":"serving/samples/hello-world/helloworld-scala/#configuring-the-service-descriptor","text":"Importantly, in helloworld-scala.yaml change the image reference to match up with the repository , name, and version specified in the build.sbt in the previous section. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-scala namespace : default spec : template : spec : containers : - image : \"your_repository_name/helloworld-scala:0.0.1\" env : - name : MESSAGE value : \"Scala & Akka on Knative says hello!\" - name : HOST value : \"localhost\"","title":"Configuring the Service descriptor"},{"location":"serving/samples/hello-world/helloworld-scala/#publishing-to-docker","text":"In order to build the project and create and push the Docker image, run either: sbt docker:publishLocal or sbt docker:publish Which of them to use is depending on whether you are publishing to a remote or a local Docker Repository.","title":"Publishing to Docker"},{"location":"serving/samples/hello-world/helloworld-scala/#deploying-to-knative-serving","text":"yaml Apply the Service yaml definition : kubectl apply --filename helloworld-scala.yaml kn With kn you can deploy the service with kn service create helloworld-scala --image = docker.io/ { username } /helloworld-scala --env TARGET = \"Scala Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. The output will look like: ``` Creating service 'helloworld-scala' in namespace 'default': 0.035s The Configuration is still working to reflect the latest desired specification. 0.139s The Route is still working to reflect the latest desired specification. 0.250s Configuration \"helloworld-scala\" is waiting for a Revision to become ready. 8.040s ... 8.136s Ingress has not yet been reconciled. 8.277s unsuccessfully observed a new generation 8.398s Ready to serve. Service 'helloworld-scala' created to latest revision 'helloworld-scala-abcd-1' is available at URL: http://helloworld-scala.default.1.2.3.4.xip.io ``` kubectl Then find the service host: kubectl get ksvc helloworld-scala \\ --output = custom-columns = NAME:.metadata.name,URL:.status.url # It will print something like this, the URL is what you're looking for. # NAME URL # helloworld-scala http://helloworld-scala.default.1.2.3.4.xip.io Finally, to try your service, use the obtained URL: curl -v http://helloworld-scala.default.1.2.3.4.xip.io kn kn service describe helloworld-scala -o url Example: http://helloworld-scala.default.1.2.3.4.xip.io Finally, to try your service, use the obtained URL: curl -v http://helloworld-scala.default.1.2.3.4.xip.io","title":"Deploying to Knative Serving"},{"location":"serving/samples/hello-world/helloworld-scala/#cleanup","text":"kubectl kubectl delete --filename helloworld-scala.yaml kubetl delete --filename helloworld-scala.yaml kn kn service delete helloworld-scala","title":"Cleanup"},{"location":"serving/samples/hello-world/helloworld-shell/","text":"Hello World - Shell \u00b6 This guide describes the steps required to create the helloworld-shell sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-shell Prerequisites \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions . Docker installed and running on your local machine, and a Docker Hub account configured. Optional: You can use the Knative CLI client kn to simplify resource creation and deployment. Alternatively, you can use kubectl to apply resource files directly. Building \u00b6 Create a new file named script.sh and paste the script below. This will run BusyBox' http returning a friendly welcome message as plain/text plus some extra information: #!/bin/sh # Print out CGI header # See https://tools.ietf.org/html/draft-robinson-www-interface-00 # for the full CGI specification echo -e \"Content-Type: text/plain\\n\" # Use environment variable TARGET or \"World\" if not set echo \"Hello ${ TARGET :=World } !\" # In this script you can perform more dynamic actions, too. # Like printing the date, checking CGI environment variables, ... Create a new file named Dockerfile and copy the code block below into it. # Busybox image that contains the simple 'httpd' # https://git.busybox.net/busybox/tree/networking/httpd.c FROM busybox # Serve from this directory WORKDIR /var/www # Prepare httpd command for being started via init # This indirection is required for proper SIGTERM handling RUN echo \"::sysinit:httpd -vv -p 8080 -u daemon -h /var/www\" > /etc/inittab # Copy over our CGI script and make it executable COPY --chown = daemon:daemon script.sh cgi-bin/index.cgi RUN chmod 755 cgi-bin/index.cgi # Startup init which in turn starts httpd CMD init Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-shell . # Push the container to docker registry docker push { username } /helloworld-shell Deploying \u00b6 After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. yaml Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-shell namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-shell env : - name : TARGET value : \"Shell Sample v1\" Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml kn With kn you can deploy the service with kn service create helloworld-shell --image = docker.io/ { username } /helloworld-shell --env TARGET = \"Shell Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. The output will look like: ``` Creating service 'helloworld-shell' in namespace 'default': 0.035s The Configuration is still working to reflect the latest desired specification. 0.139s The Route is still working to reflect the latest desired specification. 0.250s Configuration \"helloworld-shell\" is waiting for a Revision to become ready. 8.040s ... 8.136s Ingress has not yet been reconciled. 8.277s unsuccessfully observed a new generation 8.398s Ready to serve. Service 'helloworld-shell' created to latest revision 'helloworld-shell-kwdpt-1' is available at URL: http://helloworld-shell.default.1.2.3.4.xip.io ``` During the creation of your service, Knative performs the following steps: Creates of a new immutable revision for this version of the app. Programs the network to create a route, ingress, service, and load balance for your app. Automatically scales your pods up and down (including to zero active pods). Verification \u00b6 Run one of the followings commands to find the domain URL for your service. kubectl kubectl get ksvc helloworld-shell --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-shell http://helloworld-shell.default.1.2.3.4.xip.io kn kn service describe helloworld-shell -o url Example: http://helloworld-shell.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. Example: curl http://helloworld-shell.default.1.2.3.4.xip.io Hello Shell Sample v1! # Even easier with kn: curl $( kn service describe helloworld-shell -o url ) Note: Add -v option to get more details if the curl command failed. Removing \u00b6 To remove the sample app from your cluster, delete the service record. kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-shell","title":"Shell"},{"location":"serving/samples/hello-world/helloworld-shell/#hello-world-shell","text":"This guide describes the steps required to create the helloworld-shell sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-shell","title":"Hello World - Shell"},{"location":"serving/samples/hello-world/helloworld-shell/#prerequisites","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions . Docker installed and running on your local machine, and a Docker Hub account configured. Optional: You can use the Knative CLI client kn to simplify resource creation and deployment. Alternatively, you can use kubectl to apply resource files directly.","title":"Prerequisites"},{"location":"serving/samples/hello-world/helloworld-shell/#building","text":"Create a new file named script.sh and paste the script below. This will run BusyBox' http returning a friendly welcome message as plain/text plus some extra information: #!/bin/sh # Print out CGI header # See https://tools.ietf.org/html/draft-robinson-www-interface-00 # for the full CGI specification echo -e \"Content-Type: text/plain\\n\" # Use environment variable TARGET or \"World\" if not set echo \"Hello ${ TARGET :=World } !\" # In this script you can perform more dynamic actions, too. # Like printing the date, checking CGI environment variables, ... Create a new file named Dockerfile and copy the code block below into it. # Busybox image that contains the simple 'httpd' # https://git.busybox.net/busybox/tree/networking/httpd.c FROM busybox # Serve from this directory WORKDIR /var/www # Prepare httpd command for being started via init # This indirection is required for proper SIGTERM handling RUN echo \"::sysinit:httpd -vv -p 8080 -u daemon -h /var/www\" > /etc/inittab # Copy over our CGI script and make it executable COPY --chown = daemon:daemon script.sh cgi-bin/index.cgi RUN chmod 755 cgi-bin/index.cgi # Startup init which in turn starts httpd CMD init Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-shell . # Push the container to docker registry docker push { username } /helloworld-shell","title":"Building"},{"location":"serving/samples/hello-world/helloworld-shell/#deploying","text":"After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. yaml Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-shell namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-shell env : - name : TARGET value : \"Shell Sample v1\" Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml kn With kn you can deploy the service with kn service create helloworld-shell --image = docker.io/ { username } /helloworld-shell --env TARGET = \"Shell Sample v1\" This will wait until your service is deployed and ready, and ultimately it will print the URL through which you can access the service. The output will look like: ``` Creating service 'helloworld-shell' in namespace 'default': 0.035s The Configuration is still working to reflect the latest desired specification. 0.139s The Route is still working to reflect the latest desired specification. 0.250s Configuration \"helloworld-shell\" is waiting for a Revision to become ready. 8.040s ... 8.136s Ingress has not yet been reconciled. 8.277s unsuccessfully observed a new generation 8.398s Ready to serve. Service 'helloworld-shell' created to latest revision 'helloworld-shell-kwdpt-1' is available at URL: http://helloworld-shell.default.1.2.3.4.xip.io ``` During the creation of your service, Knative performs the following steps: Creates of a new immutable revision for this version of the app. Programs the network to create a route, ingress, service, and load balance for your app. Automatically scales your pods up and down (including to zero active pods).","title":"Deploying"},{"location":"serving/samples/hello-world/helloworld-shell/#verification","text":"Run one of the followings commands to find the domain URL for your service. kubectl kubectl get ksvc helloworld-shell --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-shell http://helloworld-shell.default.1.2.3.4.xip.io kn kn service describe helloworld-shell -o url Example: http://helloworld-shell.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. Example: curl http://helloworld-shell.default.1.2.3.4.xip.io Hello Shell Sample v1! # Even easier with kn: curl $( kn service describe helloworld-shell -o url ) Note: Add -v option to get more details if the curl command failed.","title":"Verification"},{"location":"serving/samples/hello-world/helloworld-shell/#removing","text":"To remove the sample app from your cluster, delete the service record. kubectl kubectl delete --filename service.yaml kn kn service delete helloworld-shell","title":"Removing"},{"location":"serving/samples/knative-routing-go/","text":"Routing across multiple Knative services - Go \u00b6 This example shows how to map multiple Knative services to different paths under a single domain name using the Istio VirtualService concept. Istio is a general-purpose reverse proxy, therefore these directions can also be used to configure routing based on other request data such as headers, or even to map Knative and external resources under the same domain name. In this sample, we set up two web services: Search service and Login service, which simply read in an env variable SERVICE_NAME and prints \"${SERVICE_NAME} is called\" . We'll then create a VirtualService with host example.com , and define routing rules in the VirtualService so that example.com/search maps to the Search service, and example.com/login maps to the Login service. Prerequisites \u00b6 A Kubernetes cluster with Knative Serving installed. Install Docker . Acquire a domain name. In this example, we use example.com . If you don't have a domain name, you can modify your hosts file (on Mac or Linux) to map example.com to your cluster's ingress IP. If you have configured a custom domain for your Knative installation, we will refer to it as in the rest of this document Check out the code: go get -d github.com/knative/docs/docs/serving/samples/knative-routing-go Setup \u00b6 To check the domain name, run the following command: kubectl get cm -n knative-serving config-domain -o yaml Then, check the value for data . The domain name should be in the format of <YOUR_DOMAIN_NAME>: \"\" , if it is available. Build the application container and publish it to a container registry: Move into the sample directory: cd $GOPATH /src/github.com/knative/docs Set your preferred container registry: If you use Google Container Registry (GCR), you will need to enable the GCR API in your GCP project. export REPO = \"gcr.io/<YOUR_PROJECT_ID>\" If you use Docker Hub as your docker image registry, replace with your dockerhub username and run the following command: export REPO = \"docker.io/<username>\" Use Docker to build your application container: docker build \\ --tag \"${REPO}/knative-routing-go\" \\ --file=docs/serving/samples/knative-routing-go/Dockerfile . Push your container to a container registry: docker push \"${REPO}/knative-routing-go\" Replace the image reference path with our published image path in the configuration file docs/serving/samples/knative-routing-go/sample.yaml : Manually replace: image: github.com/knative/docs/docs/serving/samples/knative-routing-go with image: ${REPO}/knative-routing-go If you manually changed the .yaml file, you must replace \\${REPO} with the correct path on your local machine. Or Run this command: perl -pi -e \"s@github.com/knative/docs/docs/serving/samples@${REPO}@g\" docs/serving/samples/knative-routing-go/sample.yaml Deploy the Service \u00b6 Deploy the Knative Serving sample: kubectl apply --filename docs/serving/samples/knative-routing-go/sample.yaml Exploring the Routes \u00b6 A shared Gateway knative-ingress-gateway is used within Knative service mesh for serving all incoming traffic. You can inspect it and its corresponding Kubernetes service with: Check the shared Gateway: kubectl get Gateway --namespace knative-serving --output yaml Check the corresponding Kubernetes service for the shared Gateway: INGRESSGATEWAY=istio-ingressgateway kubectl get svc $INGRESSGATEWAY --namespace istio-system --output yaml Inspect the deployed Knative services with: kubectl get ksvc You should see 2 Knative services: search-service and login-service . Access the Services \u00b6 Find the shared Gateway IP and export as an environment variable: INGRESSGATEWAY = istio-ingressgateway export GATEWAY_IP = ` kubectl get svc $INGRESSGATEWAY --namespace istio-system \\ --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\" ` Find the Search service URL with: # kubectl get route search-service --output=custom-columns=NAME:.metadata.name,URL:.status.url NAME URL search-service http://search-service.default.example.com Make a curl request to the service: curl http:// ${ GATEWAY_IP } --header \"Host:search-service.default.example.com\" You should see: Search Service is called ! Similarly, you can also directly access \"Login\" service with: curl http:// ${ GATEWAY_IP } --header \"Host:login-service.default.example.com\" You should see: Login Service is called ! Apply Custom Routing Rule \u00b6 Apply the custom routing rules defined in routing.yaml file with: kubectl apply --filename docs/serving/samples/knative-routing-go/routing.yaml If you have configured a custom domain name for your service, please replace all mentions of \"example.com\" in routing.yaml with \" \" for spec.hosts and spec.http.rewrite.authority. In addition, you need to verify how your domain template is defined. By default, we use the format of {{.Name}}.{{.Namespace}}, like search-service.default and login-service.default. However, some Knative environments may use other format like {{.Name}}-{{.Namespace}}. You can find out the format by running the command: kubectl get cm -n knative-serving config-network -o yaml Then look for the value for domainTemplate . If it is {{.Name}}-{{.Namespace}}.{{.Domain}} , you need to change search-service.default into search-service-default and login-service.default into login-service-default as well in routing.yaml . The routing.yaml file will generate a new VirtualService entry-route for domain example.com or your own domain name. View the VirtualService: kubectl get VirtualService entry-route --output yaml Send a request to the Search service and the Login service by using corresponding URIs. You should get the same results as directly accessing these services. Get the ingress IP: INGRESSGATEWAY = istio-ingressgateway export GATEWAY_IP = ` kubectl get svc $INGRESSGATEWAY --namespace istio-system \\ --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\" ` Send a request to the Search service: curl http:// ${ GATEWAY_IP } /search --header \"Host: example.com\" or curl http:// ${ GATEWAY_IP } /search --header \"Host: <YOUR_DOMAIN_NAME>\" for the case using your own domain. Send a request to the Login service: curl http:// ${ GATEWAY_IP } /login --header \"Host: example.com\" or curl http:// ${ GATEWAY_IP } /login --header \"Host: <YOUR_DOMAIN_NAME>\" for the case using your own domain. How It Works \u00b6 When an external request with host example.com or your own domain name reaches knative-ingress-gateway Gateway, the entry-route VirtualService will check if it has /search or /login URI. If the URI matches, then the host of request will be rewritten into the host of Search service or Login service correspondingly. This resets the final destination of the request. The request with updated host will be forwarded to knative-ingress-gateway Gateway again. The Gateway proxy checks the updated host, and forwards it to Search or Login service according to its host setting. Clean Up \u00b6 To clean up the sample resources: kubectl delete --filename docs/serving/samples/knative-routing-go/sample.yaml kubectl delete --filename docs/serving/samples/knative-routing-go/routing.yaml","title":"Routing services - Go"},{"location":"serving/samples/knative-routing-go/#routing-across-multiple-knative-services-go","text":"This example shows how to map multiple Knative services to different paths under a single domain name using the Istio VirtualService concept. Istio is a general-purpose reverse proxy, therefore these directions can also be used to configure routing based on other request data such as headers, or even to map Knative and external resources under the same domain name. In this sample, we set up two web services: Search service and Login service, which simply read in an env variable SERVICE_NAME and prints \"${SERVICE_NAME} is called\" . We'll then create a VirtualService with host example.com , and define routing rules in the VirtualService so that example.com/search maps to the Search service, and example.com/login maps to the Login service.","title":"Routing across multiple Knative services - Go"},{"location":"serving/samples/knative-routing-go/#prerequisites","text":"A Kubernetes cluster with Knative Serving installed. Install Docker . Acquire a domain name. In this example, we use example.com . If you don't have a domain name, you can modify your hosts file (on Mac or Linux) to map example.com to your cluster's ingress IP. If you have configured a custom domain for your Knative installation, we will refer to it as in the rest of this document Check out the code: go get -d github.com/knative/docs/docs/serving/samples/knative-routing-go","title":"Prerequisites"},{"location":"serving/samples/knative-routing-go/#setup","text":"To check the domain name, run the following command: kubectl get cm -n knative-serving config-domain -o yaml Then, check the value for data . The domain name should be in the format of <YOUR_DOMAIN_NAME>: \"\" , if it is available. Build the application container and publish it to a container registry: Move into the sample directory: cd $GOPATH /src/github.com/knative/docs Set your preferred container registry: If you use Google Container Registry (GCR), you will need to enable the GCR API in your GCP project. export REPO = \"gcr.io/<YOUR_PROJECT_ID>\" If you use Docker Hub as your docker image registry, replace with your dockerhub username and run the following command: export REPO = \"docker.io/<username>\" Use Docker to build your application container: docker build \\ --tag \"${REPO}/knative-routing-go\" \\ --file=docs/serving/samples/knative-routing-go/Dockerfile . Push your container to a container registry: docker push \"${REPO}/knative-routing-go\" Replace the image reference path with our published image path in the configuration file docs/serving/samples/knative-routing-go/sample.yaml : Manually replace: image: github.com/knative/docs/docs/serving/samples/knative-routing-go with image: ${REPO}/knative-routing-go If you manually changed the .yaml file, you must replace \\${REPO} with the correct path on your local machine. Or Run this command: perl -pi -e \"s@github.com/knative/docs/docs/serving/samples@${REPO}@g\" docs/serving/samples/knative-routing-go/sample.yaml","title":"Setup"},{"location":"serving/samples/knative-routing-go/#deploy-the-service","text":"Deploy the Knative Serving sample: kubectl apply --filename docs/serving/samples/knative-routing-go/sample.yaml","title":"Deploy the Service"},{"location":"serving/samples/knative-routing-go/#exploring-the-routes","text":"A shared Gateway knative-ingress-gateway is used within Knative service mesh for serving all incoming traffic. You can inspect it and its corresponding Kubernetes service with: Check the shared Gateway: kubectl get Gateway --namespace knative-serving --output yaml Check the corresponding Kubernetes service for the shared Gateway: INGRESSGATEWAY=istio-ingressgateway kubectl get svc $INGRESSGATEWAY --namespace istio-system --output yaml Inspect the deployed Knative services with: kubectl get ksvc You should see 2 Knative services: search-service and login-service .","title":"Exploring the Routes"},{"location":"serving/samples/knative-routing-go/#access-the-services","text":"Find the shared Gateway IP and export as an environment variable: INGRESSGATEWAY = istio-ingressgateway export GATEWAY_IP = ` kubectl get svc $INGRESSGATEWAY --namespace istio-system \\ --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\" ` Find the Search service URL with: # kubectl get route search-service --output=custom-columns=NAME:.metadata.name,URL:.status.url NAME URL search-service http://search-service.default.example.com Make a curl request to the service: curl http:// ${ GATEWAY_IP } --header \"Host:search-service.default.example.com\" You should see: Search Service is called ! Similarly, you can also directly access \"Login\" service with: curl http:// ${ GATEWAY_IP } --header \"Host:login-service.default.example.com\" You should see: Login Service is called !","title":"Access the Services"},{"location":"serving/samples/knative-routing-go/#apply-custom-routing-rule","text":"Apply the custom routing rules defined in routing.yaml file with: kubectl apply --filename docs/serving/samples/knative-routing-go/routing.yaml If you have configured a custom domain name for your service, please replace all mentions of \"example.com\" in routing.yaml with \" \" for spec.hosts and spec.http.rewrite.authority. In addition, you need to verify how your domain template is defined. By default, we use the format of {{.Name}}.{{.Namespace}}, like search-service.default and login-service.default. However, some Knative environments may use other format like {{.Name}}-{{.Namespace}}. You can find out the format by running the command: kubectl get cm -n knative-serving config-network -o yaml Then look for the value for domainTemplate . If it is {{.Name}}-{{.Namespace}}.{{.Domain}} , you need to change search-service.default into search-service-default and login-service.default into login-service-default as well in routing.yaml . The routing.yaml file will generate a new VirtualService entry-route for domain example.com or your own domain name. View the VirtualService: kubectl get VirtualService entry-route --output yaml Send a request to the Search service and the Login service by using corresponding URIs. You should get the same results as directly accessing these services. Get the ingress IP: INGRESSGATEWAY = istio-ingressgateway export GATEWAY_IP = ` kubectl get svc $INGRESSGATEWAY --namespace istio-system \\ --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\" ` Send a request to the Search service: curl http:// ${ GATEWAY_IP } /search --header \"Host: example.com\" or curl http:// ${ GATEWAY_IP } /search --header \"Host: <YOUR_DOMAIN_NAME>\" for the case using your own domain. Send a request to the Login service: curl http:// ${ GATEWAY_IP } /login --header \"Host: example.com\" or curl http:// ${ GATEWAY_IP } /login --header \"Host: <YOUR_DOMAIN_NAME>\" for the case using your own domain.","title":"Apply Custom Routing Rule"},{"location":"serving/samples/knative-routing-go/#how-it-works","text":"When an external request with host example.com or your own domain name reaches knative-ingress-gateway Gateway, the entry-route VirtualService will check if it has /search or /login URI. If the URI matches, then the host of request will be rewritten into the host of Search service or Login service correspondingly. This resets the final destination of the request. The request with updated host will be forwarded to knative-ingress-gateway Gateway again. The Gateway proxy checks the updated host, and forwards it to Search or Login service according to its host setting.","title":"How It Works"},{"location":"serving/samples/knative-routing-go/#clean-up","text":"To clean up the sample resources: kubectl delete --filename docs/serving/samples/knative-routing-go/sample.yaml kubectl delete --filename docs/serving/samples/knative-routing-go/routing.yaml","title":"Clean Up"},{"location":"serving/samples/multi-container/","text":"Knative multi-container samples \u00b6 A simple web app written in Go that you can use for multi container testing. Prerequisites \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Make sure multi-container flag is enabled as part of config-features configmap. The following steps show how you can use the sample code and deploy the app to your cluster. You can download a working copy of the sample, by entering the following command: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs Using the sample code \u00b6 To test multi container functionality, you must create two containers: a serving container, and a sidecar container. The multi-container directory is provided in the sample code, and contains predefined code and dockerfiles for creating the containers. You can update the default files and YAML by using the steps outlined in this section. Serving Container \u00b6 After you have cloned the sample repository, navigate to the servingcontainer directory: cd knative-docs/docs/serving/samples/multi-container/servingcontainer 1. Create a basic web server which listens on port 8881. You can do this by copying the following code into the servingcontainer.go file: package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" ) func handler ( w http . ResponseWriter , r * http . Request ) { log . Println ( \"serving container received a request.\" ) res , err := http . Get ( \"http://127.0.0.1:8882\" ) if err != nil { log . Fatal ( err ) } resp , err := ioutil . ReadAll ( res . Body ) if err != nil { log . Fatal ( err ) } fmt . Fprintln ( w , string ( resp )) } func main () { log . Print ( \"serving container started...\" ) http . HandleFunc ( \"/\" , handler ) log . Fatal ( http . ListenAndServe ( \":8881\" , nil )) } 1. Copy the following code into the Dockerfile file: # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. # https://hub.docker.com/_/golang FROM golang:1.15 as builder # Create and change to the app directory. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod=readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o servingcontainer # Use the official Alpine image for a lean production container. # https://hub.docker.com/_/alpine # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/servingcontainer /servingcontainer # Run the web service on container startup. CMD [ \"/servingcontainer\" ] Sidecar Container \u00b6 After you have cloned the sample repository, navigate to the sidecarcontainer directory: cd - cd knative-docs/docs/serving/samples/multi-container/sidecarcontainer Create a basic web server which listens on port 8882. You can do this by copying the following code into the sidecarcontainer.go file: package main import ( \"fmt\" \"log\" \"net/http\" ) func handler ( w http . ResponseWriter , r * http . Request ) { log . Println ( \"sidecar container received a request.\" ) fmt . Fprintln ( w , \"Yay!! multi-container works\" ) } func main () { log . Print ( \"sidecar container started...\" ) http . HandleFunc ( \"/\" , handler ) log . Fatal ( http . ListenAndServe ( \":8882\" , nil )) } Copy the following code into the Dockerfile file: # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. # https://hub.docker.com/_/golang FROM golang:1.15 as builder # Create and change to the app directory. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod=readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o sidecarcontainer # Use the official Alpine image for a lean production container. # https://hub.docker.com/_/alpine # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/sidecarcontainer /sidecarcontainer # Run the web service on container startup. CMD [ \"/sidecarcontainer\" ] Writing Knative Service YAML \u00b6 After you have cloned the sample repository, navigate to the multi-container directory: cd - cd knative-docs/docs/serving/samples/multi-container/ Copy the following YAML service definition into the service.yaml file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : multi-container namespace : default spec : template : spec : containers : - image : docker.io/{username}/servingcontainer ports : - containerPort : 8881 - image : docker.io/{username}/sidecarcontainer NOTE: Replace {username} with your Docker Hub username. Use Go tool to create a go.mod manifest: servingcontainer cd - cd knative-docs/docs/serving/samples/multi-container/servingcontainer go mod init github.com/knative/docs/docs/serving/samples/multi-container/servingcontainer sidecarcontainer cd - cd knative-docs/docs/serving/samples/multi-container/sidecarcontainer go mod init github.com/knative/docs/docs/serving/samples/multi-container/sidecarcontainer Building and deploying the sample \u00b6 After you have modified the sample code files you can build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine cd - cd knative-docs/docs/serving/samples/multi-container/servingcontainer docker build -t { username } /servingcontainer . cd - cd knative-docs/docs/serving/samples/multi-container/sidecarcontainer docker build -t { username } /sidecarcontainer . # Push the container to docker registry docker push { username } /servingcontainer docker push { username } /sidecarcontainer After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : cd - cd knative-docs/docs/serving/samples/multi-container kubectl apply --filename service.yaml Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). Run the following command to find the domain URL for your service: kubectl get ksvc multi-container --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL multi-container http://multi-container.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://multi-container.default.1.2.3.4.xip.io Yay!! multi-container works Note: Add -v option to get more detail if the curl command failed. Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"multi-container samples"},{"location":"serving/samples/multi-container/#knative-multi-container-samples","text":"A simple web app written in Go that you can use for multi container testing.","title":"Knative multi-container samples"},{"location":"serving/samples/multi-container/#prerequisites","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Make sure multi-container flag is enabled as part of config-features configmap. The following steps show how you can use the sample code and deploy the app to your cluster. You can download a working copy of the sample, by entering the following command: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs","title":"Prerequisites"},{"location":"serving/samples/multi-container/#using-the-sample-code","text":"To test multi container functionality, you must create two containers: a serving container, and a sidecar container. The multi-container directory is provided in the sample code, and contains predefined code and dockerfiles for creating the containers. You can update the default files and YAML by using the steps outlined in this section.","title":"Using the sample code"},{"location":"serving/samples/multi-container/#serving-container","text":"After you have cloned the sample repository, navigate to the servingcontainer directory: cd knative-docs/docs/serving/samples/multi-container/servingcontainer 1. Create a basic web server which listens on port 8881. You can do this by copying the following code into the servingcontainer.go file: package main import ( \"fmt\" \"io/ioutil\" \"log\" \"net/http\" ) func handler ( w http . ResponseWriter , r * http . Request ) { log . Println ( \"serving container received a request.\" ) res , err := http . Get ( \"http://127.0.0.1:8882\" ) if err != nil { log . Fatal ( err ) } resp , err := ioutil . ReadAll ( res . Body ) if err != nil { log . Fatal ( err ) } fmt . Fprintln ( w , string ( resp )) } func main () { log . Print ( \"serving container started...\" ) http . HandleFunc ( \"/\" , handler ) log . Fatal ( http . ListenAndServe ( \":8881\" , nil )) } 1. Copy the following code into the Dockerfile file: # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. # https://hub.docker.com/_/golang FROM golang:1.15 as builder # Create and change to the app directory. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod=readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o servingcontainer # Use the official Alpine image for a lean production container. # https://hub.docker.com/_/alpine # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/servingcontainer /servingcontainer # Run the web service on container startup. CMD [ \"/servingcontainer\" ]","title":"Serving Container"},{"location":"serving/samples/multi-container/#sidecar-container","text":"After you have cloned the sample repository, navigate to the sidecarcontainer directory: cd - cd knative-docs/docs/serving/samples/multi-container/sidecarcontainer Create a basic web server which listens on port 8882. You can do this by copying the following code into the sidecarcontainer.go file: package main import ( \"fmt\" \"log\" \"net/http\" ) func handler ( w http . ResponseWriter , r * http . Request ) { log . Println ( \"sidecar container received a request.\" ) fmt . Fprintln ( w , \"Yay!! multi-container works\" ) } func main () { log . Print ( \"sidecar container started...\" ) http . HandleFunc ( \"/\" , handler ) log . Fatal ( http . ListenAndServe ( \":8882\" , nil )) } Copy the following code into the Dockerfile file: # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. # https://hub.docker.com/_/golang FROM golang:1.15 as builder # Create and change to the app directory. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod=readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o sidecarcontainer # Use the official Alpine image for a lean production container. # https://hub.docker.com/_/alpine # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/sidecarcontainer /sidecarcontainer # Run the web service on container startup. CMD [ \"/sidecarcontainer\" ]","title":"Sidecar Container"},{"location":"serving/samples/multi-container/#writing-knative-service-yaml","text":"After you have cloned the sample repository, navigate to the multi-container directory: cd - cd knative-docs/docs/serving/samples/multi-container/ Copy the following YAML service definition into the service.yaml file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : multi-container namespace : default spec : template : spec : containers : - image : docker.io/{username}/servingcontainer ports : - containerPort : 8881 - image : docker.io/{username}/sidecarcontainer NOTE: Replace {username} with your Docker Hub username. Use Go tool to create a go.mod manifest: servingcontainer cd - cd knative-docs/docs/serving/samples/multi-container/servingcontainer go mod init github.com/knative/docs/docs/serving/samples/multi-container/servingcontainer sidecarcontainer cd - cd knative-docs/docs/serving/samples/multi-container/sidecarcontainer go mod init github.com/knative/docs/docs/serving/samples/multi-container/sidecarcontainer","title":"Writing Knative Service YAML"},{"location":"serving/samples/multi-container/#building-and-deploying-the-sample","text":"After you have modified the sample code files you can build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine cd - cd knative-docs/docs/serving/samples/multi-container/servingcontainer docker build -t { username } /servingcontainer . cd - cd knative-docs/docs/serving/samples/multi-container/sidecarcontainer docker build -t { username } /sidecarcontainer . # Push the container to docker registry docker push { username } /servingcontainer docker push { username } /sidecarcontainer After the build has completed and the container is pushed to Docker Hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : cd - cd knative-docs/docs/serving/samples/multi-container kubectl apply --filename service.yaml Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). Run the following command to find the domain URL for your service: kubectl get ksvc multi-container --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL multi-container http://multi-container.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://multi-container.default.1.2.3.4.xip.io Yay!! multi-container works Note: Add -v option to get more detail if the curl command failed.","title":"Building and deploying the sample"},{"location":"serving/samples/multi-container/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Removing the sample app deployment"},{"location":"serving/samples/rest-api-go/","text":"Creating a RESTful Service - Go \u00b6 This \"stock ticker\" sample demonstrates how to create and run a simple RESTful service on Knative Serving. The exposed endpoint outputs the stock price for a given \" stock symbol \", like AAPL , AMZN , GOOG , MSFT , etc. Prerequisites \u00b6 A Kubernetes cluster with Knative Serving installed and DNS configured. Docker installed locally. envsubst installed locally. This is installed by the gettext package. If not installed it can be installed by a Linux package manager, or by Homebrew on OS X. Download a copy of the code: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs Setup \u00b6 In order to run an application on Knative Serving a container image must be available to fetch from a container registry. This sample uses Docker for both building and pushing. To build and push to a container registry using Docker: From the knative-docs directory, run the following command to set your container registry endpoint as an environment variable. This sample uses Google Container Registry (GCR) : ```shell export REPO=\"gcr.io/<YOUR_PROJECT_ID>\" ``` Set up your container registry to make sure you are ready to push. To push to GCR, you need to: Create a Google Cloud Platform project . Enable the Google Container Registry API . Setup an auth helper to give the Docker client the permissions it needs to push. If you are using a different container registry, you will want to follow the registry specific instructions for both setup and authorizing the image push. Use Docker to build your application container: docker build \\ --tag \" ${ REPO } /rest-api-go\" \\ --file docs/serving/samples/rest-api-go/Dockerfile . Push your container to a container registry: docker push \" ${ REPO } /rest-api-go\" Substitute the image reference path in the template with our published image path. The command below substitutes using the \\${REPO} variable into a new file called docs/serving/samples/rest-api-go/sample.yaml . envsubst < docs/serving/samples/rest-api-go/sample-template.yaml > \\ docs/serving/samples/rest-api-go/sample.yaml Deploy the Service \u00b6 Now that our image is available from the container registry, we can deploy the Knative Serving sample: kubectl apply --filename docs/serving/samples/rest-api-go/sample.yaml The above command creates a Knative Service within your Kubernetes cluster in the default namespace. Explore the Service \u00b6 The Knative Service creates the following child resources: Knative Route Knative Configuration Knative Revision Kubernetes Deployment Kubernetes Service You can inspect the created resources with the following kubectl commands: View the created Service resource: kubectl get ksvc stock-service-example --output yaml View the created Route resource: kubectl get route -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Kubernetes Service created by the Route kubectl get service -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the created Configuration resource: kubectl get configuration -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Revision that was created by our Configuration: kubectl get revision -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Deployment created by our Revision kubectl get deployment -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml Access the Service \u00b6 To access this service and run the stock ticker, you first obtain the service URL, and then you run curl commands to send request with your stock symbol. Get the URL of the service: kubectl get ksvc stock-service-example --output = custom-columns = NAME:.metadata.name,URL:.status.url NAME URL stock-service-example http://stock-service-example.default.1.2.3.4.xip.io Send requests to the service using curl : Send a request to the index endpoint: curl http://stock-service-example.default.1.2.3.4.xip.io Response body: Welcome to the stock app! Send a request to the /stock endpoint: curl http://stock-service-example.default.1.2.3.4.xip.io/stock Response body: stock ticker not found!, require /stock/{ticker} Send a request to the /stock endpoint with your \" stock symbol \": curl http://stock-service-example.default.1.2.3.4.xip.io/stock/<SYMBOL> where <SYMBOL> is your \"stock symbol\". Response body: stock price for ticker <SYMBOL> is <PRICE> Example Request: curl http://stock-service-example.default.1.2.3.4.xip.io/stock/FAKE Response: stock price for ticker FAKE is 0.00 Next Steps \u00b6 The traffic splitting example continues from here to walk you through how to create new Revisions and then use traffic splitting between those Revisions. Clean Up \u00b6 To clean up the sample Service: kubectl delete --filename docs/serving/samples/rest-api-go/sample.yaml","title":"RESTful service - Go"},{"location":"serving/samples/rest-api-go/#creating-a-restful-service-go","text":"This \"stock ticker\" sample demonstrates how to create and run a simple RESTful service on Knative Serving. The exposed endpoint outputs the stock price for a given \" stock symbol \", like AAPL , AMZN , GOOG , MSFT , etc.","title":"Creating a RESTful Service - Go"},{"location":"serving/samples/rest-api-go/#prerequisites","text":"A Kubernetes cluster with Knative Serving installed and DNS configured. Docker installed locally. envsubst installed locally. This is installed by the gettext package. If not installed it can be installed by a Linux package manager, or by Homebrew on OS X. Download a copy of the code: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs","title":"Prerequisites"},{"location":"serving/samples/rest-api-go/#setup","text":"In order to run an application on Knative Serving a container image must be available to fetch from a container registry. This sample uses Docker for both building and pushing. To build and push to a container registry using Docker: From the knative-docs directory, run the following command to set your container registry endpoint as an environment variable. This sample uses Google Container Registry (GCR) : ```shell export REPO=\"gcr.io/<YOUR_PROJECT_ID>\" ``` Set up your container registry to make sure you are ready to push. To push to GCR, you need to: Create a Google Cloud Platform project . Enable the Google Container Registry API . Setup an auth helper to give the Docker client the permissions it needs to push. If you are using a different container registry, you will want to follow the registry specific instructions for both setup and authorizing the image push. Use Docker to build your application container: docker build \\ --tag \" ${ REPO } /rest-api-go\" \\ --file docs/serving/samples/rest-api-go/Dockerfile . Push your container to a container registry: docker push \" ${ REPO } /rest-api-go\" Substitute the image reference path in the template with our published image path. The command below substitutes using the \\${REPO} variable into a new file called docs/serving/samples/rest-api-go/sample.yaml . envsubst < docs/serving/samples/rest-api-go/sample-template.yaml > \\ docs/serving/samples/rest-api-go/sample.yaml","title":"Setup"},{"location":"serving/samples/rest-api-go/#deploy-the-service","text":"Now that our image is available from the container registry, we can deploy the Knative Serving sample: kubectl apply --filename docs/serving/samples/rest-api-go/sample.yaml The above command creates a Knative Service within your Kubernetes cluster in the default namespace.","title":"Deploy the Service"},{"location":"serving/samples/rest-api-go/#explore-the-service","text":"The Knative Service creates the following child resources: Knative Route Knative Configuration Knative Revision Kubernetes Deployment Kubernetes Service You can inspect the created resources with the following kubectl commands: View the created Service resource: kubectl get ksvc stock-service-example --output yaml View the created Route resource: kubectl get route -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Kubernetes Service created by the Route kubectl get service -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the created Configuration resource: kubectl get configuration -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Revision that was created by our Configuration: kubectl get revision -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Deployment created by our Revision kubectl get deployment -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml","title":"Explore the Service"},{"location":"serving/samples/rest-api-go/#access-the-service","text":"To access this service and run the stock ticker, you first obtain the service URL, and then you run curl commands to send request with your stock symbol. Get the URL of the service: kubectl get ksvc stock-service-example --output = custom-columns = NAME:.metadata.name,URL:.status.url NAME URL stock-service-example http://stock-service-example.default.1.2.3.4.xip.io Send requests to the service using curl : Send a request to the index endpoint: curl http://stock-service-example.default.1.2.3.4.xip.io Response body: Welcome to the stock app! Send a request to the /stock endpoint: curl http://stock-service-example.default.1.2.3.4.xip.io/stock Response body: stock ticker not found!, require /stock/{ticker} Send a request to the /stock endpoint with your \" stock symbol \": curl http://stock-service-example.default.1.2.3.4.xip.io/stock/<SYMBOL> where <SYMBOL> is your \"stock symbol\". Response body: stock price for ticker <SYMBOL> is <PRICE> Example Request: curl http://stock-service-example.default.1.2.3.4.xip.io/stock/FAKE Response: stock price for ticker FAKE is 0.00","title":"Access the Service"},{"location":"serving/samples/rest-api-go/#next-steps","text":"The traffic splitting example continues from here to walk you through how to create new Revisions and then use traffic splitting between those Revisions.","title":"Next Steps"},{"location":"serving/samples/rest-api-go/#clean-up","text":"To clean up the sample Service: kubectl delete --filename docs/serving/samples/rest-api-go/sample.yaml","title":"Clean Up"},{"location":"serving/samples/secrets-go/","text":"Knative Secrets - Go \u00b6 A simple web app written in Go that you can use for testing. It demonstrates how to use a Kubernetes secret as a Volume with Knative. We will create a new Google Service Account and place it into a Kubernetes secret, then we will mount it into a container as a Volume. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/secrets-go Before you begin \u00b6 A Kubernetes cluster with Knative installed. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Create a Google Cloud project and install the gcloud CLI and run gcloud auth login . This sample will use a mix of gcloud and kubectl commands. The rest of the sample assumes that you've set the $PROJECT_ID environment variable to your Google Cloud project id, and also set your project ID as default using gcloud config set project $PROJECT_ID . Recreating the sample code \u00b6 Create a new file named secrets.go and paste the following code. This code creates a basic web server which listens on port 8080: package main import ( \"context\" \"fmt\" \"log\" \"net/http\" \"os\" \"cloud.google.com/go/storage\" ) func main () { log . Print ( \"Secrets sample started.\" ) // This sets up the standard GCS storage client, which will pull // credentials from GOOGLE_APPLICATION_CREDENTIALS if specified. ctx := context . Background () client , err := storage . NewClient ( ctx ) if err != nil { log . Fatalf ( \"Unable to initialize storage client: %v\" , err ) } http . HandleFunc ( \"/\" , func ( w http . ResponseWriter , r * http . Request ) { // This GCS bucket has been configured so that any authenticated // user can access it (Read Only), so any Service Account can // run this sample. bkt := client . Bucket ( \"knative-secrets-sample\" ) // Access the attributes of this GCS bucket, and write it back to the // user. On failure, return a 500 and the error message. attrs , err := bkt . Attrs ( ctx ) if err != nil { http . Error ( w , err . Error (), http . StatusInternalServerError ) return } fmt . Fprintln ( w , fmt . Sprintf ( \"bucket %s, created at %s, is located in %s with storage class %s\\n\" , attrs . Name , attrs . Created , attrs . Location , attrs . StorageClass )) }) port := os . Getenv ( \"PORT\" ) if port == \"\" { port = \"8080\" } log . Fatal ( http . ListenAndServe ( fmt . Sprintf ( \":%s\" , port ), nil )) } In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Go app, see Deploying Go servers with Docker . # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. # https://hub.docker.com/_/golang FROM golang as builder # Copy local code to the container image. WORKDIR /go/src/github.com/knative/docs/hellosecrets COPY . . # Build the output command inside the container. RUN CGO_ENABLED = 0 GOOS = linux go build -v -o hellosecrets # Use a Docker multi-stage build to create a lean production image. # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine # Enable the use of outbound https RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /go/src/github.com/knative/docs/hellosecrets/hellosecrets /hellosecrets # Service must listen to $PORT environment variable. # This default value facilitates local development. ENV PORT 8080 # Run the web service on container startup. CMD [ \"/hellosecrets\" ] Create a new Google Service Account . This Service Account doesn't need any privileges, the GCS bucket has been configured so that any authenticated identity may read it. gcloud iam service-accounts create knative-secrets Create a new JSON key for this account gcloud iam service-accounts keys create robot.json \\ --iam-account = knative-secrets@ $PROJECT_ID .iam.gserviceaccount.com Create a new Kubernetes secret from this JSON key: kubectl create secret generic google-robot-secret --from-file = ./robot.json You can achieve a similar result by editting secret.yaml , copying the contents of robot.json as instructed there, and running kubectl apply --filename secret.yaml . Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : secrets-go namespace : default spec : template : spec : containers : # Replace {username} with your DockerHub username - image : docker.io/{username}/secrets-go env : # This directs the Google Cloud SDK to use the identity and project # defined by the Service Account (aka robot) in the JSON file at # this path. # - `/var/secret` is determined by the `volumeMounts[0].mountPath` # below. This can be changed if both places are changed. # - `robot.json` is determined by the \"key\" that is used to hold the # secret content in the Kubernetes secret. This can be changed # if both places are changed. - name : GOOGLE_APPLICATION_CREDENTIALS value : /var/secret/robot.json # This section specified where in the container we want the # volume containing our secret to be mounted. volumeMounts : - name : robot-secret mountPath : /var/secret # This section attaches the secret \"google-robot-secret\" to # the Pod holding the user container. volumes : - name : robot-secret secret : secretName : google-robot-secret Building and deploying the sample \u00b6 Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /secrets-go . # Push the container to docker registry docker push { username } /secrets-go After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). Run the following command to find the domain URL for your service: kubectl get ksvc secrets-go --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL secrets-go http://secrets-go.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://secrets-go.default.1.2.3.4.xip.io bucket knative-secrets-sample, created at 2019 -02-01 14 :44:05.804 +0000 UTC, is located in US with storage class MULTI_REGIONAL Note: Add -v option to get more detail if the curl command failed. Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml kubectl delete secret google-robot-secret","title":"Secrets - Go"},{"location":"serving/samples/secrets-go/#knative-secrets-go","text":"A simple web app written in Go that you can use for testing. It demonstrates how to use a Kubernetes secret as a Volume with Knative. We will create a new Google Service Account and place it into a Kubernetes secret, then we will mount it into a container as a Volume. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.23.0\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/secrets-go","title":"Knative Secrets - Go"},{"location":"serving/samples/secrets-go/#before-you-begin","text":"A Kubernetes cluster with Knative installed. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Create a Google Cloud project and install the gcloud CLI and run gcloud auth login . This sample will use a mix of gcloud and kubectl commands. The rest of the sample assumes that you've set the $PROJECT_ID environment variable to your Google Cloud project id, and also set your project ID as default using gcloud config set project $PROJECT_ID .","title":"Before you begin"},{"location":"serving/samples/secrets-go/#recreating-the-sample-code","text":"Create a new file named secrets.go and paste the following code. This code creates a basic web server which listens on port 8080: package main import ( \"context\" \"fmt\" \"log\" \"net/http\" \"os\" \"cloud.google.com/go/storage\" ) func main () { log . Print ( \"Secrets sample started.\" ) // This sets up the standard GCS storage client, which will pull // credentials from GOOGLE_APPLICATION_CREDENTIALS if specified. ctx := context . Background () client , err := storage . NewClient ( ctx ) if err != nil { log . Fatalf ( \"Unable to initialize storage client: %v\" , err ) } http . HandleFunc ( \"/\" , func ( w http . ResponseWriter , r * http . Request ) { // This GCS bucket has been configured so that any authenticated // user can access it (Read Only), so any Service Account can // run this sample. bkt := client . Bucket ( \"knative-secrets-sample\" ) // Access the attributes of this GCS bucket, and write it back to the // user. On failure, return a 500 and the error message. attrs , err := bkt . Attrs ( ctx ) if err != nil { http . Error ( w , err . Error (), http . StatusInternalServerError ) return } fmt . Fprintln ( w , fmt . Sprintf ( \"bucket %s, created at %s, is located in %s with storage class %s\\n\" , attrs . Name , attrs . Created , attrs . Location , attrs . StorageClass )) }) port := os . Getenv ( \"PORT\" ) if port == \"\" { port = \"8080\" } log . Fatal ( http . ListenAndServe ( fmt . Sprintf ( \":%s\" , port ), nil )) } In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Go app, see Deploying Go servers with Docker . # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. # https://hub.docker.com/_/golang FROM golang as builder # Copy local code to the container image. WORKDIR /go/src/github.com/knative/docs/hellosecrets COPY . . # Build the output command inside the container. RUN CGO_ENABLED = 0 GOOS = linux go build -v -o hellosecrets # Use a Docker multi-stage build to create a lean production image. # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine # Enable the use of outbound https RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /go/src/github.com/knative/docs/hellosecrets/hellosecrets /hellosecrets # Service must listen to $PORT environment variable. # This default value facilitates local development. ENV PORT 8080 # Run the web service on container startup. CMD [ \"/hellosecrets\" ] Create a new Google Service Account . This Service Account doesn't need any privileges, the GCS bucket has been configured so that any authenticated identity may read it. gcloud iam service-accounts create knative-secrets Create a new JSON key for this account gcloud iam service-accounts keys create robot.json \\ --iam-account = knative-secrets@ $PROJECT_ID .iam.gserviceaccount.com Create a new Kubernetes secret from this JSON key: kubectl create secret generic google-robot-secret --from-file = ./robot.json You can achieve a similar result by editting secret.yaml , copying the contents of robot.json as instructed there, and running kubectl apply --filename secret.yaml . Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : secrets-go namespace : default spec : template : spec : containers : # Replace {username} with your DockerHub username - image : docker.io/{username}/secrets-go env : # This directs the Google Cloud SDK to use the identity and project # defined by the Service Account (aka robot) in the JSON file at # this path. # - `/var/secret` is determined by the `volumeMounts[0].mountPath` # below. This can be changed if both places are changed. # - `robot.json` is determined by the \"key\" that is used to hold the # secret content in the Kubernetes secret. This can be changed # if both places are changed. - name : GOOGLE_APPLICATION_CREDENTIALS value : /var/secret/robot.json # This section specified where in the container we want the # volume containing our secret to be mounted. volumeMounts : - name : robot-secret mountPath : /var/secret # This section attaches the secret \"google-robot-secret\" to # the Pod holding the user container. volumes : - name : robot-secret secret : secretName : google-robot-secret","title":"Recreating the sample code"},{"location":"serving/samples/secrets-go/#building-and-deploying-the-sample","text":"Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /secrets-go . # Push the container to docker registry docker push { username } /secrets-go After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Ensure that the container image value in service.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml Now that your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). Run the following command to find the domain URL for your service: kubectl get ksvc secrets-go --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL secrets-go http://secrets-go.default.1.2.3.4.xip.io Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://secrets-go.default.1.2.3.4.xip.io bucket knative-secrets-sample, created at 2019 -02-01 14 :44:05.804 +0000 UTC, is located in US with storage class MULTI_REGIONAL Note: Add -v option to get more detail if the curl command failed.","title":"Building and deploying the sample"},{"location":"serving/samples/secrets-go/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml kubectl delete secret google-robot-secret","title":"Removing the sample app deployment"},{"location":"serving/samples/tag-header-based-routing/","text":"Tag Header Based Routing \u00b6 This sample explains the use of tag header based routing. Tag Header Based Routing Feature \u00b6 Tag header based routing allows users to send requests directly to specific tagged revisions with the same URL of Knative Service. To do this, you must set the specific header Knative-Serving-Tag: {revision-tag} in the request. Currently Istio, Contour and Kourier ingress support this feature. Prerequestie \u00b6 A Knative cluster that has an ingress controller installed with Knative version 0.16 and above. Move into the docs directory: cd $GOPATH /src/github.com/knative/docs Enabling tag header based routing \u00b6 This feature is disabled by default. To enable this feature, run the following command: kubectl patch cm config-features -n knative-serving -p '{\"data\":{\"tag-header-based-routing\":\"Enabled\"}}' Build images \u00b6 Follow the instructions in helloworld-go to build the helloworld image and upload it to your container repository. Replace {username} in the sample.yaml with your Docker Hub username. Setting up the revisions with tag \u00b6 In this sample, two Revisions are created. The first Revision is tagged with rev1 . With this configuration, users can send requests directly to the first Revision using the URL of Knative Service plus the header Knative-Serving-Tag: rev1 . The Knative Service is configured to route all of the traffic to the second Revision, which means if users do not provide the Knative-Serving-Tag or they provide an incorrect value for Knative-Serving-Tag , the requests will be routed to the second Revision. Run the following command to set up the Knative Service and Revisions. kubectl apply -f docs/serving/samples/tag-header-based-routing/sample.yaml Check the created resources \u00b6 Check the two created Revisions using the following command kubectl get revisions You should see there are two Revisions: tag-header-revision-1 and tag-header-revision-2 . It may take a few minutes for the Revisions to become ready. Check the Knative Service using the following command kubectl get ksvc tag-header -oyaml You should see the following block which indicates the tag rev1 is successfully added to the first Revision. - revisionName: tag-header-revision-1 percent: 0 tag: rev1 - revisionName: tag-header-revision-2 percent: 100 Sending request with tag header \u00b6 Run the following command to send a request to the first Revision. curl ${INGRESS_IP} -H \"Host:tag-header.default.example.com\" -H \"Knative-Serving-Tag:rev1\" where ${INGRESS_IP} is the IP of your ingress. You should get the following response: Hello First Revision! Run the following command to send requests without the Knative-Serving-Tag header: curl ${INGRESS_IP} -H \"Host:tag-header.default.example.com\" You should get the response from the second Revision: Hello Second Revision! Run the following command to send requests with an incorrect Knative-Serving-Tag header: curl ${INGRESS_IP} -H \"Host:tag-header.default.example.com\" -H \"Knative-Serving-Tag:wrongHeader\" You should get the response from the second Revision: Hello Second Revision!","title":"Tag Header Based Routing"},{"location":"serving/samples/tag-header-based-routing/#tag-header-based-routing","text":"This sample explains the use of tag header based routing.","title":"Tag Header Based Routing"},{"location":"serving/samples/tag-header-based-routing/#tag-header-based-routing-feature","text":"Tag header based routing allows users to send requests directly to specific tagged revisions with the same URL of Knative Service. To do this, you must set the specific header Knative-Serving-Tag: {revision-tag} in the request. Currently Istio, Contour and Kourier ingress support this feature.","title":"Tag Header Based Routing Feature"},{"location":"serving/samples/tag-header-based-routing/#prerequestie","text":"A Knative cluster that has an ingress controller installed with Knative version 0.16 and above. Move into the docs directory: cd $GOPATH /src/github.com/knative/docs","title":"Prerequestie"},{"location":"serving/samples/tag-header-based-routing/#enabling-tag-header-based-routing","text":"This feature is disabled by default. To enable this feature, run the following command: kubectl patch cm config-features -n knative-serving -p '{\"data\":{\"tag-header-based-routing\":\"Enabled\"}}'","title":"Enabling tag header based routing"},{"location":"serving/samples/tag-header-based-routing/#build-images","text":"Follow the instructions in helloworld-go to build the helloworld image and upload it to your container repository. Replace {username} in the sample.yaml with your Docker Hub username.","title":"Build images"},{"location":"serving/samples/tag-header-based-routing/#setting-up-the-revisions-with-tag","text":"In this sample, two Revisions are created. The first Revision is tagged with rev1 . With this configuration, users can send requests directly to the first Revision using the URL of Knative Service plus the header Knative-Serving-Tag: rev1 . The Knative Service is configured to route all of the traffic to the second Revision, which means if users do not provide the Knative-Serving-Tag or they provide an incorrect value for Knative-Serving-Tag , the requests will be routed to the second Revision. Run the following command to set up the Knative Service and Revisions. kubectl apply -f docs/serving/samples/tag-header-based-routing/sample.yaml","title":"Setting up the revisions with tag"},{"location":"serving/samples/tag-header-based-routing/#check-the-created-resources","text":"Check the two created Revisions using the following command kubectl get revisions You should see there are two Revisions: tag-header-revision-1 and tag-header-revision-2 . It may take a few minutes for the Revisions to become ready. Check the Knative Service using the following command kubectl get ksvc tag-header -oyaml You should see the following block which indicates the tag rev1 is successfully added to the first Revision. - revisionName: tag-header-revision-1 percent: 0 tag: rev1 - revisionName: tag-header-revision-2 percent: 100","title":"Check the created resources"},{"location":"serving/samples/tag-header-based-routing/#sending-request-with-tag-header","text":"Run the following command to send a request to the first Revision. curl ${INGRESS_IP} -H \"Host:tag-header.default.example.com\" -H \"Knative-Serving-Tag:rev1\" where ${INGRESS_IP} is the IP of your ingress. You should get the following response: Hello First Revision! Run the following command to send requests without the Knative-Serving-Tag header: curl ${INGRESS_IP} -H \"Host:tag-header.default.example.com\" You should get the response from the second Revision: Hello Second Revision! Run the following command to send requests with an incorrect Knative-Serving-Tag header: curl ${INGRESS_IP} -H \"Host:tag-header.default.example.com\" -H \"Knative-Serving-Tag:wrongHeader\" You should get the response from the second Revision: Hello Second Revision!","title":"Sending request with tag header"},{"location":"serving/samples/traffic-splitting/","text":"Simple Traffic Splitting Between Revisions \u00b6 This samples builds off of the Creating a RESTful Service sample to illustrate updating a Service to create a new Revision as well as splitting traffic between the two created Revisions. Prerequisites \u00b6 Complete the Service creation steps in Creating a RESTful Service . Move into the docs directory: cd $GOPATH /src/github.com/knative/docs Using the traffic: block \u00b6 The service was originally created without a traffic: block, which means that it will automatically deploy the latest updates as they become ready. To split traffic between multiple Revisions, we will start to use a customized traffic: block. The traffic: block enables users to split traffic over any number of fixed Revisions, or the floating \"latest revision\" for the Service. It also enables users to name the specific sub-routes, so that they can be directly addressed for qualification or debugging. The first thing we will do is look at the traffic block that was defaulted for us in the previous sample: Fetch the state of the Service, and note the traffic: block that will run the latest ready revision, each time we update our template. Also note that under status: we see a specific revisionName: here, which is what it has resolved to (in this case the name we asked for). $ kubectl get ksvc -oyaml stock-service-example apiVersion: serving.knative.dev/v1 kind: Service metadata: name: stock-service-example ... spec: template: ... # A defaulted version of what we provided. traffic: - latestRevision: true percent: 100 status: ... traffic: - percent: 100 revisionName: stock-service-example-first The release_sample.yaml in this directory overwrites the defaulted traffic block with a block that fixes traffic to the revision stock-service-example-first , while keeping the latest ready revision available via the sub-route \"latest\". kubectl apply --filename docs/serving/samples/traffic-splitting/release_sample.yaml The spec of the Service should now show our traffic block with the Revision name we specified above. kubectl get ksvc stock-service-example --output yaml Updating the Service \u00b6 This section describes how to create a new Revision by updating your Service. A new Revision is created every time a value in the template section of the Service spec is updated. The updated_sample.yaml in this folder changes the environment variable RESOURCE from stock to share . Applying this change will result in a new Revision. For comparison, you can diff the release_sample.yaml with the updated_sample.yaml . diff serving/samples/traffic-splitting/release_sample.yaml \\ serving/samples/traffic-splitting/updated_sample.yaml Execute the command below to update Service, resulting in a new Revision. kubectl apply --filename docs/serving/samples/traffic-splitting/updated_sample.yaml With our traffic block, traffic will not shift to the new Revision automatically. However, it will be available via the URL associated with our latest sub-route. This can be verified through the Service status, by finding the entry of status.traffic for latest : kubectl get ksvc stock-service-example --output yaml The readiness of the Service can be verified through the Service Conditions. When the Service conditions report it is ready again, you can access the new Revision using the same method as found in the previous sample using the Service hostname found above. # Replace \"latest\" with whichever tag for which we want the hostname. export LATEST_HOSTNAME = ` kubectl get ksvc stock-service-example --output jsonpath = \"{.status.traffic[?(@.tag=='latest')].url}\" | cut -d '/' -f 3 ` curl --header \"Host: ${ LATEST_HOSTNAME } \" http:// ${ INGRESS_IP } Visiting the Service's domain will still hit the original Revision, since we configured it to receive 100% of our main traffic (you can also use the current sub-route). curl --header \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_IP } Traffic Splitting \u00b6 Updating the service to split traffic between the two revisions is done by extending our traffic list, and splitting the percent across them. Execute the command below to update Service, resulting in a 50/50 traffic split. kubectl apply --filename docs/serving/samples/traffic-splitting/split_sample.yaml Verify the deployment by checking the service status: kubectl get ksvc --output yaml Once updated, curl requests to the base domain should result in responses split evenly between Welcome to the share app! and Welcome to the stock app! . curl --header \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_IP } Clean Up \u00b6 To clean up the sample service: kubectl delete --filename docs/serving/samples/traffic-splitting/split_sample.yaml","title":"Traffic splitting"},{"location":"serving/samples/traffic-splitting/#simple-traffic-splitting-between-revisions","text":"This samples builds off of the Creating a RESTful Service sample to illustrate updating a Service to create a new Revision as well as splitting traffic between the two created Revisions.","title":"Simple Traffic Splitting Between Revisions"},{"location":"serving/samples/traffic-splitting/#prerequisites","text":"Complete the Service creation steps in Creating a RESTful Service . Move into the docs directory: cd $GOPATH /src/github.com/knative/docs","title":"Prerequisites"},{"location":"serving/samples/traffic-splitting/#using-the-traffic-block","text":"The service was originally created without a traffic: block, which means that it will automatically deploy the latest updates as they become ready. To split traffic between multiple Revisions, we will start to use a customized traffic: block. The traffic: block enables users to split traffic over any number of fixed Revisions, or the floating \"latest revision\" for the Service. It also enables users to name the specific sub-routes, so that they can be directly addressed for qualification or debugging. The first thing we will do is look at the traffic block that was defaulted for us in the previous sample: Fetch the state of the Service, and note the traffic: block that will run the latest ready revision, each time we update our template. Also note that under status: we see a specific revisionName: here, which is what it has resolved to (in this case the name we asked for). $ kubectl get ksvc -oyaml stock-service-example apiVersion: serving.knative.dev/v1 kind: Service metadata: name: stock-service-example ... spec: template: ... # A defaulted version of what we provided. traffic: - latestRevision: true percent: 100 status: ... traffic: - percent: 100 revisionName: stock-service-example-first The release_sample.yaml in this directory overwrites the defaulted traffic block with a block that fixes traffic to the revision stock-service-example-first , while keeping the latest ready revision available via the sub-route \"latest\". kubectl apply --filename docs/serving/samples/traffic-splitting/release_sample.yaml The spec of the Service should now show our traffic block with the Revision name we specified above. kubectl get ksvc stock-service-example --output yaml","title":"Using the traffic: block"},{"location":"serving/samples/traffic-splitting/#updating-the-service","text":"This section describes how to create a new Revision by updating your Service. A new Revision is created every time a value in the template section of the Service spec is updated. The updated_sample.yaml in this folder changes the environment variable RESOURCE from stock to share . Applying this change will result in a new Revision. For comparison, you can diff the release_sample.yaml with the updated_sample.yaml . diff serving/samples/traffic-splitting/release_sample.yaml \\ serving/samples/traffic-splitting/updated_sample.yaml Execute the command below to update Service, resulting in a new Revision. kubectl apply --filename docs/serving/samples/traffic-splitting/updated_sample.yaml With our traffic block, traffic will not shift to the new Revision automatically. However, it will be available via the URL associated with our latest sub-route. This can be verified through the Service status, by finding the entry of status.traffic for latest : kubectl get ksvc stock-service-example --output yaml The readiness of the Service can be verified through the Service Conditions. When the Service conditions report it is ready again, you can access the new Revision using the same method as found in the previous sample using the Service hostname found above. # Replace \"latest\" with whichever tag for which we want the hostname. export LATEST_HOSTNAME = ` kubectl get ksvc stock-service-example --output jsonpath = \"{.status.traffic[?(@.tag=='latest')].url}\" | cut -d '/' -f 3 ` curl --header \"Host: ${ LATEST_HOSTNAME } \" http:// ${ INGRESS_IP } Visiting the Service's domain will still hit the original Revision, since we configured it to receive 100% of our main traffic (you can also use the current sub-route). curl --header \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_IP }","title":"Updating the Service"},{"location":"serving/samples/traffic-splitting/#traffic-splitting","text":"Updating the service to split traffic between the two revisions is done by extending our traffic list, and splitting the percent across them. Execute the command below to update Service, resulting in a 50/50 traffic split. kubectl apply --filename docs/serving/samples/traffic-splitting/split_sample.yaml Verify the deployment by checking the service status: kubectl get ksvc --output yaml Once updated, curl requests to the base domain should result in responses split evenly between Welcome to the share app! and Welcome to the stock app! . curl --header \"Host: ${ SERVICE_HOSTNAME } \" http:// ${ INGRESS_IP }","title":"Traffic Splitting"},{"location":"serving/samples/traffic-splitting/#clean-up","text":"To clean up the sample service: kubectl delete --filename docs/serving/samples/traffic-splitting/split_sample.yaml","title":"Clean Up"},{"location":"serving/services/","text":"Knative services \u00b6 Knative services are used to deploy an application. Each Knative service is defined by a route and a configuration, which have the same name as the service. The configuration and route are created by the service controller, and their configuration is derived from the configuration of the service. Each time the configuration is updated, a new revision is created. Revisions are immutable snapshots of a particular configuration, and use underlying Kubernetes resources to scale the number of pods based on traffic.","title":"Overview"},{"location":"serving/services/#knative-services","text":"Knative services are used to deploy an application. Each Knative service is defined by a route and a configuration, which have the same name as the service. The configuration and route are created by the service controller, and their configuration is derived from the configuration of the service. Each time the configuration is updated, a new revision is created. Revisions are immutable snapshots of a particular configuration, and use underlying Kubernetes resources to scale the number of pods based on traffic.","title":"Knative services"},{"location":"serving/services/creating-services/","text":"Creating Knative services \u00b6 To create an application using Knative, you must create a YAML file that defines a Knative service. This YAML file specifies metadata about the application, points to the hosted image of the app and allows the service to be configured. This guide uses the Hello World sample app in Go to demonstrate the structure of a Service YAML file and the basic workflow for deploying an app. These steps can be adapted for your own application if you have an image of it available on Docker Hub, Google Container Registry, or another container image registry. The Hello World sample app works as follows: 1. Reads an environment variable TARGET . 2. Prints Hello ${TARGET}! . If TARGET is not defined, it will use World as the TARGET . Prerequisites \u00b6 To create a Knative service, you will need: * A Kubernetes cluster with Knative Serving installed . * Custom domains set up for Knative services. Procedure \u00b6 Create a new file named service.yaml containing the following information: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" * apiVersion : The current Knative version. * name (metadata): The name of the application. * namespace : The namespace that the application will use. * image : The URL of the image container. * name (env): The environment variable printed out by the sample application. NOTE: If you\u2019re deploying an image of your own app, update the name of the app and the URL of the image accordingly. From the directory where the new service.yaml file was created, deploy the application by applying the service.yaml file. kubectl apply -f service.yaml Now that your app has been deployed, Knative will perform the following steps: Create a new immutable revision for this version of the app. Perform network programming to create a route, ingress, service, and load balancer for your app. Automatically scale your pods up and down based on traffic, including to zero active pods. Modifying Knative services \u00b6 Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The serving.knative.dev/service label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified above must be removed. The Service updates its status fields based on the corresponding status value for the owned Route and Configuration. The Service must include conditions of RoutesReady and ConfigurationsReady in addition to the generic Ready condition. Other conditions can also be present. Next steps \u00b6 To get started with deploying a Knative application, see the Getting Started with App Deployment documentation. For more information about the Knative Service object, see the Resource Types documentation.","title":"Creating a service"},{"location":"serving/services/creating-services/#creating-knative-services","text":"To create an application using Knative, you must create a YAML file that defines a Knative service. This YAML file specifies metadata about the application, points to the hosted image of the app and allows the service to be configured. This guide uses the Hello World sample app in Go to demonstrate the structure of a Service YAML file and the basic workflow for deploying an app. These steps can be adapted for your own application if you have an image of it available on Docker Hub, Google Container Registry, or another container image registry. The Hello World sample app works as follows: 1. Reads an environment variable TARGET . 2. Prints Hello ${TARGET}! . If TARGET is not defined, it will use World as the TARGET .","title":"Creating Knative services"},{"location":"serving/services/creating-services/#prerequisites","text":"To create a Knative service, you will need: * A Kubernetes cluster with Knative Serving installed . * Custom domains set up for Knative services.","title":"Prerequisites"},{"location":"serving/services/creating-services/#procedure","text":"Create a new file named service.yaml containing the following information: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" * apiVersion : The current Knative version. * name (metadata): The name of the application. * namespace : The namespace that the application will use. * image : The URL of the image container. * name (env): The environment variable printed out by the sample application. NOTE: If you\u2019re deploying an image of your own app, update the name of the app and the URL of the image accordingly. From the directory where the new service.yaml file was created, deploy the application by applying the service.yaml file. kubectl apply -f service.yaml Now that your app has been deployed, Knative will perform the following steps: Create a new immutable revision for this version of the app. Perform network programming to create a route, ingress, service, and load balancer for your app. Automatically scale your pods up and down based on traffic, including to zero active pods.","title":"Procedure"},{"location":"serving/services/creating-services/#modifying-knative-services","text":"Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The serving.knative.dev/service label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified above must be removed. The Service updates its status fields based on the corresponding status value for the owned Route and Configuration. The Service must include conditions of RoutesReady and ConfigurationsReady in addition to the generic Ready condition. Other conditions can also be present.","title":"Modifying Knative services"},{"location":"serving/services/creating-services/#next-steps","text":"To get started with deploying a Knative application, see the Getting Started with App Deployment documentation. For more information about the Knative Service object, see the Resource Types documentation.","title":"Next steps"},{"location":"serving/services/deployment/","text":"Modifying the Deployment Config Map \u00b6 The config-deployment ConfigMap is located in the knative-serving namespace. This ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes Deployment resources, that back Knative services, are configured. Accessing the Deployment ConfigMap \u00b6 To view the current Deployment ConfigMap: kubectl get configmap -n knative-serving config-deployment -oyaml Configuring progress deadlines \u00b6 Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision. By default, this value is set to 120 seconds. The value is expressed as a Go time.Duration string representation, but must be rounded to a second precision. The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal Failed state, if the initial scale cannot be achieved within the time limit defined by this setting. You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment: It takes a long time to pull the Service image, due to the size of the image. It takes a long time for the Service to become READY , due to priming of the initial cache state. The cluster is relies on cluster autoscaling to allocate resources for new pods. See the Kubernetes documentation for more information. The following example shows a snippet of an example Deployment Config Map that sets this value to 10 minutes: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : ... progressDeadline : \"10m\" ...","title":"Deployment Configuration"},{"location":"serving/services/deployment/#modifying-the-deployment-config-map","text":"The config-deployment ConfigMap is located in the knative-serving namespace. This ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes Deployment resources, that back Knative services, are configured.","title":"Modifying the Deployment Config Map"},{"location":"serving/services/deployment/#accessing-the-deployment-configmap","text":"To view the current Deployment ConfigMap: kubectl get configmap -n knative-serving config-deployment -oyaml","title":"Accessing the Deployment ConfigMap"},{"location":"serving/services/deployment/#configuring-progress-deadlines","text":"Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision. By default, this value is set to 120 seconds. The value is expressed as a Go time.Duration string representation, but must be rounded to a second precision. The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal Failed state, if the initial scale cannot be achieved within the time limit defined by this setting. You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment: It takes a long time to pull the Service image, due to the size of the image. It takes a long time for the Service to become READY , due to priming of the initial cache state. The cluster is relies on cluster autoscaling to allocate resources for new pods. See the Kubernetes documentation for more information. The following example shows a snippet of an example Deployment Config Map that sets this value to 10 minutes: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : ... progressDeadline : \"10m\" ...","title":"Configuring progress deadlines"},{"location":"serving/services/private-services/","text":"Creating a private service \u00b6 By default services deployed through Knative are published to an external IP address, making them public services on a public IP address and with a public URL. While this is useful for services that need to be accessible from outside of the cluster, frequently you may be building a back-end service which should not be available from outside of the cluster. Knative provides three ways to enable private services which are only available inside the cluster: To make all services private, change the default domain to svc.cluster.local by editing the config-domain ConfigMap . This changes all services deployed through Knative to only be published to the cluster. To make an individual service private, the service or route can be labelled so that it is not published to the external gateway. Use custom domain mappings . Label a service to be cluster-local only \u00b6 To configure a Knative service to only be available on the cluster-local network, and not on the public internet, you can apply the networking.knative.dev/visibility=cluster-local label to a Knative service, a route or a Kubernetes service object. To label a Knative service: kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local By labeling the Kubernetes service you can restrict visibility in a more fine-grained way. See subroutes for information about tagged routes. To label a route when the route is used directly without a Knative service: kubectl label route ${ ROUTE_NAME } networking.knative.dev/visibility = cluster-local To label a Kubernetes service: kubectl label service ${ SERVICE_NAME } networking.knative.dev/visibility = cluster-local Example \u00b6 You can deploy the Hello World sample and then convert it to be an cluster-local service by labelling the service: kubectl label kservice helloworld-go networking.knative.dev/visibility = cluster-local You can then verify that the change has been made by verifying the URL for the helloworld-go service: kubectl get kservice helloworld-go NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.svc.cluster.local helloworld-go-2bz5l helloworld-go-2bz5l True The service returns the a URL with the svc.cluster.local domain, indicating the service is only available in the cluster local network.","title":"Configuring private services"},{"location":"serving/services/private-services/#creating-a-private-service","text":"By default services deployed through Knative are published to an external IP address, making them public services on a public IP address and with a public URL. While this is useful for services that need to be accessible from outside of the cluster, frequently you may be building a back-end service which should not be available from outside of the cluster. Knative provides three ways to enable private services which are only available inside the cluster: To make all services private, change the default domain to svc.cluster.local by editing the config-domain ConfigMap . This changes all services deployed through Knative to only be published to the cluster. To make an individual service private, the service or route can be labelled so that it is not published to the external gateway. Use custom domain mappings .","title":"Creating a private service"},{"location":"serving/services/private-services/#label-a-service-to-be-cluster-local-only","text":"To configure a Knative service to only be available on the cluster-local network, and not on the public internet, you can apply the networking.knative.dev/visibility=cluster-local label to a Knative service, a route or a Kubernetes service object. To label a Knative service: kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local By labeling the Kubernetes service you can restrict visibility in a more fine-grained way. See subroutes for information about tagged routes. To label a route when the route is used directly without a Knative service: kubectl label route ${ ROUTE_NAME } networking.knative.dev/visibility = cluster-local To label a Kubernetes service: kubectl label service ${ SERVICE_NAME } networking.knative.dev/visibility = cluster-local","title":"Label a service to be cluster-local only"},{"location":"serving/services/private-services/#example","text":"You can deploy the Hello World sample and then convert it to be an cluster-local service by labelling the service: kubectl label kservice helloworld-go networking.knative.dev/visibility = cluster-local You can then verify that the change has been made by verifying the URL for the helloworld-go service: kubectl get kservice helloworld-go NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.svc.cluster.local helloworld-go-2bz5l helloworld-go-2bz5l True The service returns the a URL with the svc.cluster.local domain, indicating the service is only available in the cluster local network.","title":"Example"},{"location":"serving/spec/knative-api-specification-1.0/","text":"Knative Serving API Specification \u00b6 This file has been moved to the Knative Specs Repository","title":"Knative Serving API Specification"},{"location":"serving/spec/knative-api-specification-1.0/#knative-serving-api-specification","text":"This file has been moved to the Knative Specs Repository","title":"Knative Serving API Specification"},{"location":"upgrade/upgrade-installation-with-operator/","text":"Upgrading using the Knative Operator \u00b6 The attribute spec.version is the only field you need to change in the Serving or Eventing custom resource to perform an upgrade. You do not need to specify the version for the patch number, because the Knative Operator matches the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify \"0.22\" to upgrade to the 0.22 release, you do not need to specify the exact patch number. The Knative Operator supports up to the last three major releases. For example, if the current version of the Operator is 0.22, it bundles and supports the installation of Knative versions 0.19, 0.20, 0.21 and 0.22. NOTE: In the following examples, Knative Serving custom resources are installed in the knative-serving namespace, and Knative Eventing custom resources are installed in the knative-eventing namespace. Performing the upgrade \u00b6 To upgrade, apply the Operator custom resources, adding the spec.version for the Knative version that you want to upgrade to: kubectl apply -f - <<EOF apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"0.22\" EOF ## Verifying the upgrade To confirm that your Knative components have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace : ``` bash kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s networking-istio-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s You can also verify the status of Knative by checking the custom resources: kubectl get KnativeServing knative-serving --namespace knative-serving kubectl get KnativeEventing knative-eventing --namespace knative-eventing These commands return something similar to: NAME VERSION READY REASON knative-serving 0 .20.0 True NAME VERSION READY REASON knative-eventing 0 .20.0 True Rollback \u00b6 If the upgrade fails, you can rollback to restore your Knative to the previous version. For example, if something goes wrong with an upgrade to 0.22, and your previous version is 0.21, you can apply the following custom resources to restore Knative Serving and Eventing to version 0.21. For Knative Serving: ```yaml kubectl apply -f - <<EOF apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.21\" EOF For Knative Eventing: ```yaml kubectl apply -f - <<EOF apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.21\" EOF","title":"Upgrading using the Knative Operator"},{"location":"upgrade/upgrade-installation-with-operator/#upgrading-using-the-knative-operator","text":"The attribute spec.version is the only field you need to change in the Serving or Eventing custom resource to perform an upgrade. You do not need to specify the version for the patch number, because the Knative Operator matches the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify \"0.22\" to upgrade to the 0.22 release, you do not need to specify the exact patch number. The Knative Operator supports up to the last three major releases. For example, if the current version of the Operator is 0.22, it bundles and supports the installation of Knative versions 0.19, 0.20, 0.21 and 0.22. NOTE: In the following examples, Knative Serving custom resources are installed in the knative-serving namespace, and Knative Eventing custom resources are installed in the knative-eventing namespace.","title":"Upgrading using the Knative Operator"},{"location":"upgrade/upgrade-installation-with-operator/#performing-the-upgrade","text":"To upgrade, apply the Operator custom resources, adding the spec.version for the Knative version that you want to upgrade to: kubectl apply -f - <<EOF apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"0.22\" EOF ## Verifying the upgrade To confirm that your Knative components have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace : ``` bash kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s networking-istio-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s You can also verify the status of Knative by checking the custom resources: kubectl get KnativeServing knative-serving --namespace knative-serving kubectl get KnativeEventing knative-eventing --namespace knative-eventing These commands return something similar to: NAME VERSION READY REASON knative-serving 0 .20.0 True NAME VERSION READY REASON knative-eventing 0 .20.0 True","title":"Performing the upgrade"},{"location":"upgrade/upgrade-installation-with-operator/#rollback","text":"If the upgrade fails, you can rollback to restore your Knative to the previous version. For example, if something goes wrong with an upgrade to 0.22, and your previous version is 0.21, you can apply the following custom resources to restore Knative Serving and Eventing to version 0.21. For Knative Serving: ```yaml kubectl apply -f - <<EOF apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.21\" EOF For Knative Eventing: ```yaml kubectl apply -f - <<EOF apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.21\" EOF","title":"Rollback"},{"location":"upgrade/upgrade-installation/","text":"Upgrading Knative \u00b6 You can use the kubectl apply command to upgrade your Knative components and plugins. Before you begin \u00b6 Before upgrading, there are a few steps you must take to ensure a successful upgrade process. Identify breaking changes \u00b6 You should be aware of any breaking changes between your current and desired versions of Knative. Breaking changes between Knative versions are documented in the Knative release notes. Before upgrading, review the release notes for the target version to learn about any changes you might need to make to your Knative applications: Serving Eventing Release notes are published with each version on the \"Releases\" page of their respective repositories in GitHub. View current pod status \u00b6 Before upgrading, view the status of the pods for the namespaces you plan on upgrading. This allows you to compare the before and after state of your namespace. For example, if you are upgrading Knative Serving and Eventing, enter the following commands to see the current state of each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing Upgrade plugins \u00b6 If you have a plugin installed, make sure to upgrade it at the same time as you upgrade your Knative components. Run pre-install tools before upgrade \u00b6 In some upgrades there are some steps that must happen before the actual upgrade, and these are identified in the release notes. Upgrade existing resources to the latest stored version \u00b6 Our custom resources are stored within Kubernetes at a particular version. As we introduce newer and remove older supported versions, you must migrate the resources to the designated stored version. This ensures removing older versions will succeed when upgrading. For the various subprojects there is a K8s job to help operators perform this migration. The release notes for each release will state explicitly whether a migration is required. Performing the upgrade \u00b6 To upgrade, apply the YAML files for the subsequent minor versions of all your installed Knative components and features, remembering to only upgrade by one minor version at a time. For a cluster running version 0.20 of the Knative Serving and Eventing components, the following command upgrades the installation to v0.22.0: kubectl apply -f https://github.com/knative/serving/releases/download/v0.22.0/serving-core.yaml \\ -f https://github.com/knative/eventing/releases/download/v0.22.0/eventing.yaml \\ Run post-install tools after the upgrade \u00b6 In some upgrades there are some steps that must happen after the actual upgrade, and these are identified in the release notes. Verifying the upgrade \u00b6 To confirm that your components and plugins have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-79f674fb7b-dgvss 2 /2 Running 0 43s autoscaler-96dc49858-b24bm 2 /2 Running 1 43s autoscaler-hpa-d887d4895-njtrb 1 /1 Running 0 43s controller-6bcdd87fd6-zz9fx 1 /1 Running 0 41s networking-istio-7fcd97cbf7-z2xmr 1 /1 Running 0 40s webhook-747b799559-4sj6q 1 /1 Running 0 41s NAME READY STATUS RESTARTS AGE eventing-controller-69ffcc6f7d-5l7th 1 /1 Running 0 83s eventing-webhook-6c56fcd86c-42dr8 1 /1 Running 0 81s imc-controller-6bcf5957b5-6ccp2 1 /1 Running 0 80s imc-dispatcher-f59b7c57-q9xcl 1 /1 Running 0 80s sources-controller-8596684d7b-jxkmd 1 /1 Running 0 83s If the age of all your pods has been reset and all pods are up and running, the upgrade was completed successfully. You might notice a status of Terminating for the old pods as they are cleaned up. If necessary, repeat the upgrade process until you reach your desired minor version number.","title":"Upgrading Knative"},{"location":"upgrade/upgrade-installation/#upgrading-knative","text":"You can use the kubectl apply command to upgrade your Knative components and plugins.","title":"Upgrading Knative"},{"location":"upgrade/upgrade-installation/#before-you-begin","text":"Before upgrading, there are a few steps you must take to ensure a successful upgrade process.","title":"Before you begin"},{"location":"upgrade/upgrade-installation/#identify-breaking-changes","text":"You should be aware of any breaking changes between your current and desired versions of Knative. Breaking changes between Knative versions are documented in the Knative release notes. Before upgrading, review the release notes for the target version to learn about any changes you might need to make to your Knative applications: Serving Eventing Release notes are published with each version on the \"Releases\" page of their respective repositories in GitHub.","title":"Identify breaking changes"},{"location":"upgrade/upgrade-installation/#view-current-pod-status","text":"Before upgrading, view the status of the pods for the namespaces you plan on upgrading. This allows you to compare the before and after state of your namespace. For example, if you are upgrading Knative Serving and Eventing, enter the following commands to see the current state of each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing","title":"View current pod status"},{"location":"upgrade/upgrade-installation/#upgrade-plugins","text":"If you have a plugin installed, make sure to upgrade it at the same time as you upgrade your Knative components.","title":"Upgrade plugins"},{"location":"upgrade/upgrade-installation/#run-pre-install-tools-before-upgrade","text":"In some upgrades there are some steps that must happen before the actual upgrade, and these are identified in the release notes.","title":"Run pre-install tools before upgrade"},{"location":"upgrade/upgrade-installation/#upgrade-existing-resources-to-the-latest-stored-version","text":"Our custom resources are stored within Kubernetes at a particular version. As we introduce newer and remove older supported versions, you must migrate the resources to the designated stored version. This ensures removing older versions will succeed when upgrading. For the various subprojects there is a K8s job to help operators perform this migration. The release notes for each release will state explicitly whether a migration is required.","title":"Upgrade existing resources to the latest stored version"},{"location":"upgrade/upgrade-installation/#performing-the-upgrade","text":"To upgrade, apply the YAML files for the subsequent minor versions of all your installed Knative components and features, remembering to only upgrade by one minor version at a time. For a cluster running version 0.20 of the Knative Serving and Eventing components, the following command upgrades the installation to v0.22.0: kubectl apply -f https://github.com/knative/serving/releases/download/v0.22.0/serving-core.yaml \\ -f https://github.com/knative/eventing/releases/download/v0.22.0/eventing.yaml \\","title":"Performing the upgrade"},{"location":"upgrade/upgrade-installation/#run-post-install-tools-after-the-upgrade","text":"In some upgrades there are some steps that must happen after the actual upgrade, and these are identified in the release notes.","title":"Run post-install tools after the upgrade"},{"location":"upgrade/upgrade-installation/#verifying-the-upgrade","text":"To confirm that your components and plugins have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-79f674fb7b-dgvss 2 /2 Running 0 43s autoscaler-96dc49858-b24bm 2 /2 Running 1 43s autoscaler-hpa-d887d4895-njtrb 1 /1 Running 0 43s controller-6bcdd87fd6-zz9fx 1 /1 Running 0 41s networking-istio-7fcd97cbf7-z2xmr 1 /1 Running 0 40s webhook-747b799559-4sj6q 1 /1 Running 0 41s NAME READY STATUS RESTARTS AGE eventing-controller-69ffcc6f7d-5l7th 1 /1 Running 0 83s eventing-webhook-6c56fcd86c-42dr8 1 /1 Running 0 81s imc-controller-6bcf5957b5-6ccp2 1 /1 Running 0 80s imc-dispatcher-f59b7c57-q9xcl 1 /1 Running 0 80s sources-controller-8596684d7b-jxkmd 1 /1 Running 0 83s If the age of all your pods has been reset and all pods are up and running, the upgrade was completed successfully. You might notice a status of Terminating for the old pods as they are cleaned up. If necessary, repeat the upgrade process until you reach your desired minor version number.","title":"Verifying the upgrade"}]}